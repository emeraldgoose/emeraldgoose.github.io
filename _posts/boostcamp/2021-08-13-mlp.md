---
title: MLP 정리
categories:
  - BoostCamp
tags: [mlp]
---
## Neural Networks
> Neural Networks are function approximators that stack affine transformations followed by nonlinear transforms.

- 뉴럴 네트워크는 수학적이고 비선형 연산이 반복적으로 일어나는 어떤 함수를 모방(근사)하는 것이다.

## Linear Neural Networks
- 간단한 예제를 들어보자
    - Data : $D = (x_i,j_i)_{i=1}^N$
    - Model : $\hat{y} = wx + b$
    - Loss : loss$= \frac{1}{N}\sum_{i=1}^N (y_i-\hat{y_i})^2$
- 그러면 최적화 변수에 대해서 편미분을 계산할 수 있다
    - $\frac{\partial loss}{\partial w} = \frac{\partial}{\partial w}\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y})^2 = \frac{\partial}{\partial w}\frac{1}{N}\sum_{i=1}^N(y_i - wx_i - b)^2 = -\frac{1}{N}\sum_{i=1}^N-2(y_i-wx_i-b)x_i$
- 이제 최적화 변수를 반복적으로 업데이트할 수 있다.
    - $w \leftarrow w - \eta\frac{\partial loss}{\partial w}$
    - $b \leftarrow b - \eta\frac{\partial loss}{\partial b}$
- 물론 multi dimension에 대해서도 핸들할 수 있다.
    - $y = W^Tx + b$
    - 위와 같은 식을 $x \overset{\underset{\text{\{ w,b \}}}\{\}} \{\rightarrow\} y$

## Beyond Linear Neural Networks
- What if we stack more?
    - $y = W_2^Th = W_2^TW_1^Tx$
- 비선형 방식도 가능
    - $y = W_2^Th = W_2^T\rho(W_1^Tx)$, $\rho$는 Nonlinear transform이다.
- Activation functions
    - ReLU
    - Sigmoid
    - Hyperbolic Tangent

## Multi-Layer Perceptron
- $y = W_2^Th = W_2^T\rho(W_1^Tx)$
- 출력을 다시 입력으로 넣어 레이어를 쌓은 모델을 MLP라 한다.
- 물론 더 깊게 쌓을 수 있다.
    - $y =  W_3^T h = W_3^T\rho(W_2^Th_1)$ $ = W_3^T\rho(W_2^T \rho(W_1^T x))$
- loss function은 다음과 같이 사용될 수 있지만 상황에 따라 달라질 수 있다.
    - Regression Task : MSE
    - Classification Task : CE
    - Probabilistic Task : MLE
