---
title: airflow 체험기
categories:
  - airflow
tags: [airflow, Ops]
---
> 부캠 때 에러로 사용못한 airflow를 이제서야 체험해본 것을 정리한 글입니다. (무지성 주의)

## airflow 시작
데이터 엔지니어링 도구인 airflow를 로컬에 설치한 후 몇가지를 살펴보았는데 airflow는 데이터를 관리하는 것이 아닌 함수 단위나 스크립트 단위로 실행시켜주는 도구라는 것을 깨닫게 되었습니다.  
그렇다면 데이터를 생성해서 스케줄링을 돌리자!라는 생각이 들어 재밌어보여서 바로 실행에 옮겼습니다. 다시 어떤 데이터를 생성해야 하나?라는 생각에 기업이라면 로그들을 사용하므로 저도 간단한 로그를 생성해서 db에 넣는 것 까지 해볼 생각이었습니다.  

### 데이터 형태
실제 로그들을 보면 많은 양의 정보가 들어있지만 간단하게 생성할 생각이므로 (타임스탬프, level, 메시지, id, 오류 check)로 구성했습니다.

```{"timestamp": 1641699997.0480268, "level": "ERROR", "id": 23, "message": "An unexpected error occurred.", "check": 0}```

check항목은 10%의 확률로 오류인경우 check에 1이 들어가도록 설정했습니다. 실제로는 좀 더 복잡한 데이터 검증을 하겠지만 여기서는 간단하게 에러인지 아닌지 확인하는 정도만 확인하도록 구성했습니다.  

## 데이터 생성
각종 서버에서 생성된 로그들은 한 곳에 모아지거나 어느정도 분산된 데이터베이스로 전달되는데 여기는 메시지 큐(Message Queue, MQ)를 사용하고 싶었습니다. 로그들이 메시지 큐로 들어가 에어플로우 worker들이 큐에서 하나씩 빼와서 에러 체크한 후 db에 넣어주는 것을 생각했습니다.  
메시지 큐는 RabbitMQ, Redis 등이 있는데 어디서 들어본 Redis를 사용해서 큐를 구축했습니다. [링크](https://blog.naver.com/wideeyed/221370229153)에서 코드들을 가져와 코드들을 다시 재구성했습니다.  

```python
# producer.py
import json
import time
import random
from redisqueue import RedisQueue
from constant import REDIS_HOST, REDIS_PORT

q = RedisQueue('my-queue', host=REDIS_HOST, port=REDIS_PORT, db=0)
level = ['INFO', 'WARNING', 'ERROR']
message = ['All passed.', 'Incorrect access', 'An unexpected error occurred.']

if __name__ == "__main__":
    # message put
    for i in range(30):
        cur_time = '{"timestamp":' + str(time.time()) + '}'
        element  = json.loads(cur_time)

        # Add Your Own Data
        ridx = random.randint(0,2)
        rndn = random.random()
        element['level'] = level[ridx]
        element['id'] = i
        element['message'] = message[ridx]
        element['check'] = 0 if rndn > 0.1 else 1

        element_str = json.dumps(element)
        print(element_str)
        q.put(element_str)
        time.sleep(1)
    
```
먼저, `producer.py`는 로그를 생성하는 파일입니다. 랜덤으로 ['INFO', 'WARNING', 'ERROR']를 선택하게 하고 그에 맞는 메시지도 포함시키도록 했습니다. 에러는 위에서 말한대로 10%의 확률로 일어나고 json 포맷 형태로 큐에 들어가고 1초마다 생성하도록 구성했습니다.

## Redis 세팅
Redis를 사용하려면 Redis가 돌아가는 서버를 돌려야 하는데 참고한 블로그와 마찬가지로 도커를 이용하기로 했습니다. 이후에 airflow까지 도커로 돌릴 생각이었기 때문입니다. (사실 맥북 용량이 얼마 안남았습니다. 조만간 포맷해야...)
도커에서 이미지를 가져와야 하는데 저는 블로그와 똑같이 redis:latest 위에서 작업했습니다. dockerfile을 만들지는 않고 컨테이너 위에서 설치했습니다.  

먼저, redis-server라는 이름의 컨테이너에서 6379번 포트를 열어서 redis 이미지를 실행시킵니다.
`docker run --name redis-server -p 6379:6379 -it redis:latest /bin/bash`

redis 서버를 사용하려면 host 주소와 포트번호를 알아야 하는데 이를 위해 ifconfig를 설치할 것입니다.  
`apt-get update && apt-get install net-tools && ifconfig`
그러면 eth0에 172.17.0.x의 주소가 보이는데 이것을 기록해놓으면 됩니다.

- 나중에 알았는데 vsc에서 도커 익스텐션을 설치한 후 bridge에서 검사를 누르면 열러있는 도커들의 주소들을 알 수 있습니다.

`redis-server`실행하게 되면 redis 서버가 실행되면서 세팅은 끝나게 됩니다.

### Redis-client 세팅
이 컨테이너는 `producer.py`를 실행시켜 로그를 생성해 큐에 메시지를 넣는 컨테이너입니다. python 이미지 위에서 redis를 설치하면 끝납니다. 직접 실행시켜도 되고 watch 명령어로 주기적으로 실행시켜도 됩니다.  

## Airflow 세팅
airflow는 이전에 세팅한 경험이 있어서 비교적 쉽게 할 수 있었습니다. 이번에는 python 이미지 위에서 작업했습니다.  
`docker run --name airflow-server -p 8080:8080 -it python:3.7.0 /bin/bash`  
저는 `-v`옵션을 통해서 file sharing을 활성화했습니다. 이 옵션을 선택하면 로컬에 있는 폴더에서 컨테이너 내부로 파일을 쉽게 옮길 수 있습니다. vsc로 코드를 작성하면 `cp`명령어로 업데이트까지 편하게 할 수 있습니다.  

먼저, airflow는 airflow webserver와 airflow scheduler를 동시에 실행시켜야 합니다. 그래서 screen을 이용하기로 했습니다.
`apt-get update && apt-get install screen`  

`AIRFLOW_HOME`을 비롯해 DB 초기화, Admin 계정을 만들어 줍니다. 이제 screen으로 서버와 스케줄러를 돌려줍니다. 스크린에서 빠져나오는 키는 `ctrl+a+d`입니다.  
`screen -S airflow-scheduler`  
`airflow scheduler`  
`screen -S airflow-server`  
`airflow webserver -p 8080`  

## Dag작성
airflow가 돌아가는 컨테이너에서 `AIRFLOW_HOME` 위치에서 `mkdir dags`로 dag를 넣어줄 디렉토리를 생성합니다.  
dag는 다음과 같이 작성합니다.

```python
# func.py
import json
import time
import datetime
import sqlite3
from sqlite3.dbapi2 import Error
from redisqueue import RedisQueue
from constant import REDIS_HOST, REDIS_PORT

def consumer():
    q = RedisQueue('my-queue', host=REDIS_HOST, port=REDIS_PORT, db=0)
    msg = q.get(isBlocking=False)
    while msg is not None:
        msg_json = json.loads(msg.decode('utf-8'))
        if not msg_json['check']:
            try:
                with sqlite3.connect('sqltie.db') as con:
                    cur = con.cursor()
                    _date = datetime.datetime.fromtimestamp(
                        int(msg_json['timestamp'])) \
                        .strftime('%Y-%m-%d %H:%M:%S')
                    cur.execute(
                        "INSERT INTO table \
                        (time, level, id) VALUES (?,?,?)",
                        (str(_date),msg_json['level'],msg_json['id']))
            except Error as e:
                con.rollback()
                print(e)          
        time.sleep(1)
```
위의 코드에서 `isBlocking=True`로 두면 큐가 비어있을 때 무한정 대기한다는 뜻입니다. False로 두어서 큐가 비어있으면 태스크를 끝내도록 작성했습니다.

```python
# mydags.py
from datetime import timedelta
from sys import argv

from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

from func import consumer

with DAG(
    dag_id='data_collector',
    description='My dag',
    start_date=days_ago(2) ,
    schedule_interval="0 0/5 * * *", # 5분간격
    tags=['my_dags'],
    concurrency=10,
) as dag:

    t1 = BashOperator(
        task_id='start_schedule',
        bash_command='echo START DATA COLLECTOR',
        owner='gooose',
        retries=3,
        retry_delay=timedelta(minutes=1),
    )

    t2 = PythonOperator(
        task_id='collector_0',
        python_callable=consumer,
        depends_on_past=True,
        owner='goooose',
        retries=3,
        retry_delay=timedelta(minutes=1),
    )

    t3 = PythonOperator(
        task_id='collector_1',
        python_callable=consumer,
        depends_on_past=True,
        owner='goooose',
        retries=3,
        retry_delay=timedelta(minutes=1),
    )

    t4 = PythonOperator(
        task_id='collector_2',
        python_callable=consumer,
        depends_on_past=True,
        owner='goose',
        retries=3,
        retry_delay=timedelta(minutes=1),
    )

    t5 = BashOperator(
        task_id='end_schedule',
        bash_command='echo END DATA COLLECTOR',
        owner='gooose',
        retries=3,
        retry_delay=timedelta(minutes=1),
    )

    # 태스크 순서 정의
    t1>>[t2, t3, t4]>>t5
```

## 몰랐던 airflow.cfg
처음 생각은 consumer역할을 맡은 t2, t3, t4에서 큐에서 하나씩 로그를 빼낸 후 에러 검증 후 db에 넣는 것을 생각했습니다.  

그러나, collector_0만 실행되고 나머지 collector는 큐에 대기하다가 실행이 끝났습니다.  
airflow의 병렬처리를 할 수 있는 방법에 대해 찾아보니 celery를 사용하는 것을 볼 수 있었습니다.

airflow 설정파일인 `airflow.cfg`를 보면 다음과 같은 항목이 보입니다.  
`executor = SequentialExecutor`  
airflow의 초기 세팅은 SequentialExecutor이었고 collector_0만 실행되는 이유도 같은 것이었습니다.  

그렇다면 celery만 사용하면 되는것이 아닌가?하는 의문이 생길 수 있는데 여기부터 좀 어려웠습니다.  
celery가 지원하는 db는 postgresql, mysql 정도이고 세팅하려면 새로 컨테이너를 올려야 한다는 판단을 했습니다.

## 현재 위치
mysql을 선택하고 컨테이너에 올렸는데 `Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/mysql/lib/plugin/caching_sha2_password.so, 2): image not found`이런 에러로 더이상 진행되지 못하고 있습니다.  

해결방법인 `ALTER USER root@localhost IDENTIFIED WITH mysql_natvie_password by 'password'`까지 시도했지만 해결되지 않았습니다.    

다른 DB인 postgresql로 바꾼 뒤에 다시 시도해보고 성공하면 다음 포스트에 후기를 남길 수 있을 것 같습니다.