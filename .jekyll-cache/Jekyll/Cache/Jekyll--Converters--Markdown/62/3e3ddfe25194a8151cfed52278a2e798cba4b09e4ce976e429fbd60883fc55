I"A<blockquote>
  <p>í˜„ì¬ NLPëŠ” Transformerê°€ ì£¼ë„í•˜ê³  ê·¸ ì´ì „ì—ëŠ” RNNê³¼ ê°™ì´ Recurrentí•œ ëª¨ë¸ì´ ì£¼ë„í–ˆì§€ë§Œ ê·¸ ì´ì „ì—ëŠ” ë‹¨ì–´ ë° ë¬¸ì„œë¥¼ ìˆ«ì í˜•íƒœë¡œ ë‚˜íƒ€ë‚´ëŠ” Bag-of Words ê¸°ë²•ì„ ì •ë¦¬í•œë‹¤.</p>
</blockquote>

<h2 id="bag-of-words-representation">Bag-of-Words Representation</h2>

<ul>
  <li>Step 1. Constructing the <strong>vocabulary</strong> containing <strong>unique words</strong> â†’ ì‚¬ì „ì„ ë§Œë“œëŠ” ê³¼ì •
    <ul>
      <li>â€œJohn really really loves this movieâ€, â€œJane really likes this songâ€</li>
      <li>Vocabulary : {â€œJohnâ€, â€œreallyâ€, â€œlovesâ€, â€œthisâ€, â€œmovieâ€, â€œJaneâ€, â€œlikesâ€, â€œsongâ€}</li>
    </ul>
  </li>
  <li>Step 2. Encoding unique words to <strong>one-hot vectors</strong>
    <ul>
      <li>Vocabulary : {â€œJohnâ€, â€œreallyâ€, â€œlovesâ€, â€œthisâ€, â€œmovieâ€, â€œJaneâ€, â€œlikesâ€, â€œsongâ€}
        <ul>
          <li>John : [1 0 0 0 0 0 0 0]</li>
          <li>movie : [0 0 0 0 1 0 0 0]</li>
          <li>loves : [0 0 1 0 0 0 0 0]</li>
          <li>â€¦</li>
        </ul>
      </li>
      <li>ì„ì˜ì˜ ë‹¨ì–´ë“¤ê°„ì˜ ê±°ë¦¬ëŠ” $\sqrt{2}$ (Euclidean distance)</li>
      <li>ì„ì˜ì˜ ë‹¨ì–´ë“¤ê°„ì˜ cosine similarityëŠ” 0
        <ul>
          <li>ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ê°™ì€ ë²¡í„°ì¼ë•Œ 1, ë‹¤ë¥´ë©´ 0</li>
        </ul>
      </li>
      <li>Bag-of-Words vector : A sentence / document can be represented as <strong>the sum of one-hot vectors</strong>
        <ul>
          <li>sentence 1 : â€œJohn really really loves this movieâ€
            <ul>
              <li>John + really + really + loves + this + movie : [1 2 1 1 1 0 0 0]</li>
            </ul>
          </li>
          <li>sentence 2 : â€œJane really likes this songâ€
            <ul>
              <li>Jane + really + likes + this + song : [0 1 0 1 0 1 1 1]</li>
            </ul>
          </li>
          <li>Bag-of-Words vectorë¼ ë¶€ë¥´ëŠ” ì´ìœ ëŠ” ë¬¸ì¥ë§ˆë‹¤ ë‹¨ì–´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ê°€ë°©ì— ë„£ê³  ê°€ë°©ì— ë„£ì€ ë‹¨ì–´ë“¤ì˜ ìˆ˜ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•œ ê²ƒì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</li>
        </ul>
      </li>
    </ul>

    <p>## NaiveBayes Classifier for Document Classification</p>

    <ul>
      <li>Bag-of-Words for Document Classification
        <ul>
          <li>Bag-of-Words vectorë¥¼ ì •í•´ì§„ ì¹´í…Œì½”ë¦¬ í˜¹ì€ í´ë˜ìŠ¤ ì¤‘ì˜ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œì ì¸ ë°©ë²•</li>
        </ul>
      </li>
      <li>Bayesâ€™ Rule Applied to Documents and Classes
        <ul>
          <li>MAP is â€œmaximum a posterionâ€ â†’ most likely class</li>
          <li>a document $d$ and a class $c$, the number of class is $C$</li>
          <li>$C_{MAP} = argmax_{c \in C} \space P(c|d) = argmax_{c \in C}\space \frac{P(d|c)P(c)}{P(d)} = argmax_{c \in C} P(d|c)P(c)$
            <ul>
              <li>Bayes Ruleì— ì˜í•´ $P(d)$ê°€ í•„ìš”í•˜ì§€ë§Œ argmax operationìœ¼ë¡œ ì¸í•´ í•„ìš”í•˜ì§€ ì•Šê²Œ ëœë‹¤.</li>
            </ul>
          </li>
          <li>$P(d|c)P(c) = P(w_1, w_2, â€¦, w_n|c)P(c) â†’ P(c)\Pi_{w_i \in W}P(w_i|c)$
            <ul>
              <li>by conditional independence assumption</li>
              <li>íŠ¹ì • ì¹´í…Œì½”ë¦¬ $c$ê°€ ê³ ì •ë˜ì—ˆì„ ë•Œ ë¬¸ì„œ $d$ê°€ ë‚˜íƒ€ë‚  í™•ë¥  $P(d|c)$ì€ ë¬¸ì„œ ì•ˆ word $w_1$ë¶€í„° $w_n$ê¹Œì§€ ë™ì‹œ ì‚¬ê±´ìœ¼ë¡œì„œ ë³¼ ìˆ˜ ìˆë‹¤.</li>
              <li>ê° ë‹¨ì–´ë“¤ì˜ í™•ë¥ ì´ $c$ê°€ ê³ ì •ë˜ì—ˆì„ ê²½ìš° ë…ë¦½ìœ¼ë¡œ ê°€ì •í•  ìˆ˜ ìˆë‹¤ë©´ ê° ë‹¨ì–´ë“¤ì˜ í™•ë¥ ì„ ëª¨ë‘ ê³±í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>í•™ìŠµ ë°ì´í„° ë‚´ì— íŠ¹ì • ë‹¨ì–´ê°€ ì „í˜€ ë°œê²¬ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° í™•ë¥  ê³„ì‚° ì‹œ ê·¸ ë‹¨ì–´ì— ëŒ€í•œ í™•ë¥ ê°’ì€ 0ì´ ë˜ë©´ì„œ ê·¸ ë‹¨ì–´ê°€ ë¬¸ì¥ê³¼ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆì–´ë„ í•´ë‹¹ í´ë˜ìŠ¤ë¡œ ì ˆëŒ€ ë¶„ë¥˜ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.</li>
    </ul>
  </li>
</ul>
:ET