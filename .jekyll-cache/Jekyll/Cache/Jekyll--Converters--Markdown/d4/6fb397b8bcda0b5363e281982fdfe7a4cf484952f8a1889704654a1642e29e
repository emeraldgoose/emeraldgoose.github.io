I"<h2 id="rag">RAG</h2>
<p>RAG (Retrieval Augmented Generation)란, LLM이 학습한 데이터로부터 답변하지 않고 외부 문서(Context)를 참조할 수 있도록 하는 시스템을 말합니다. RAG는 LLM이 자신이 학습한 데이터가 아닌 외부의 최신 정보를 참조하여 답변의 부정확성이나 환각(Hallucination)을 줄일 수 있습니다.</p>

<h2 id="retriever">Retriever</h2>
<p>Retriever는 유저의 질문에 관련이 높은 문서를 가져오는 역할을 담당하는 구성요소입니다. 관련 문서를 가져오는 전략으로는 BM25, Bi-Encoder 등의 방식이 있습니다.</p>

<h3 id="lexical-search-bm25">Lexical Search: BM25</h3>
<p>BM25는 키워드 매칭을 통해 점수를 내는 방식을 말합니다. TF-IDF를 활용해 질의와 문서간의 유사도를 계산하여 빠르고 간단한 질문에 적합합니다. 하지만, 질의의 문맥을 이용하지 못해 관련된 문서가 검색되는 경우도 있습니다.</p>

<h3 id="semantic-search-bi-encoder">Semantic Search: Bi-Encoder</h3>
<p>Bi-Encoder는 질의와 문서간의 임베딩 벡터 유사도를 계산하여 점수를 내는 방식입니다. BERT와 같은 언어모델을 이용해 문맥적 의미가 담긴 임베딩 벡터를 생성하여 질의와 문서의 의미적인 유사성을 잘 찾아낼 수 있습니다. 미리 벡터로 변환하여 저장할 수 있어 검색 시 유사도만 계산하면 되므로 크게 느리지 않습니다. 하지만, 질의나 문서가 길어질수록 벡터로 표현할 때 정보가 손실될 수 있고 Query와 Candidates간의 관계성을 고려하지 않기 때문에 Cross-Encoder 대비 정확도가 떨어집니다.</p>

<h3 id="hybrid-search-lexical-search--semantic-search">Hybrid Search: Lexical Search + Semantic Search</h3>
<p>Hybrid Search는 유저 질의의 키워드를 중점으로 찾는 BM25와 유저 질의와 의미적으로 유사한 문서를 찾는 Bi-Encoder를 같이 사용하는 방법을 말합니다. Hybrid Search의 검색 결과는 Lexcial Search 혹은 Semantic Search를 단독으로 사용한 결과보다 정확한 문서를 검색할 수 있습니다. 검색 결과에 가중치를 부여하면 도메인 특성에 따라 최적화할 수 있습니다. 보통 자주 사용하는 계산방식으로는 CC(Convex Combination)과 RRF(Reciprocal Rank Fusion)이 있습니다.</p>

<p><strong>Convex Combination</strong></p>
<ul>
  <li>$score(doc) = \alpha * score_{lex}(doc) + \beta * score_{sem}(doc)$</li>
</ul>

<p>CC는 alpha와 beta을 각각 스코어에 곱해 최종 스코어를 계산하는 방식입니다. 두 스코어 값을 합치기 위해 각 스코어마다 Normalization 과정이 필요합니다.</p>

<p><strong>Reciprocal Rank Fusion</strong></p>
<ul>
  <li>$score(doc) = \frac{1}{k + rank_{lex}(doc)} + \frac{1}{k + rank_{sem}(doc)}$</li>
</ul>

<p>RRF는 가중치가 아닌 문서의 순위를 이용하는 방식입니다. 가중치를 이용하지 않기 때문에 Normalization 과정이 필요하지 않고 순위가 낮은 문서를 위해 조정하는 임의의 값입니다.</p>

<h3 id="cross-encoder">Cross-Encoder</h3>
<p>Cross-Encoder는 하나의 언어모델에 Query와 Candidate를 같이 넣어 유사도를 계산하는 방식입니다. Bi-Encoder 대비 예측 성능이 괜찮지만 Cross-Encoder를 Retreiver로 사용한다면 매 질의마다 모든 문서와의 유사도를 계산해야 하기 때문에 단독으로 사용하기 어렵습니다.</p>

<h2 id="reranking-전략">Reranking 전략</h2>
<p>Reranking 전략은 적절한 문서 후보군을 선별하고 Cross-Encoder로 순위를 재조정하는 전략을 말합니다. 이렇게 구성하면, Cross-Encoder만 사용했을 때보다 계산 비용이 줄고 정확도를 확보할 수 있는 방법입니다.</p>

<figure>
    <img src="https://framerusercontent.com/images/4atMCrE67i6XDQdSySevR8sVhM.png" />
    <figcaption>https://dify.ai/blog/hybrid-search-rerank-rag-improvement</figcaption>
</figure>

<p>그렇다면 RAG 시스템에서 Reranking 전략이 왜 필요한지 생각해볼 수 있는데 LLM의 답변 정확도가 Context의 순서와 관계가 있기 때문입니다.
<a href="https://arxiv.org/abs/2307.03172">Lost in the Middle: How Language Models Use Long Contexts</a> 논문에는 정답과 관련된 문서가 가운데 위치할 수록 답변 성능이 떨어지는 것을 확인할 수 있습니다. 따라서, 이러한 이유 때문에 RAG 시스템에서 Hybrid Search의 문서 순위를 재조정하는 Reranking 전략이 필요함을 알 수 있습니다.</p>

<h2 id="rag-pipeline">RAG Pipeline</h2>
<figure>
    <a href="https://lh3.googleusercontent.com/d/1DGlGK8ImHDX82xyIlxN1z_tYZZl28Gcb" onclick="return false;" data-lightbox="gallery">
        <img src="https://lh3.googleusercontent.com/d/1DGlGK8ImHDX82xyIlxN1z_tYZZl28Gcb" alt="01" />
    </a>
</figure>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://www.elastic.co/search-labs/blog/hybrid-search-elasticsearch">hyrbrid-search-elasticsearch</a></li>
  <li><a href="https://medium.com/@nuatmochoi/%ED%95%98%EC%9D%B4%EB%B8%8C%EB%A6%AC%EB%93%9C-%EA%B2%80%EC%83%89-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0-feat-ensembleretriever-knowledge-bases-for-amazon-bedrock-d6ef1a0daaf1">하이브리드 검색 구현하기 (feat. EnsembleRetriever, Knowledge Bases for Amazon Bedrock)</a></li>
  <li><a href="https://heygeronimo.tistory.com/101">[논문이해] Lost in the Middle: How Language Models Use Long Contexts</a></li>
</ul>
:ET