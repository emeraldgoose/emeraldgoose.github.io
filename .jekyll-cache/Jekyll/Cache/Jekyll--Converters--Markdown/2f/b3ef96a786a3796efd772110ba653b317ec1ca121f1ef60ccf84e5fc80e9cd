I"Ø<blockquote>
  <p>ì½”ë”©í…ŒìŠ¤íŠ¸ë¡œ Pythonìœ¼ë¡œë§Œ MLPë¥¼ êµ¬í˜„í•˜ëŠ” ë¬¸ì œê°€ ë‚˜ì™”ë˜ ì ì´ ìˆìŠµë‹ˆë‹¤. ë‹¹ì‹œì— ì—­ì „íŒŒ êµ¬í˜„ì„ í•˜ì§€ ëª»í•´ ì½”ë”©í…ŒìŠ¤íŠ¸ì—ì„œ ë–¨ì–´ì¡Œì—ˆê³  ì™„ì „íˆ ë°”ë‹¥ì—ì„œë¶€í„° êµ¬í˜„í•´ë³´ê³ ì ì‹œì‘í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.</p>
</blockquote>

<h2 id="multi-layer-perceptron">Multi-Layer Perceptron</h2>
<p>Multi-Layer Perceptron(MLP)ì€ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì¸µ(layer)ë“¤ì´ ìŒ“ì—¬ ì‹ ê²½ë§ì„ ì´ë£¨ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. êµ¬í˜„ì´ ê°„ë‹¨í•˜ê¸° ë•Œë¬¸ì— ë”¥ëŸ¬ë‹ì„ ë°”ë‹¥ë¶€í„° êµ¬í˜„í•˜ëŠ” í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•˜ëŠ”ë° ì¢‹ì€ ëª¨ë¸ì…ë‹ˆë‹¤.
ì €ëŠ” MNISTë¥¼ ë°ì´í„°ì…‹ìœ¼ë¡œ í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê³  classification taskë¥¼ ìˆ˜í–‰í•´ë³¼ ê²ƒì…ë‹ˆë‹¤.</p>

<h2 id="linear">Linear</h2>
<p>Linear ë ˆì´ì–´ëŠ” affine linear transformationì„ ìˆ˜í–‰í•˜ëŠ” ë ˆì´ì–´ì…ë‹ˆë‹¤. Linear ë ˆì´ì–´ë¥¼ fully-connected layer, dense layer ë“±ìœ¼ë¡œ ë¶€ë¥´ê¸°ë„ í•©ë‹ˆë‹¤.
Linear ë ˆì´ì–´ëŠ” weightì™€ biasë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë“¤ê³  ìˆìŠµë‹ˆë‹¤. weightëŠ” ì…ë ¥í•œ ë°ì´í„°ê°€ ì¶œë ¥ ë°ì´í„°ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ê²°ì •í•˜ëŠ” ê°’ì´ê³  biasëŠ” í•¨ìˆ˜ì˜ ì´ë™ì„ ë„ì™€ ì„ í˜•ì¡°í•©ë§Œìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ì—†ëŠ” ê²ƒì„ í•™ìŠµí•©ë‹ˆë‹¤.</p>

<h3 id="forward">Forward</h3>
<p>Linearì˜ ìˆ˜ì‹ì€ ê°„ë‹¨í•©ë‹ˆë‹¤.</p>

<p>$y = xW + b$</p>

<script src="https://gist.github.com/emeraldgoose/bf998f275c9582e38d80d9a3f20d78e3.js"></script>

<h3 id="backward">Backward</h3>
<p>ë¨¼ì €, forward ìˆ˜ì‹ì— ëŒ€í•´ í¸ë¯¸ë¶„ì„ í•˜ì—¬ ë‹¤ìŒì˜ ìˆ˜ì‹ë“¤ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>$\frac{\partial y}{\partial x} = W$</p>

<p>$\frac{\partial y}{\partial W} = x$</p>

<p>$\frac{\partial y}{\partial b} = 1$</p>

<p>ë‹¤ìŒ, chain ruleì— ì˜í•´ x, W, bì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial W}$</p>

<p>$\frac{\partial L}{\partial b} = \sum \frac{\partial L}{\partial y}$</p>

<p>$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial W}$</p>

<p>ì¶œë ¥ ë ˆì´ì–´ë¡œë¶€í„° ì˜¤ëŠ” upstream gradientë¥¼ $dz$ë¼ ê°€ì •í•˜ì—¬ ìˆ˜ì‹ì„ ì •ë¦¬í•©ë‹ˆë‹¤.</p>

<p>$\frac{\partial L}{\partial W} = x^Tdz$</p>

<p>$\frac{\partial L}{\partial b} = \sum dz$</p>

<p>$\frac{\partial L}{\partial x} = dzW^T$</p>

<script src="https://gist.github.com/emeraldgoose/0e2d30464acae480545d5e98a1cfd4dc.js"></script>

<h2 id="sigmoid">Sigmoid</h2>
<p>SigmoidëŠ” ì…ë ¥ê°’ì„ 0ê³¼ 1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í™œì„±í™” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë§¤ìš° í° ê°’ì€ 1ë¡œ ê·¼ì‚¬í•˜ê³  ë§¤ìš° ì‘ì€ ê°’ì€ 0ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤.</p>

<p>Sigmoidì™€ ê°™ì€ í™œì„±í™”í•¨ìˆ˜ë“¤ì€ ë¹„ì„ í˜•ì„±(Non-linear)ì„ ê°€ì§€ê³  ìˆì–´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ë¹„ì„ í˜•ì„±ì„ ì¶”ê°€í•˜ì—¬ ë³µì¡í•œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ”ë° ë„ì™€ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Sigmoidì˜ ë˜ë‹¤ë¥¸ íŠ¹ì§•ì€ element-wiseí•œ ì ì…ë‹ˆë‹¤. ì…ë ¥ ê°ê°ì— sigmoid ì—°ì‚°ì´ ì ìš©ë˜ì–´ ê° ì—°ì‚°ë“¤ì´ independentí•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì—­ì „íŒŒ ì‹œ í†µê³¼ë˜ëŠ” ì—°ì‚°ë„ independentí•˜ê²Œ ë©ë‹ˆë‹¤.</p>

<h3 id="forward-1">Forward</h3>
<p>Sigmoidì˜ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>

<p>$\sigma(x) = \frac{1}{1 + e^{-x}}$</p>

<script src="https://gist.github.com/emeraldgoose/647edf09aec3d969ab65bf76808e9dcb.js"></script>

<h3 id="backward-1">Backward</h3>
<p>Sigmoidë¥¼ ë¯¸ë¶„í•˜ê²Œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>

<p>$\frac{\partial \sigma}{\partial x} = \sigma(x)(1 - \sigma(x))$</p>

<p>ìœ„ì—ì„œ ì„¤ëª…í•œëŒ€ë¡œ backwardì‹œ element-wise ì—°ì‚°ì´ í•„ìš”í•˜ë¯€ë¡œ numpy.multiplyë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.</p>

<script src="https://gist.github.com/emeraldgoose/449960a17ab916d2d36d3657e900f143.js"></script>

<h2 id="ê²°ê³¼">ê²°ê³¼</h2>
<p>MNIST 5000ì¥ì„ í›ˆë ¨ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê³  1000ì¥ì„ í…ŒìŠ¤íŠ¸ë°ì´í„°ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.</p>

<script src="https://gist.github.com/emeraldgoose/d11ab0c99747c51f0050001749de89a4.js"></script>

<p>ëª¨ë¸ì€ Linear -&gt; Sigmoid -&gt; Dropout(0.3) -&gt; Linear -&gt; Sigmoid -&gt; Linearìœ¼ë¡œ ì´ì–´ì§€ë„ë¡ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.</p>

<p><img src="https://onedrive.live.com/embed?resid=502FD124B305BA80%213207&amp;authkey=%21AHbDw6fwQLOIC2Y&amp;width=608&amp;height=604" alt="" width="400" />
<img src="https://onedrive.live.com/embed?resid=502FD124B305BA80%213206&amp;authkey=%21ANIblIieb6OZcuE&amp;width=601&amp;height=604" alt="" width="400" /></p>

<p>10 ì—í¬í¬ì—ë„ lossê°€ ì˜ ë–¨ì–´ì§€ê³  Accuracyë„ ì˜ ì¦ê°€í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<h2 id="ì½”ë“œ">ì½”ë“œ</h2>
<p><a href="https://github.com/emeraldgoose/hcrot">https://github.com/emeraldgoose/hcrot</a></p>

<h2 id="reference">Reference</h2>
<ul>
  <li><a href="http://taewan.kim/post/sigmoid_diff/">http://taewan.kim/post/sigmoid_diff/</a></li>
  <li><a href="https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/">https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/</a></li>
  <li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">https://pytorch.org/docs/stable/generated/torch.nn.Linear.html</a></li>
  <li><a href="https://velog.io/@gjtang/Softmax-with-Loss-%EA%B3%84%EC%B8%B5-%EA%B3%84%EC%82%B0%EA%B7%B8%EB%9E%98%ED%94%84">https://velog.io/@gjtang/Softmax-with-Loss-%EA%B3%84%EC%B8%B5-%EA%B3%84%EC%82%B0%EA%B7%B8%EB%9E%98%ED%94%84</a></li>
  <li><a href="https://aew61.github.io/blog/artificial_neural_networks/1_background/1.b_activation_functions_and_derivatives.html">https://aew61.github.io/blog/artificial_neural_networks/1_background/1.b_activation_functions_and_derivatives.html</a></li>
</ul>
:ET