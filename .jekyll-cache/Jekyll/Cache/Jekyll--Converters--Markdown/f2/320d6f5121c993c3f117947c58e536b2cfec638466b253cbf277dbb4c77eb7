I"|N<h2 id="미분">미분</h2>
<ul>
  <li>함수 f의 주어진 점(x,f(x))에서의 미분을 통해 접선의 기울기를 구할 수 있다. 접선의 기울기는 어느 방향으로 점을 움직여야 함수값이 증가하는지/감소하는지 알 수 있습니다.</li>
  <li>미분값을 더하면 경사상승법(gradient ascent)라 하며 극댓값의 위치를 구할 때 사용합니다.</li>
  <li>미분값을 빼면 경사하강법(gradient descent)라 하며 극소값의 위치를 구할 때 사용합니다.</li>
  <li>경사상승법과 경사하강법 모두 극값에 도착하면 움직임을 멈추게 됩니다. (미분값이 0이므로)</li>
</ul>

<h2 id="경사하강법">경사하강법</h2>
<ul>
  <li>경사하강법(Gradient Descent)는 함수값이 낮아지는 방향으로 독립변수 값을 변형시켜가며 최종적으로 극값에 이를때까지 반복하는 방법입니다.</li>
  <li>최적화 할 함수 $f(x)$에 대하여 먼저 시작점 $x_0$를 정합니다. 현재 위치 $x_i$에서 다음 위치인 $x_{i+1}$은 다음과 같이 계산됩니다.
    <ul>
      <li>$x_{i+1} = x_i - \gamma_{i} \nabla f(x_i)$</li>
      <li>$\gamma_i $는 이동할 거리를 조절하는 매개변수입니다.</li>
    </ul>
  </li>
</ul>

<h3 id="경사하강법-알고리즘">경사하강법: 알고리즘</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
input: gradient(미분을 계산하는 함수), init(시작점), lr(학습률), eps(종료조건 위한 상수) 
output: var
"""</span>
<span class="n">var</span> <span class="o">=</span> <span class="n">init</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
<span class="k">while</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">):</span> <span class="c1"># 컴퓨터가 계산할 때 미분이 정확히 0이 되는 것은 불가능하므로 eps보다 작을 때 종료한다.
</span>  <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span> <span class="c1"># 다음 위치 = 현재 위치 - 이동거리 * 기울기
</span>  <span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">var</span><span class="p">)</span> <span class="c1"># 움직인 현재 위치에서 다시 미분값을 계산한다.
</span></code></pre></div></div>

<h2 id="변수가-벡터인-경우">변수가 벡터인 경우</h2>
<ul>
  <li>벡터가 입력인 경우 편미분(partial differentiataion)을 사용합니다.</li>
  <li>각 변수별로 편미분을 계산한 그레디언트 벡터(gradient vector, $\nabla f$)를 이용하여 경사하강법/경사상승법에 사용할 수 있습니다.
    <ul>
      <li>$\nabla f = (\partial x_1f, \partial x_2f,…,\partial x_df)$</li>
    </ul>
  </li>
</ul>

<h3 id="경사하강법-알고리즘-1">경사하강법: 알고리즘</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
input: gradient, init, lr, eps
output: var
"""</span>
<span class="n">var</span> <span class="o">=</span> <span class="n">init</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
<span class="k">while</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">):</span> <span class="c1"># 벡터는 절댓값 대신 노름을 계산해서 종료조건을 설정한다
</span>  <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>
  <span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="경사하강법으로-선형회귀-계수-구하기">경사하강법으로 선형회귀 계수 구하기</h2>
<ul>
  <li>데이터를 선형모델(linear model)로 해석하면 $X\beta=\hat{y} \approx y$이므로 $\beta$에 대해서 다음과 같이 정리할 수 있습니다.
    <ul>
      <li>$\beta$를 최소화하는 $min||y-\hat{y}||_2$ 식을 얻을 수 있습니다.</li>
    </ul>
  </li>
  <li>따라서 선형회귀의 목적식은 $min||y-\hat{y}||_2$이고 이를 최소화하는 $\beta$를 찾아야 하므로 다음과 같은 그레디언트 벡터를 구해야 합니다.
    <ul>
      <li>$\nabla_\beta ||y-X\beta|_2 = (\partial_{\beta_1}||y-X\beta||_2,…,\partial_{\beta_d}||y-X\beta||_2)$</li>
      <li>$||y-X\beta||_2$말고 $||y-X\beta||_2^2$를 최적화 해도 좋습니다.</li>
    </ul>
  </li>
  <li>$\beta$에 대해 미분하게 되면 $\partial_{\beta_k}||y-X\beta||_2 = -\frac{X^T(y-X\beta)}{n||y-X\beta||_2}$인 식을 얻을 수 있습니다.</li>
  <li>목적식을 최소화하는 $\beta$를 구하는 경사하강법 식은 다음과 같습니다.
    <ul>
      <li>$\beta^{(t+1)} \leftarrow \beta^{(t)}-\lambda \nabla_\beta ||y-X\beta^{(t)}||_2$</li>
      <li>위의 식을 정리하면 $\beta^{(t+1)} \leftarrow \beta^{(t)}+\frac{2\lambda}{n}X^T(y-X\beta^{(t)})$인 식을 얻을 수 있습니다.</li>
    </ul>
  </li>
</ul>

<h3 id="경사하강법-기반-선형회귀-알고리즘">경사하강법 기반 선형회귀 알고리즘</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
input: X, y, lr, T(학습횟수)
Output: beta
"""</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
  <span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span> <span class="c1"># (y - Xb)
</span>  <span class="n">grad</span> <span class="o">=</span> <span class="o">-</span> <span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">error</span> 
  <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>
</code></pre></div></div>

<h3 id="코드로-구현">코드로 구현</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="c1"># print('numpy version : {\%\s}'%(np.__version__))
</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span> <span class="c1"># 1000개의 랜덤 x값
</span><span class="n">train_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span> <span class="c1"># y값의 초기화
</span>
<span class="c1"># f(x) = 5x+3으로 가정
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">train_y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">train_x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">3</span>

<span class="c1"># parameter
</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span>

<span class="n">lr_rate</span> <span class="o">=</span> <span class="mf">1e-2</span> <span class="c1"># learning rate
</span><span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
  <span class="n">pred_y</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">train_x</span> <span class="o">+</span> <span class="n">b</span> <span class="c1"># prediction
</span>
  <span class="c1"># gradient
</span>  <span class="n">grad_w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">pred_y</span> <span class="o">-</span> <span class="n">train_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">train_x</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span> <span class="c1"># (1)
</span>  <span class="n">grad_b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">pred_y</span> <span class="o">-</span> <span class="n">train_y</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span> <span class="c1"># (2)
</span>
  <span class="c1"># update
</span>  <span class="n">w</span> <span class="o">-=</span> <span class="n">lr_rate</span> <span class="o">*</span> <span class="n">grad_w</span>
  <span class="n">b</span> <span class="o">-=</span> <span class="n">lr_rate</span> <span class="o">*</span> <span class="n">grad_b</span>

  <span class="c1"># error
</span>  <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">pred_y</span> <span class="o">-</span> <span class="n">train_y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
  <span class="n">errors</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'w: {}, b: {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">))</span> <span class="c1"># w: 5.008559005692836, b: 1.9170264334235934
</span></code></pre></div></div>

<ul>
  <li>grad_w, grad_b를 구하는 방법
    <ul>
      <li>error = $\frac{1}{n}(\hat{y}-y)^2$인데 이것을 $w$,$b$에 대해 편미분하면 다음과 같다.</li>
      <li>$\frac{\partial err}{\partial w}=\frac{2}{n}(\hat{y}-y)X$</li>
      <li>$\frac{\partial err}{\partial w}=\frac{2}{n}(\hat{y}-y)$</li>
      <li>따라서 $dw=((\hat{y}-y)X).mean$, $db=(\hat{y}-y).mean$이 된다.</li>
    </ul>
  </li>
  <li>에러를 그래프로 출력하면 다음과 같습니다.
<img src="https://drive.google.com/uc?export=view&amp;id=1WEEOKyJg8Tv43sd0ZoqQ9WRhjeCRNQoz" alt="" /></li>
</ul>

<h2 id="경사하강법은-만능">경사하강법은 만능?</h2>
<ul>
  <li>이론적으로 경사하강법은 미분가능하고 볼록(convex)한 함수에 대해선 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장됩니다.</li>
  <li>그러나 비선형회귀 문제의 경우 목적식이 볼록하지 않기 때문에 수렴이 항상 보장되지 않습니다.
    <ul>
      <li>극소값의 위치를 찾았어도 최솟값의 위치가 아닐 수 있습니다.</li>
    </ul>
  </li>
  <li>이를 위해 확률적 경사하강법(stochastic gradient descent)를 사용하게 됩니다.</li>
</ul>

<h2 id="확률적-경사하강법">확률적 경사하강법</h2>
<ul>
  <li>확률적 경사하강법은 데이터 한 개 또는 일부 활용하여 업데이트 합니다.</li>
  <li>볼록이 아닌(non-convex) 목적식도 SGD를 통해 최적화할 수 있습니다.</li>
  <li>$\theta^{(t+1)} \leftarrow \theta^{(t)}-\hat{\nabla_\theta L(\theta^{(t)})}, \space E[\hat{\nabla_\theta L}] \approx \nabla_\theta L$</li>
  <li>데이터의 일부를 가지고 패러미터를 업데이트하기 때문에 연산자원을 좀 더 효율적으로 활용할 수 있습니다.</li>
</ul>

<h3 id="확률적-경사하강법의-원리-미니배치-연산">확률적 경사하강법의 원리: 미니배치 연산</h3>
<ul>
  <li>경사하강법은 전체데이터 $D = (X,y)$를 가지고 목적식의 그레디언트 벡터를 계산합니다.</li>
  <li>SGD는 미니배치 $D_{(b)}=(X_{(b)},y_{(b)}) \subset D$를 가지고 그레디언트 벡터를 계산합니다.</li>
  <li>매번 다른 미니배치를 사용하기 때문에 목적식 모양(곡선 모양)이 바뀌게 됩니다.
    <ul>
      <li>경사하강법의 경우 error가 줄어들 때 곡선형태로 줄어들지만, SGD는 요동치면서 줄어드는 것을 볼 수 있습니다.</li>
    </ul>
  </li>
  <li>또한, SGD는 볼록이 아닌 목적식에서도 사용가능하므로 경사하강법보다 머신러닝 학습에 더 효율적입니다.</li>
</ul>

<h3 id="코드로-구현-1">코드로 구현</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="c1"># print('numpy version : {\%\s}'%(np.__version__))
</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>

<span class="c1"># f(x) = 5x+3으로 가정
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">train_y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">train_x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">3</span>

<span class="c1"># parameter
</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span>

<span class="n">lr_rate</span> <span class="o">=</span> <span class="mf">1e-2</span> <span class="c1"># learning rate
</span><span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
  <span class="c1"># mini batch
</span>  <span class="c1"># 1000개 중 10개를 랜덤하게 뽑아 미니배치 생성
</span>  <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="n">mini_x</span> <span class="o">=</span> <span class="n">train_x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
  <span class="n">mini_y</span> <span class="o">=</span> <span class="n">train_y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

  <span class="n">pred_y</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">mini_x</span> <span class="o">+</span> <span class="n">b</span> <span class="c1"># prediction
</span>
  <span class="c1"># gradient
</span>  <span class="n">grad_w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">pred_y</span> <span class="o">-</span> <span class="n">mini_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">mini_x</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">mini_x</span><span class="p">)</span>
  <span class="n">grad_b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">pred_y</span> <span class="o">-</span> <span class="n">mini_y</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">mini_x</span><span class="p">)</span>

  <span class="c1"># update
</span>  <span class="n">w</span> <span class="o">-=</span> <span class="n">lr_rate</span> <span class="o">*</span> <span class="n">grad_w</span>
  <span class="n">b</span> <span class="o">-=</span> <span class="n">lr_rate</span> <span class="o">*</span> <span class="n">grad_b</span>

  <span class="c1"># error
</span>  <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">pred_y</span> <span class="o">-</span> <span class="n">mini_y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">mini_x</span><span class="p">)</span>
  <span class="n">errors</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'w: {}, b: {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">))</span> <span class="c1"># w: 5.008246440224214, b: 1.9408158614474957
</span></code></pre></div></div>

<ul>
  <li>에러를 그래프로 출력하면 다음과 같다.
<img src="https://drive.google.com/uc?export=view&amp;id=1oTfDOO8oFBbyWZim8zhkXazqs24DU70d" alt="" /></li>
</ul>
:ET