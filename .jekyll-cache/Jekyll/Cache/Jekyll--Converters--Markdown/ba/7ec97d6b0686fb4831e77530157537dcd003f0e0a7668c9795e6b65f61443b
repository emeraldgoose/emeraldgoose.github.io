I"<blockquote>
  <p>모두의연구소에서 논문저자가 직접 논문을 리뷰해주는 세미나가 열렸습니다. 주제가 재밌어 보여 세미나에 참가하여 발표를 듣고 논문을 다시 읽어보고 리뷰하려고 합니다.</p>
</blockquote>

<h2 id="motivation">Motivation</h2>
<p>GPT라는 Large Scale Langauge Model이 등장하면서 언어 모델의 새로운 시대를 열게 되었습니다. GPT에서 In-context Learning이라는 방식을 사용하는 점이 특징입니다.</p>

<h3 id="in-context-learning">In-context Learning</h3>
<p>In-context Learning은 사전학습 모델에 풀고자 하는 태스크를 input으로 넣는 방식을 말합니다. 예제에 따라 zero-shot, one-shot, few-shot learning으로 나뉘어집니다.</p>
<ul>
  <li>Task description + example(zero/one/few shot) -&gt; answer</li>
</ul>

<h2 id="task-definition">Task Definition</h2>
<p>모델은 HyperCLOVA를 사용하고 1.3B의 파라미터를 가지고 있습니다.</p>

<p>Corpus는 네이버블로그, 네이버카페, 네이버뉴스, 네이버댓글, 네이버지식인, 위키피디아, 모두의말뭉치를 사용합니다.</p>

<p>Downstream Task로는 NSMC(영화리뷰), KorQuAD, KLUE-YNAT(뉴스제목분류), AI hub(한영/영한 번역)로 진행합니다.</p>

<h2 id="experimental-results">Experimental Results</h2>
<h3 id="effect-of-corpus-size">Effect of Corpus size</h3>
<p><img src="https://drive.google.com/uc?export=view&amp;id=" alt="" /><br />
테이블에서 말뭉치의 종류에 따라 다르게 in-context learning 성능이 나타는 것을 볼 수 있습니다.</p>
<ul>
  <li>블로그 데이터(Blog)로 학습한 모델이 카페(Cafe)나 뉴스(News)로 학습한 모델보다 few-shot 성능이 ALL 모델에 근접합니다.
    <ul>
      <li>ALL 모델은 모든 데이터를 학습한 모델을 말합니다.</li>
    </ul>
  </li>
  <li>모두의말뭉치(Modu)</li>
</ul>

<h3 id="effect-of-combining-corpora">Effect of Combining Corpora</h3>

<h3 id="effect-of-domain-relevance">Effect of Domain Relevance</h3>

<h3 id="perplexity-and-downstream-task">Perplexity and Downstream Task</h3>

<h2 id="conclusion">Conclusion</h2>
:ET