I"<h1 id="objective">Objective</h1>
<p>앞에서 구현한 LayerNorm, MultiHeadAttention, GELU를 사용하고 이전에 구현해둔 Linear, Dropout, Softmax 클래스를 사용하여 Transformer 클래스를 구현하여 테스트해봅니다.</p>

<p>가장 바깥쪽에 위치한 Transformer부터 시작해서 EncoderLayer, DecoderLayer 순으로 설명하고자 합니다.</p>

<h1 id="transformer">Transformer</h1>
<p>Transformer 클래스의 구조는 TransformerEncoder와 TransformerDeocder로 구성됩니다. Transformer로 들어오는 입력은 인코더를 통해 인코딩되어 디코더의 입력으로 사용됩니다.</p>

<p><img src="https://lh3.googleusercontent.com/d/1gZ0C9THux083GBFOzvyd9J2nuQ2iAdPS" alt="" width="500" /></p>

<h2 id="forward">Forward</h2>
<p>Transformer 클래스를 구현하기 위해 TransformerEncoder와 TransformerDecoder에서 사용할 encoder_layer, encoder_norm, decoder_layer, decoder_norm을 선언합니다.</p>

<script src="https://gist.github.com/emeraldgoose/ba48acb781e5437b6585d18d57ecd83e.js"></script>

<h2 id="backward">Backward</h2>
<p>Backward에서는 Encoder와 Decoder의 backward 함수를 호출하고 리턴되는 기울기들을 저장합니다.</p>

<script src="https://gist.github.com/emeraldgoose/a9b4402ce637590185e56919604d6efe.js"></script>

<h1 id="transformerencoder">TransformerEncoder</h1>
<p>Transformer의 Encoder는 EncoderLayer들이 스택되어 있는 구조로 구현됩니다.</p>

<h2 id="forward-1">Forward</h2>
<p>Pytorch의 TransformerEncoder 클래스는 인코더 레이어를 <code class="language-plaintext highlighter-rouge">num_layers</code>만큼 복사하여 ModuleList로 구성합니다. Transformer 클래스에서 선언된 EncoderLayer를 <code class="language-plaintext highlighter-rouge">_get_clone</code> 함수에서 <code class="language-plaintext highlighter-rouge">copy.deepcopy()</code>로 복사하기 때문에 스택되어 있는 인코더 레이어들은 같은 초기 파라미터를 가지고 다른 기울기를 가지게 됩니다.</p>

<script src="https://gist.github.com/emeraldgoose/c3997f35f4a3e76a6da5d697109de5ae.js"></script>

<h2 id="backward-1">Backward</h2>
<p>Forward에서 반복문을 통해 순서대로 계산하고 있으므로 그 역순으로 Backward 함수를 불러 계산하고 각 레이어의 기울기를 저장합니다.</p>

<script src="https://gist.github.com/emeraldgoose/734b8145dc245a8199e3f3a271081ffe.js"></script>

<h1 id="transformerdecoder">TransformerDecoder</h1>
<p>Transformer의 Decoder는 DecoderLayer들이 스택되어 있는 구조로 구현됩니다. 다음 그림의 오른쪽 처럼 Decoder는 Output 임베딩과 인코딩 정보를 입력으로 받아 출력값을 계산합니다.</p>

<h2 id="forward-2">Forward</h2>
<p>forward 함수의 argument로 <code class="language-plaintext highlighter-rouge">tgt</code>와 <code class="language-plaintext highlighter-rouge">memory</code>가 있습니다. <code class="language-plaintext highlighter-rouge">tgt</code>는 output 임베딩을 말하고 <code class="language-plaintext highlighter-rouge">memory</code>는 인코더 출력을 말합니다.</p>

<p>Encoder 구현과 마찬가지로 Transformer 클래스에서 선언된 DecoderLayer를 복사하여 ModuleList로 구성하고 반복문을 통해 호출하여 계산합니다.</p>

<script src="https://gist.github.com/emeraldgoose/23d416bbbe0732af2d080e7c1aa4f1eb.js"></script>

<h2 id="backward-2">Backward</h2>
<p>Foward에서 반복문을 통해 순서대로 계산하고 있으므로 그 역순으로 Backward 함수를 불러 계산하고 각 레이어의 기울기를 저장합니다.</p>

<script src="https://gist.github.com/emeraldgoose/52a3b5ed4af3d7a7bbb7606455d9e39c.js"></script>

<h1 id="transformerencoderlayer">TransformerEncoderLayer</h1>
<p>Transformer의 인코딩을 담당하는 레이어입니다. TransformerEncoder로 들어온 입력은 EncoderLayer의 순서대로 처리되며 최종 출력은 Decoder에서 사용됩니다.</p>

<h2 id="forward-3">Forward</h2>
<p>계산 순서는 들어온 입력이 먼저 MultiheadAttention을 거치고 FeedForward 연산을 통해 인코딩됩니다. 각 결과는 Residual Connection 구조를 사용하여 입력과 더해준 후 Layer Normalization을 수행합니다.
<script src="https://gist.github.com/emeraldgoose/511db6e7427152cf747b55f2982e3570.js"></script></p>

<h2 id="backward-3">Backward</h2>
<p>Backward 연산은 Forward의 역순으로 진행되며 Forward에서 사용된 Residual Connection은 Backward에서는 upstream gradient와 더해지게 됩니다.
<script src="https://gist.github.com/emeraldgoose/5ce3f68358a1d4c2b1a0b9151981f1e7.js"></script></p>

<h1 id="transformerdecoderlayer">TransformerDecoderLayer</h1>
<p>Transformer의 디코딩을 담당하는 레이어입니다. 인코더의 출력을 디코더에서 사용하여 output 시퀀스 이후에 나올 토큰을 예측하게 됩니다.</p>

<h2 id="forward-4">Forward</h2>
<p>forward의 argument로 <code class="language-plaintext highlighter-rouge">tgt</code>와 <code class="language-plaintext highlighter-rouge">memory</code>가 있습니다. <code class="language-plaintext highlighter-rouge">tgt</code>는 output 임베딩 입력을 담당하고 <code class="language-plaintext highlighter-rouge">memory</code>는 인코더의 출력을 의미합니다. EncoderLayer와 마찬가지로 각 단계마다 Residual Connection 구조를 사용합니다.</p>

<script src="https://gist.github.com/emeraldgoose/d55c7cf1dda2b952a0e3ac3610f2f84d.js"></script>

<h2 id="backward-4">Backward</h2>
<p>Backward 연산은 Forward의 역순으로 진행되며 Residual Connection은 upstream gradient와 더해지게 됩니다.</p>

<script src="https://gist.github.com/emeraldgoose/be7a478bcd7b067516a81ca37e4d2239.js"></script>

<h1 id="result-1">Result 1</h1>
<p>이전 테스트와 마찬가지로 MNIST 5000장과 테스트 1000장으로 실험했습니다. hidden_size는 32, learning_rate는 1e-3, 10 epoch로 학습을 진행했습니다.</p>

<p>다음은 학습에 사용한 모델을 정의한 코드입니다.</p>

<script src="https://gist.github.com/emeraldgoose/b998ee81096e78ccc7694291df5f242e.js"></script>

<p>MNIST 이미지에 순서 정보를 주기 위해 positional encoding 정보를 추가했습니다. 그리고 Transformer의 출력값이 (batch_size, 28, embed_size)이므로 Linear 레이어로 통과시키게 되면 (batch_size, 28, 10)이 되어버리기 때문에 Flatten 레이어를 통해 (batch_size, 28 * embed_size)로 바꿔준 후 Linear 레이어를 통해 (batch_size, 10) 크기를 가진 logits 값으로 출력하도록 모델을 구성했습니다.</p>

<p>아래 그래프들은 학습시킨 결과입니다. 왼쪽 그래프는 loss, 오른쪽 그래프는 accuracy를 기록한 것입니다.</p>

<p><img src="https://lh3.googleusercontent.com/d/1epA5L2HrTdj0b9uFXM8Jn2mdtW-jI7X5" alt="" width="450" />
<img src="https://lh3.googleusercontent.com/d/1ePHIHmU0QPcUtIeOhHfGkU5EZkS0aJUM" alt="" width="450" /></p>

<p>hidden size가 크지 않았지만 잘 학습되는 것을 볼 수 있습니다. hidden size를 256으로 올리고 학습을 돌려보면 accuracy가 0.95 이상으로 올라가기도 합니다.</p>

<h1 id="result-2">Result 2</h1>

<h1 id="code">Code</h1>
<p><a href="https://github.com/emeraldgoose/hcrot">https://github.com/emeraldgoose/hcrot</a></p>
:ET