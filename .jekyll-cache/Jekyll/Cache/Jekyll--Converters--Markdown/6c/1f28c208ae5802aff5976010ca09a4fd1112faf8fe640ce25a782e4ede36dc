I"<blockquote>
  <p><a href="https://www.coursera.org/projects/finetuning-large-language-models-project">Finetuning Large Language Models - Deeplearning.ai</a></p>
</blockquote>

<h1 id="why-finetune">Why Finetune?</h1>

<p>Finetuningì€ GPT-3ì™€ ê°™ì€ ë²”ìš© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì±„íŒ…ì„ ì˜ í•  ìˆ˜ ìˆëŠ” ChatGPT í˜¹ì€ ìë™ìœ¼ë¡œ ì½”ë“œë¥¼ ì™„ì„±í•˜ëŠ” co-pilotê³¼ ê°™ì€ ëª¨ë¸ë¡œ ì „ë¬¸í™”í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. finetuningì€ ëª¨ë¸ì„ ë³´ë‹¤ ì¼ê´€ëœ ë™ì‘ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ê²ƒ ì™¸ì—ë„, í™˜ê°(hallucination)ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>finetuningì€ prompt engineeringê³¼ ì°¨ì´ì ì´ ì¡´ì¬í•©ë‹ˆë‹¤. prompt engineeringì€ ì‹œì‘í•˜ëŠ”ë° ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ˆê¸°ë¹„ìš©ì´ ì ê²Œ ë“ ë‹¤ëŠ” ì¥ì ì´ ìˆê³  ì‹œì‘í•˜ê¸° ìœ„í•œ ê¸°ìˆ ì  ì§€ì‹ì´ í•„ìš”í•˜ì§€ ì•Šì§€ë§Œ í™˜ê°ì— ëŒ€í•œ ë¬¸ì œê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ì´ë¯¸ í•™ìŠµí•œ ì˜ëª»ëœ ì •ë³´ë¥¼ ìˆ˜ì •í•˜ëŠ” ê²ƒì´ ì–´ë µê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ ì˜ëª»ëœ ì •ë³´ë¥¼ ì¶œë ¥í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.</p>

<p>finetuningì€ ë§ì€ ìˆ˜ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê³  ëª¨ë¸ì´ ìƒˆë¡œìš´ ì •ë³´ë¥¼ í•™ìŠµí•˜ëŠ”ë° ì¢‹ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ì „ì— í•™ìŠµí–ˆì„ ì˜ëª»ëœ ì •ë³´ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ì´ì „ì— í•™ìŠµë˜ì§€ ì•Šì€ ìµœê·¼ ì •ë³´ë¥¼ ì…ë ¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¢‹ì€ í’ˆì§ˆì˜ ë” ë§ì€ ë°ì´í„°ì™€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.</p>

<p>ë”°ë¼ì„œ, prompt engineeringì€ ë‹¤ì–‘í•œ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë‚˜ í”„ë¡œí† íƒ€ì…ì— ì í•©í•˜ê³  finetuningì€ ì—”í„°í”„ë¼ì´ì¦ˆì™€ í”„ë¡œë•ì…˜ì— ì í•©í•©ë‹ˆë‹¤.</p>

<p>finetuningì„ í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” 3ê°€ì§€ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.</p>

<ul>
  <li>Pytorch(Meta) : Low-level interface</li>
  <li>Huggingface : Pytorch ë³´ë‹¤ ê³ ìˆ˜ì¤€ì˜ ì¸í„°í˜ì´ìŠ¤</li>
  <li>Llama(Llamini) : ì„¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤‘ ê°€ì¥ ë†’ì€ ìˆ˜ì¤€ì˜ ì¸í„°í˜ì´ìŠ¤
    <ul>
      <li>ì—¬ê¸°ì„œëŠ” ì£¼ë¡œ Llamini ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Llamini Library
</span><span class="kn">from</span> <span class="nn">llama</span> <span class="kn">import</span> <span class="n">BasicModelRunner</span>
<span class="n">non_finetuned</span> <span class="o">=</span> <span class="n">BasicModelRunner</span><span class="p">(</span><span class="s">"meta-llama/Llama-2-7b-hf"</span><span class="p">)</span>
<span class="n">non_finetuned_output</span> <span class="o">=</span> <span class="n">non_finetuned</span><span class="p">(</span><span class="s">"Tell me how to train my dog to sit"</span><span class="p">)</span>

<span class="c1"># Output
# .
# Tell me how to train my dog to sit. 
# I have a 10 month old puppy and I want to train him to sit. 
# I have tried the treat method and he just sits there and looks at me like I am crazy. 
# I have tried the "sit" command and he just looks at me like I am crazy. 
# I have tried the "sit" command and he just looks at me like I am crazy. 
</span><span class="p">...</span>
<span class="c1"># I have tried the "sit" command and he just looks at me like I am crazy. 
# I have tried the "sit" command and he just looks
</span>
<span class="n">finetuned_model</span> <span class="o">=</span> <span class="n">BasicModelRunner</span><span class="p">(</span><span class="s">"meta-llama/Llama-2-7b-chat-hf"</span><span class="p">)</span>
<span class="n">finetuned_otuput</span> <span class="o">=</span> <span class="n">finetuned_model</span><span class="p">(</span><span class="s">"Tell me how to train my dog to sit"</span><span class="p">)</span>

<span class="c1"># Output
# on command.
# Training a dog to sit on command is a basic obedience command that can be achieved with patience, consistency, and positive reinforcement. Here's a step-by-step guide on how to train your dog to sit on command:
</span>
<span class="c1"># 1. Choose a quiet and distraction-free area: Find a quiet area with minimal distractions where your dog can focus on you.
# 2. Have treats ready: Choose your dog's favorite treats and have them ready to use as rewards.
# 3. Stand in front of your dog: Stand in front of your dog and hold a treat close to their nose.
# 4. Move the treat up and back: Slowly move the treat up and back, towards your dog's tail, while saying "sit" in a calm and clear voice.
# 5. Dog will sit: As you move the treat, your dog will naturally sit down to follow the treat. The moment their bottom touches the ground, say "good sit" and give them the treat.
# 6. Repeat the process: Repeat steps 3-5 several times, so your dog starts to associate the command "sit" with
</span></code></pre></div></div>

<h1 id="where-finetuning-fits-in">Where finetuning fits in</h1>

<p>Finetuningì€ ì‚¬ì „í›ˆë ¨(Pretrain) ë‹¨ê³„ ë’¤ì— ì§„í–‰í•©ë‹ˆë‹¤. ì‚¬ì „í›ˆë ¨ì€ ì „í˜€ ì„¸íŒ…ë˜ì§€ ì•Šì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©° ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ëª©í‘œë¥¼ ê°€ì§€ê³  í›ˆë ¨í•©ë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ â€˜wantsâ€™ ë’¤ì— â€˜uponâ€™ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨í•˜ë©° ë¼ë²¨ë§ë˜ì§€ ì•Šì€ ê±°ëŒ€í•œ ì–‘ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í•™ìŠµ ë°©ë²•ì„ Self-supervised Learningì´ë¼ í•©ë‹ˆë‹¤.</p>

<p>Finetuningì€ ëª¨ë¸ì˜ ë™ì‘ì„ ë³€í™”ì‹œí‚µë‹ˆë‹¤. ëª¨ë¸ ì‘ë‹µì„ ì¼ê´€ì„±ìˆê²Œ í•˜ê±°ë‚˜ ì§ˆë¬¸ì— ì¢€ ë” ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ë˜í•œ, ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ ë°œí˜„(?)ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€í™”ì— ë” ëŠ¥ìˆ™í•´ì ¸ ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•´ ì´ì•¼ê¸° í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ì´ì „ì—ëŠ” ì´ëŸ¬í•œ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ë§ì€ Prompt engineeringì„ í•´ì•¼ í–ˆì§€ë§Œ finetuningì€ ì‰½ê²Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.</p>

<p>Finetuningì„ ì²˜ìŒ í•˜ê²Œ ëœë‹¤ë©´ ì¶”ì²œí•˜ëŠ” ëª‡ê°€ì§€ ë‹¨ê³„ì— ëŒ€í•´ ì†Œê°œí•©ë‹ˆë‹¤.</p>

<ol>
  <li>Identify task(s) by prompt-engineering a large LLM</li>
  <li>Find tasks that you see an LLM doing ~OK at</li>
  <li>Pick one task</li>
  <li>Get ~1000 inputs and outputs for the task
    <ul>
      <li>Better than the ~OK from the LLM</li>
    </ul>
  </li>
  <li>Finetune a small LLM on this data</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">jsonlines</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="n">filename</span> <span class="o">=</span> <span class="s">"lamini_docs.jsonl"</span>
<span class="n">instruction_dataset_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_json</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">examples</span> <span class="o">=</span> <span class="n">instruction_dataset_df</span><span class="p">.</span><span class="n">to_dict</span><span class="p">()</span>

<span class="n">prompt_template_qa</span> <span class="o">=</span> <span class="s">"""### Question:
{question}

### Answer:
{answer}"""</span>

<span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s">"question"</span><span class="p">])</span>
<span class="n">finetuning_dataset_text_only</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">finetuning_dataset_question_answer</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_examples</span><span class="p">):</span>
  <span class="n">question</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s">"question"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
  <span class="n">answer</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s">"answer"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>

  <span class="n">text_with_prompt_template_qa</span> <span class="o">=</span> <span class="n">prompt_template_qa</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
    <span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">,</span>
    <span class="n">answer</span><span class="o">=</span><span class="n">answer</span>
  <span class="p">)</span>
  <span class="n">finetuning_dataset_text_only</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
    <span class="p">{</span>
      <span class="s">"text"</span><span class="p">:</span> <span class="n">text_with_prompt_template_qa</span>
    <span class="p">}</span>
  <span class="p">)</span>

  <span class="n">text_with_prompt_template_q</span> <span class="o">=</span> <span class="n">prompt_template_q</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
    <span class="n">question</span><span class="o">=</span><span class="n">question</span>
  <span class="p">)</span>
  <span class="n">finetuning_dataset_question_answer</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
    <span class="p">{</span>
      <span class="s">"question"</span><span class="p">:</span> <span class="n">text_with_prompt_template_q</span><span class="p">,</span>
      <span class="s">"answer"</span><span class="p">:</span> <span class="n">answer</span>
    <span class="p">}</span>
  <span class="p">)</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">finetuning_dataset_text_only</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Output
# {'text': '### Question:\n'
#          'What are the different types of documents available in the '
#          "repository (e.g., installation guide, API documentation, developer's "
#          'guide)?\n'
#          '\n'
#          '### Answer:\n'
#          'Lamini has documentation on Getting Started, Authentication, '
#          'Question Answer Model, Python Library, Batching, Error Handling, '
#          'Advanced topics, and class documentation on LLM Engine available at '
#          'https://lamini-ai.github.io/.'}
</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">finetuning_dataset_question_answer</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Output
# {'answer': 'Lamini has documentation on Getting Started, Authentication, '
#            'Question Answer Model, Python Library, Batching, Error Handling, '
#            'Advanced topics, and class documentation on LLM Engine available '
#            'at https://lamini-ai.github.io/.',
#  'question': '### Question:\n'
#              'What are the different types of documents available in the '
#              'repository (e.g., installation guide, API documentation, '
#              "developer's guide)?\n"
#              '\n'
#              '### Answer:'}
</span></code></pre></div></div>

<h1 id="instruction-finetuning">Instruction finetuning</h1>

<p>Finetuning ì¤‘ Instruction Finetuning(instruction-tuned, instruction-following)ì´ë¼ëŠ” finetuningì´ ìˆìŠµë‹ˆë‹¤. ì´ ë°©ì‹ì€ ëª¨ë¸ì„ ì±—ë´‡ê³¼ ê°™ì´ í–‰ë™í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. Instruction-following ë°ì´í„°ì…‹ì˜ ì˜ˆë¡œëŠ” FAQs, Customer support conversation, Slak ë©”ì‹œì§€ ë“±ì´ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ ë°ì´í„°ê°€ ì—†ë‹¤ë©´ LLMì„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ Non-QnA ë°ì´í„°ë¥¼ QnA ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ë„ ì¡´ì¬í•©ë‹ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Instruction-tuning
</span><span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">jsonlines</span>

<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="n">instruction_tuned_dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span>
	<span class="s">"tatsu-lab/alpaca"</span><span class="p">,</span> 
	<span class="n">split</span><span class="o">=</span><span class="s">"train"</span><span class="p">,</span> 
	<span class="n">streaming</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="c1"># Two prompt templates
</span><span class="n">prompt_template_with_input</span> <span class="o">=</span> <span class="s">"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:"""</span>

<span class="n">prompt_template_without_input</span> <span class="o">=</span> <span class="s">"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:"""</span>

<span class="c1"># Hydrate prompts (add data to prompts)
</span><span class="n">processed_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">top_m</span><span class="p">:</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">j</span><span class="p">[</span><span class="s">"input"</span><span class="p">]:</span>
    <span class="n">processed_prompt</span> <span class="o">=</span> <span class="n">prompt_template_without_input</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">instruction</span><span class="o">=</span><span class="n">j</span><span class="p">[</span><span class="s">"instruction"</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">processed_prompt</span> <span class="o">=</span> <span class="n">prompt_template_with_input</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">instruction</span><span class="o">=</span><span class="n">j</span><span class="p">[</span><span class="s">"instruction"</span><span class="p">],</span> <span class="nb">input</span><span class="o">=</span><span class="n">j</span><span class="p">[</span><span class="s">"input"</span><span class="p">])</span>

  <span class="n">processed_data</span><span class="p">.</span><span class="n">append</span><span class="p">({</span><span class="s">"input"</span><span class="p">:</span> <span class="n">processed_prompt</span><span class="p">,</span> <span class="s">"output"</span><span class="p">:</span> <span class="n">j</span><span class="p">[</span><span class="s">"output"</span><span class="p">]})</span>

<span class="c1"># Save data to jsonl
</span><span class="k">with</span> <span class="n">jsonlines</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s">'alpaca_processed.jsonl'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
    <span class="n">writer</span><span class="p">.</span><span class="n">write_all</span><span class="p">(</span><span class="n">processed_data</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">llama</span> <span class="kn">import</span> <span class="n">BasicModelRunner</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_input_tokens</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">max_output_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="c1"># Tokenize
</span>  <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">,</span>
    <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_input_tokens</span>
  <span class="p">)</span>

  <span class="c1"># Generate
</span>  <span class="n">device</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">device</span>
  <span class="n">generated_tokens_with_prompt</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_output_tokens</span>
  <span class="p">)</span>

  <span class="c1"># Decode
</span>  <span class="n">generated_text_with_prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_tokens_with_prompt</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="c1"># Strip the prompt
</span>  <span class="n">generated_text_answer</span> <span class="o">=</span> <span class="n">generated_text_with_prompt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):]</span>

  <span class="k">return</span> <span class="n">generated_text_answer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"EleutherAI/pythia-70m"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"EleutherAI/pythia-70m"</span><span class="p">)</span>

<span class="n">finetuning_dataset_path</span> <span class="o">=</span> <span class="s">"lamini/lamini_docs"</span>
<span class="n">finetuning_dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">finetuning_dataset_path</span><span class="p">)</span>

<span class="n">test_sample</span> <span class="o">=</span> <span class="n">finetuning_dataset</span><span class="p">[</span><span class="s">"test"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">inference</span><span class="p">(</span><span class="n">test_sample</span><span class="p">[</span><span class="s">"question"</span><span class="p">],</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>

<span class="c1"># Output
# I have a question about the following:
# 
# How do I get the correct documentation to work?
# 
# A:
# 
# I think you need to use the following code:
# 
# A:
# 
# You can use the following code to get the correct documentation.
# 
# A:
# 
# You can use the following code to get the correct documentation.
# 
# A:
# 
# You can use the following
</span></code></pre></div></div>
<h1 id="data-preparation">Data preparation</h1>

<p>ë°ì´í„° ì¤€ë¹„ì— í•„ìš”í•œ ëª‡ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.</p>

<ol>
  <li>ê³ í’ˆì§ˆì˜ ë°ì´í„°</li>
  <li>ë°ì´í„°ì˜ ë‹¤ì–‘ì„±</li>
  <li>ë°ì´í„°ì˜ ì–‘
    <ul>
      <li>ë°ì´í„°ì˜ ì–‘ë³´ë‹¤ í’ˆì§ˆì´ ë” ì¤‘ìš”í•¨</li>
    </ul>
  </li>
</ol>

<p>ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ìŠ¤í…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>

<ol>
  <li>instruction-response ìŒì„ ìˆ˜ì§‘í•œë‹¤.</li>
  <li>ì´ëŸ¬í•œ ìŒì„ ì—°ê²°(concatenate)í•˜ê±°ë‚˜ Prompt í…œí”Œë¦¿ì„ ì¶”ê°€í•œë‹¤.</li>
  <li>ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ê³  íŒ¨ë”©ì„ ì¶”ê°€í•˜ê±°ë‚˜ ë°ì´í„°ë¥¼ ì˜ë¼ ì•Œë§ì€ í¬ê¸°ë¡œ ëª¨ë¸ì— ì…ë ¥í•˜ë„ë¡ í•œë‹¤.</li>
  <li>í•´ë‹¹ ë°ì´í„°ë¥¼ trainê³¼ testë¡œ ë¶„ë¦¬í•œë‹¤.</li>
</ol>

<p>í† í¬ë‚˜ì´ì§•ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ê°ê°ì˜ í…ìŠ¤íŠ¸ ì¡°ê°ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì§• ë„êµ¬ê°€ ìˆìœ¼ë©° ëª¨ë¸ë“¤ì€ í›ˆë ¨ëœ íŠ¹ì • í† í¬ë‚˜ì´ì €ì™€ ì—°ê´€ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì˜ëª»ëœ í† í¬ë‚˜ì´ì €ë¥¼ ëª¨ë¸ì— ì‚¬ìš©í•˜ë©´ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì ë° ë‹¨ì–´ë¥¼ ë‚˜íƒ€ë‚´ì–´ ëª¨ë¸ì´ í˜¼ë€ìŠ¤ëŸ¬ì›Œì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">datasets</span>

<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"EleutherAI/pythia-70m"</span><span class="p">)</span>

<span class="n">filename</span> <span class="o">=</span> <span class="s">"lamini_docs.jsonl"</span>
<span class="n">instruction_dataset_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_json</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">examples</span> <span class="o">=</span> <span class="n">instruction_dataset_df</span><span class="p">.</span><span class="n">to_dict</span><span class="p">()</span>

<span class="n">prompt_template</span> <span class="o">=</span> <span class="s">"""### Question:
{question}

### Answer:"""</span>

<span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s">"question"</span><span class="p">])</span>
<span class="n">finetuning_dataset</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_examples</span><span class="p">):</span>
  <span class="n">question</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s">"question"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
  <span class="n">answer</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s">"answer"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
  <span class="n">text_with_prompt_template</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">)</span>
  <span class="n">finetuning_dataset</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
    <span class="p">{</span>
      <span class="s">"question"</span><span class="p">:</span> <span class="n">text_with_prompt_template</span><span class="p">,</span> 
      <span class="s">"answer"</span><span class="p">:</span> <span class="n">answer</span>
    <span class="p">}</span>
  <span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
  <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token</span>
  <span class="n">tokenized_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s">"np"</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="p">)</span>

  <span class="n">max_length</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
    <span class="n">tokenized_inputs</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="mi">2048</span>
  <span class="p">)</span>
  <span class="n">tokenizer</span><span class="p">.</span><span class="n">truncation_side</span> <span class="o">=</span> <span class="s">"left"</span>
  <span class="n">tokenized_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s">"np"</span><span class="p">,</span>
    <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span>
  <span class="p">)</span>

  <span class="k">return</span> <span class="n">tokenized_inputs</span>

<span class="n">finetuning_dataset_loaded</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span>
  <span class="s">"json"</span><span class="p">,</span> 
  <span class="n">data_files</span><span class="o">=</span><span class="n">filename</span><span class="p">,</span> 
  <span class="n">split</span><span class="o">=</span><span class="s">"train"</span>
<span class="p">)</span>

<span class="n">tokenized_dataset</span> <span class="o">=</span> <span class="n">finetuning_dataset_loaded</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span>
  <span class="n">tokenize_function</span><span class="p">,</span>
  <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">drop_last_batch</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="n">tokenized_dataset</span> <span class="o">=</span> <span class="n">tokenized_dataset</span><span class="p">.</span><span class="n">add_column</span><span class="p">(</span><span class="s">"labels"</span><span class="p">,</span> <span class="n">tokenized_dataset</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">])</span>
<span class="n">split_dataset</span> <span class="o">=</span> <span class="n">tokenized_dataset</span><span class="p">.</span><span class="n">train_test_split</span><span class="p">(</span>
  <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
  <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
  <span class="n">seed</span><span class="o">=</span><span class="mi">123</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">split_dataset</span><span class="p">)</span>

<span class="c1"># Output
# DatasetDict({
#     train: Dataset({
#         features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],
#         num_rows: 1260
#     })
#     test: Dataset({
#         features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],
#         num_rows: 140
#     })
# })
</span></code></pre></div></div>

<h1 id="training-process">Training process</h1>
<p>ê¸°ì¡´ ì‹ ê²½ë§ì—ì„œì˜ í•™ìŠµì€ í•™ìŠµ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ê³  lossë¥¼ ê³„ì‚°í•˜ì—¬ ì—­ì „íŒŒê³¼ì •ì„ ê±°ì³ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” learning rate, learning rate scheduler, optimizer hyperparametersê°€ ëœë‹¤. ê·¸ë¦¬ê³  Pytorchì™€ llaminië¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ê°€ ì•„ë˜ ì½”ë“œì™€ ê°™ì€ í˜•íƒœë¡œ ì‘ì„±ë©ë‹ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Llamini
</span><span class="kn">from</span> <span class="nn">llama</span> <span class="kn">import</span> <span class="n">BasicModelRunner</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BasicModelRunner</span><span class="p">(</span><span class="s">"EleutherAI/pythia-410m"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">load_data_from_jsonlines</span><span class="p">(</span><span class="s">"lamini_docs.jsonl"</span><span class="p">,</span> <span class="n">input_key</span><span class="o">=</span><span class="s">"question"</span><span class="p">,</span> <span class="n">output_key</span><span class="o">=</span><span class="s">"answer"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">is_public</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">config</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">jsonlines</span>

<span class="kn">from</span> <span class="nn">utilities</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">llama</span> <span class="kn">import</span> <span class="n">BasicModelRunner</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="p">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>
<span class="n">global_config</span> <span class="o">=</span> <span class="bp">None</span>

<span class="c1"># Load the Lamini docs dataset
</span><span class="n">dataset_name</span> <span class="o">=</span> <span class="s">"lamini_docs.jsonl"</span>
<span class="n">dataset_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"/content/</span><span class="si">{</span><span class="n">dataset_name</span><span class="si">}</span><span class="s">"</span>
<span class="n">use_hf</span> <span class="o">=</span> <span class="bp">False</span>

<span class="n">dataset_path</span> <span class="o">=</span> <span class="s">"lamini/lamini_docs"</span>
<span class="n">use_hf</span> <span class="o">=</span> <span class="bp">True</span>

<span class="c1"># Set up model, training config, and tokenizer
</span><span class="n">model_name</span> <span class="o">=</span> <span class="s">"EleutherAI/pythia-70m"</span>
<span class="n">training_config</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">"model"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"pretrained_name"</span><span class="p">:</span> <span class="n">model_name</span><span class="p">,</span>
    <span class="s">"max_length"</span> <span class="p">:</span> <span class="mi">2048</span>
  <span class="p">},</span>
  <span class="s">"datasets"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"use_hf"</span><span class="p">:</span> <span class="n">use_hf</span><span class="p">,</span>
    <span class="s">"path"</span><span class="p">:</span> <span class="n">dataset_path</span>
  <span class="p">},</span>
  <span class="s">"verbose"</span><span class="p">:</span> <span class="bp">True</span>
<span class="p">}</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">tokenize_and_split_data</span><span class="p">(</span><span class="n">training_config</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

<span class="c1"># Load the base model
</span><span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">device_count</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span>
<span class="k">if</span> <span class="n">device_count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
  <span class="n">logger</span><span class="p">.</span><span class="n">debug</span><span class="p">(</span><span class="s">"Select GPU device"</span><span class="p">)</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">logger</span><span class="p">.</span><span class="n">debug</span><span class="p">(</span><span class="s">"Select CPU device"</span><span class="p">)</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">)</span>

<span class="n">base_model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Define function to carry out inference
</span><span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_input_tokens</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">max_output_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="c1"># Tokenize
</span>  <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">,</span>
    <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_input_tokens</span>
  <span class="p">)</span>

  <span class="c1"># Generate
</span>  <span class="n">device</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">device</span>
  <span class="n">generated_tokens_with_prompt</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_output_tokens</span>
  <span class="p">)</span>

  <span class="c1"># Decode
</span>  <span class="n">generated_text_with_prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_tokens_with_prompt</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="c1"># Strip the prompt
</span>  <span class="n">generated_text_answer</span> <span class="o">=</span> <span class="n">generated_text_with_prompt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):]</span>

  <span class="k">return</span> <span class="n">generated_text_answer</span>

<span class="c1"># Setup trainig
</span><span class="n">max_steps</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">trained_model_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"lamini_docs_</span><span class="si">{</span><span class="n">max_steps</span><span class="si">}</span><span class="s">_steps"</span>
<span class="n">output_dir</span> <span class="o">=</span> <span class="n">trained_model_name</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>

  <span class="c1"># Learning rate
</span>  <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0e-5</span><span class="p">,</span>

  <span class="c1"># Number of training epochs
</span>  <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

  <span class="c1"># Max steps to train for (each step is a batch of data)
</span>  <span class="c1"># Overrides num_train_epochs, if not -1
</span>  <span class="n">max_steps</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span>

  <span class="c1"># Batch size for training
</span>  <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

  <span class="c1"># Directory to save model checkpoints
</span>  <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">,</span>

  <span class="c1"># Other arguments
</span>  <span class="n">overwrite_output_dir</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># Overwrite the content of the output directory
</span>  <span class="n">disable_tqdm</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># Disable progress bars
</span>  <span class="n">eval_steps</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="c1"># Number of update steps between two evaluations
</span>  <span class="n">save_steps</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="c1"># After # steps model is saved
</span>  <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># Number of warmup steps for learning rate scheduler
</span>  <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># Batch size for evaluation
</span>  <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s">"steps"</span><span class="p">,</span>
  <span class="n">logging_strategy</span><span class="o">=</span><span class="s">"steps"</span><span class="p">,</span>
  <span class="n">logging_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">optim</span><span class="o">=</span><span class="s">"adafactor"</span><span class="p">,</span>
  <span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
  <span class="n">gradient_checkpointing</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>

  <span class="c1"># Parameters for early stopping
</span>  <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">metric_for_best_model</span><span class="o">=</span><span class="s">"eval_loss"</span><span class="p">,</span>
  <span class="n">greater_is_better</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>

<span class="n">model_flops</span> <span class="o">=</span> <span class="p">(</span>
  <span class="n">base_model</span><span class="p">.</span><span class="n">floating_point_ops</span><span class="p">(</span>
    <span class="p">{</span>
      <span class="s">"input_ids"</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">training_config</span><span class="p">[</span><span class="s">"model"</span><span class="p">][</span><span class="s">"max_length"</span><span class="p">])</span>
      <span class="p">)</span>
    <span class="p">}</span>
  <span class="p">)</span>
  <span class="o">*</span> <span class="n">training_args</span><span class="p">.</span><span class="n">gradient_accumulation_steps</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
  <span class="n">model</span><span class="o">=</span><span class="n">base_model</span><span class="p">,</span>
  <span class="n">model_flops</span><span class="o">=</span><span class="n">model_flops</span><span class="p">,</span>
  <span class="n">total_steps</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span>
  <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
  <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
  <span class="n">eval_dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Train a few steps
</span><span class="n">training_output</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># Save model locally
</span><span class="n">save_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">output_dir</span><span class="si">}</span><span class="s">/final'</span>

<span class="n">trainer</span><span class="p">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">save_dir</span><span class="p">)</span>

<span class="n">finetuned_slightly_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">local_files_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">finetuned_slightly_model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Finetune a model in 3 lines of code using Lamini
</span><span class="n">model</span> <span class="o">=</span> <span class="n">BasicModelRunner</span><span class="p">(</span><span class="s">"EleutherAI/pythia-410m"</span><span class="p">)</span> 
<span class="n">model</span><span class="p">.</span><span class="n">load_data_from_jsonlines</span><span class="p">(</span><span class="s">"lamini_docs.jsonl"</span><span class="p">,</span> <span class="n">input_key</span><span class="o">=</span><span class="s">"question"</span><span class="p">,</span> <span class="n">output_key</span><span class="o">=</span><span class="s">"answer"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">is_public</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="n">lofd</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">out</span><span class="p">[</span><span class="s">'eval_results'</span><span class="p">]:</span>
  <span class="n">q</span>  <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">e</span><span class="p">[</span><span class="s">'input'</span><span class="p">]</span><span class="si">}</span><span class="s">"</span>
  <span class="n">at</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">e</span><span class="p">[</span><span class="s">'outputs'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s">'output'</span><span class="p">]</span><span class="si">}</span><span class="s">"</span>
  <span class="n">ab</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">e</span><span class="p">[</span><span class="s">'outputs'</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="s">'output'</span><span class="p">]</span><span class="si">}</span><span class="s">"</span>
  <span class="n">di</span> <span class="o">=</span> <span class="p">{</span><span class="s">'question'</span><span class="p">:</span> <span class="n">q</span><span class="p">,</span> <span class="s">'trained model'</span><span class="p">:</span> <span class="n">at</span><span class="p">,</span> <span class="s">'Base Model'</span> <span class="p">:</span> <span class="n">ab</span><span class="p">}</span>
  <span class="n">lofd</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">di</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">lofd</span><span class="p">)</span>
<span class="n">style_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">set_properties</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="s">'text-align'</span><span class="p">:</span> <span class="s">'left'</span><span class="p">})</span>
<span class="n">style_df</span> <span class="o">=</span> <span class="n">style_df</span><span class="p">.</span><span class="n">set_properties</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="s">"vertical-align"</span><span class="p">:</span> <span class="s">"text-top"</span><span class="p">})</span>

<span class="c1"># Output
# question : Does Lamini have the ability to understand and generate code for audio processing tasks?
# trained model : Yes, Lamini has the ability to understand and generate code.
# Base Model : A: Lamini is a very good language for audio processing.\nA: I think...
</span></code></pre></div></div>

<h1 id="evaluation-and-iteration">Evaluation and iteration</h1>

<p>ëª¨ë¸ì„ í›ˆë ¨í•œ í›„ì—ëŠ” ë‹¤ìŒ ë‹¨ê³„ë¡œ ëª¨ë¸ì„ í‰ê°€í•˜ê³  ì„±ëŠ¥ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ëª¨ë¸ì„ ê°œì„ í•˜ëŠ”ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆëŠ” ë°˜ë³µì ì¸ ê³¼ì •ì…ë‹ˆë‹¤.</p>

<p>ìƒì„± ëª¨ë¸ì€ ëª…í™•í•œ ì¸¡ì • ì§€í‘œê°€ ì—†ì–´ í‰ê°€í•˜ëŠ” ê²ƒì´ ì–´ë µìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ Human expert evaluationì´ ì¢…ì¢… ê°€ì¥ ì‹ ë¢°ì„±ìˆëŠ” ë°©ë²•ì´ë©° í•´ë‹¹ ë„ë©”ì¸ì„ ì´í•´í•˜ëŠ” ì „ë¬¸ê°€ë“¤ì´ í‰ê°€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.</p>

<p>LLM ë²¤ì¹˜ë§ˆí¬ëŠ” ì—¬ëŸ¬ í‰ê°€ ë°©ë²•ì˜ ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.</p>

<ul>
  <li>ARC : ì´ˆë“±í•™êµ ì§ˆë¬¸ì˜ ëª¨ìŒ</li>
  <li>HellaSwag : ìƒì‹ í…ŒìŠ¤íŠ¸</li>
  <li>MMLU : ë‹¤ì–‘í•œ ì´ˆë“±í•™êµ ê³¼ëª©</li>
  <li>TruthfulQA : ëª¨ë¸ì´ ì˜¨ë¼ì¸ì—ì„œ í”íˆ ì°¾ì„ ìˆ˜ ìˆëŠ” ê±°ì§“ë§ì„ ì¬í˜„í•˜ëŠ” ëŠ¥ë ¥ì„ ì¸¡ì •</li>
</ul>

<p>ëª¨ë¸ì„ ë¶„ì„í•˜ê³  í‰ê°€í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë°©ë²• ì¤‘ â€œError Analysisâ€ê°€ ìˆìŠµë‹ˆë‹¤. Error AnalysisëŠ” ì—ëŸ¬ë¥¼ ë²”ì£¼í™”í•˜ì—¬ ë§¤ìš° ì¼ë°˜ì ì¸ ì—ëŸ¬ ìœ í˜•ì„ ì´í•´í•˜ê³  ê°€ì¥ í”í•œ ì—ëŸ¬ì™€ ë§¤ìš° ì¹˜ëª…ì ì¸ ì—ëŸ¬ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.</p>

<p>ëª¨ë¸ì„ finetuningí•˜ê¸° ì „ì— ê¸°ë³¸ ëª¨ë¸ì˜ ì—ëŸ¬ë¥¼ ë¶„ì„í•œ í›„ finetuningì„ í–ˆì„ ë•Œ ê°€ì¥ í° íš¨ê³¼ë¥¼ ì¤„ ìˆ˜ ìˆëŠ” ë°ì´í„° ì¢…ë¥˜ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>ì—ëŸ¬ì˜ ë²”ì£¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>

<ul>
  <li>Misspelling(ë§ì¶¤ë²• ì˜¤ë¥˜)</li>
  <li>Too long(ê¸¸ì´) : ë°ì´í„°ì…‹ì´ ëœ ì¥í™©í•˜ë„ë¡ í•˜ì—¬ ëª¨ë¸ì´ ì§ˆë¬¸ì— ëª…í™•í•˜ê²Œ ë‹µë³€í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.</li>
  <li>Repetitive(ë°˜ë³µ) : ëª¨ë¸ì´ ë°˜ë³µì ì¸ ì‘ë‹µì„ í•  ìˆ˜ ìˆëŠ”ë° ì´ë¥¼ í•´ê²°í•˜ëŠ” í•œ ê°€ì§€ ë°©ë²•ì€ stop tokenë“¤ì„ ì‚¬ìš©í•˜ê±°ë‚˜(eos í† í°ì„ ê°€ë¦¬í‚¤ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤) Prompt í…œí”Œë¦¿ì„ ì´ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë°ì´í„°ì…‹ì— ë°˜ë³µì´ ì ê³  ë‹¤ì–‘ì„±ì´ ìˆëŠ” ì˜ˆì œë¥¼ í¬í•¨í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•©ë‹ˆë‹¤.</li>
</ul>

<p>ì´ ê°•ì˜ì—ì„œëŠ” ë²¤ì¹˜ë§ˆí¬ì— ë„ˆë¬´ ì§‘ì°©í•˜ì§€ ë§ë¼ê³  í•©ë‹ˆë‹¤. ëª¨ë¸ì„ ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ” ë°©ì‹ì´ê¸´ í•˜ì§€ë§Œ, ì‹¤ì œ ì‚¬ìš©ì‚¬ë¡€ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ finetuningëœ ëª¨ë¸ì€ ë‹¤ì–‘í•œ taskì— ë§ê²Œ ì¡°ì •ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ë‹¤ì–‘í•œ í‰ê°€ ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">config</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">difflib</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">utilities</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="p">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>
<span class="n">global_config</span> <span class="o">=</span> <span class="bp">None</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s">"lamini/lamini_docs"</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s">"test"</span><span class="p">]</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s">"lamini/lamini_docs_finetuned"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Setup a really basic evaluation function
</span><span class="k">def</span> <span class="nf">is_exact_match</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">a</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">==</span> <span class="n">b</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span>

<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span> <span class="c1"># dropoutê³¼ ê°™ì€ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤
</span>
<span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_input_tokens</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">max_output_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="c1"># Tokenize
</span>  <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token</span>
  <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span>
      <span class="n">text</span><span class="p">,</span>
      <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">,</span>
      <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
      <span class="n">max_length</span><span class="o">=</span><span class="n">max_input_tokens</span>
  <span class="p">)</span>

  <span class="c1"># Generate
</span>  <span class="n">device</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">device</span>
  <span class="n">generated_tokens_with_prompt</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_output_tokens</span>
  <span class="p">)</span>

  <span class="c1"># Decode
</span>  <span class="n">generated_text_with_prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_tokens_with_prompt</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="c1"># Strip the prompt
</span>  <span class="n">generated_text_answer</span> <span class="o">=</span> <span class="n">generated_text_with_prompt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):]</span>

  <span class="k">return</span> <span class="n">generated_text_answer</span>

<span class="c1"># Run model and compare to expected answer
</span><span class="n">generated_answer</span> <span class="o">=</span> <span class="n">inference</span><span class="p">(</span><span class="n">test_question</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

<span class="n">answer</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s">"answer"</span><span class="p">]</span>

<span class="n">exact_match</span> <span class="o">=</span> <span class="n">is_exact_match</span><span class="p">(</span><span class="n">generated_answer</span><span class="p">,</span> <span class="n">answer</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">exact_match</span><span class="p">)</span>

<span class="c1"># Output
# False
# generated_answerì™€ answerê°€ ê°™ì€ì§€ ë‹¨ìˆœíˆ í™•ì¸í•˜ëŠ” ê°„ë‹¨í•œ ë°©ë²•ì…ë‹ˆë‹¤
# ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” generated_answerì™€ answerë¥¼ LLMì— ì…ë ¥í•˜ê³  ê°™ì€ ë‹µë³€ì¸ì§€ ì ìˆ˜ë¡œ ë§¤ê²¨ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ í™•ì¸í•  ìˆ˜ ìˆë‹¤
</span>
<span class="c1"># Run over entire dataset
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s">'exact_matches'</span><span class="p">:</span> <span class="p">[]}</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"i Evaluating: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">item</span><span class="p">))</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s">'question'</span><span class="p">]</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s">'answer'</span><span class="p">]</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="n">predicted_answer</span> <span class="o">=</span> <span class="n">inference</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
      <span class="k">continue</span>
    <span class="n">predictions</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">predicted_answer</span><span class="p">,</span> <span class="n">answer</span><span class="p">])</span>

    <span class="c1">#fixed: exact_match = is_exact_match(generated_answer, answer)
</span>    <span class="n">exact_match</span> <span class="o">=</span> <span class="n">is_exact_match</span><span class="p">(</span><span class="n">predicted_answer</span><span class="p">,</span> <span class="n">answer</span><span class="p">)</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s">'exact_matches'</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">exact_match</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">n</span> <span class="ow">and</span> <span class="n">n</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
      <span class="k">break</span>
</code></pre></div></div>

<h1 id="consideration-on-getting-started-now">Consideration on getting started now</h1>

<p>Finetuningê³¼ì •ì— ëŒ€í•œ ì‹¤ìš©ì ì¸ ì ‘ê·¼ì„ ì†Œê°œí•©ë‹ˆë‹¤.</p>

<ol>
  <li>Taskë¥¼ ë¨¼ì € ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.</li>
  <li>Taskì˜ ì…ë ¥ê³¼ ì¶œë ¥ì— ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.</li>
  <li>ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    <ul>
      <li>Prompt Template</li>
    </ul>
  </li>
  <li>ë¨¼ì € ì‘ì€ ëª¨ë¸ì„ finetuningí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    <ul>
      <li>4ì–µ(400M)ì—ì„œ 10ì–µ(1B) íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ì¶”ì²œí•©ë‹ˆë‹¤.</li>
    </ul>
  </li>
  <li>ë°ì´í„°ì˜ ì–‘ì„ ë³€ê²½í•˜ë©´ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.</li>
  <li>ëª¨ë¸ì„ í‰ê°€í•˜ë©´ì„œ ì˜ ë˜ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.</li>
  <li>ëª¨ë¸ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë” ë§ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.</li>
  <li>Taskë¥¼ ì¢€ ë” ë³µì¡í•˜ê²Œ í•´ë´…ë‹ˆë‹¤.
    <ul>
      <li>ê¸€ì“°ê¸° ì‘ì—…ì€ ì½ê¸° ì‘ì—…ë³´ë‹¤ ë” ë§ì€ í† í°ì„ ìƒì„±í•´ì•¼ í•˜ë¯€ë¡œ ì–´ë µìŠµë‹ˆë‹¤.</li>
      <li>ì—¬ëŸ¬ ê°€ì§€ ì‘ì—…ì„ ë¬¶ì–´ ëª¨ë¸ì´ ì—¬ëŸ¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.</li>
    </ul>
  </li>
  <li>ì„±ëŠ¥ì„ ìœ„í•´ ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë ¤ë´…ë‹ˆë‹¤.</li>
</ol>

<p>ì‘ì—… ë³µì¡ì„±ì— ë”°ë¥¸ ëª¨ë¸ í¬ê¸°ë¥¼ ê°ì•ˆí•  ë•Œ í•˜ë“œì›¨ì–´ì— ëŒ€í•œ ê³ ë ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. v100 1ì¥ì€ 16GB ë©”ëª¨ë¦¬ë¥¼ ê°€ì§€ê³  ìˆì–´ ì¶”ë¡ ì‹œ ìµœëŒ€ 7Bì˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ í›ˆë ¨ì‹œ ìµœëŒ€ 1B ëª¨ë¸ë§Œ ì‚¬ìš©ê°€ëŠ¥í•©ë‹ˆë‹¤. ë” í° ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ë¥¸ ì˜µì…˜ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.</p>

<p>ë” í° ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ëŠ” ê²½ìš° PEFT(Parameter-efficient Fine-tuning)ë‚˜ LoRa(Low-Rank Adaptation)ì™€ ê°™ì€ ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì¤‘ LoRaëŠ” main pretrained weightë“¤ì„ freezingí•˜ê³  ì¼ë¶€ ë ˆì´ì–´ì•ˆì— ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ìƒˆë¡­ê²Œ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ë¥¼ ë©”ì¸ ê°€ì¤‘ì¹˜ë¡œ ë‹¤ì‹œ ë³‘í•©í•˜ì—¬ fine-tuningëœ ëª¨ë¸ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
:ET