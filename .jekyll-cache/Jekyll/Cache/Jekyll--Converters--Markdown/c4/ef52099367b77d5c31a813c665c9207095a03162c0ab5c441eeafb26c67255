I"ŠA<h2 id="related">Related</h2>
<blockquote>
  <p>ì´ì „ í¬ìŠ¤íŠ¸ì—ì„œ MLPë¥¼ êµ¬í˜„í–ˆê³  ì´ë²ˆì—ëŠ” CNNì„ êµ¬í˜„í•˜ëŠ” ì‚½ì§ˆì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.</p>
</blockquote>

<h2 id="cnn">CNN</h2>
<p>CNNì€ [Conv2d + Pooling + (Activation)] ë ˆì´ì–´ê°€ ìˆ˜ì§ìœ¼ë¡œ ìŒ“ì—¬ìˆëŠ” ë‰´ëŸ´ë„·ì„ ë§í•©ë‹ˆë‹¤. 
êµ¬í˜„í•´ë³´ë ¤ëŠ” CNNì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<ul>
  <li>Layer1 : Conv2d(1, 5, 5) -&gt; ReLU -&gt; MaxPool2d(2, 2)</li>
  <li>Layer2 : Conv2d(5, 7, 5) -&gt; ReLU -&gt; MaxPool2d(2, 2)</li>
  <li>Flatten Layer</li>
  <li>Linear Layer</li>
</ul>

<h2 id="conv2d">Conv2d</h2>
<h3 id="convolution">Convolution</h3>
<p>ë‹¨ìˆœí•˜ê²Œ forë¬¸ ì¤‘ì²©ìœ¼ë¡œ Convolutionì„ êµ¬í˜„í•˜ë©´ ì—°ì‚°ì†ë„ê°€ ë„ˆë¬´ ëŠë ¤ì§€ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ ë°©ë²•ë“¤ì„ ì°¾ì•„ë´¤ìŠµë‹ˆë‹¤.</p>
<ul>
  <li>Numpyë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•</li>
  <li>FFTë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•</li>
</ul>

<p>FFT(Fast Fourier Transform)ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ìˆ˜ì‹ê³¼ êµ¬í˜„ë°©ë²•ì´ ì–´ë ¤ì›Œì„œ í¬ê¸°í•˜ê³  ì²« ë²ˆì§¸ ë°©ë²•ì¸ Numpyë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤. ì½”ë“œëŠ” ìŠ¤íƒ ì˜¤ë²„í”Œë¡œìš°ì— ìˆëŠ” ì½”ë“œë¥¼ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">convolve2d</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span><span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">shape</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">f</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">strd</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">lib</span><span class="p">.</span><span class="n">stride_tricks</span><span class="p">.</span><span class="n">as_strided</span>
    <span class="n">subM</span> <span class="o">=</span> <span class="n">strd</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">s</span><span class="p">,</span> <span class="n">strides</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="n">strides</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'ij,ijkl-&gt;kl'</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">subM</span><span class="p">)</span>
</code></pre></div></div>
<p>ë™ì‘ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<ul>
  <li>ì ìš©í•  ì´ë¯¸ì§€ë¥¼ ì»¤ë„ í¬ê¸°ì˜ ë§ê²Œ ì˜ë¼ì„œ ì €ì¥í•©ë‹ˆë‹¤.</li>
  <li>ì˜ë ¤ì§„ ì´ë¯¸ì§€ë“¤ì„ í•˜ë‚˜ì”© êº¼ë‚´ ì»¤ë„ê³¼ element-wise productë¥¼ ì§„í–‰í•˜ì—¬ ë”í•œ ê°’ë“¤ì„ ë¦¬í„´í•©ë‹ˆë‹¤.</li>
  <li>forë¬¸ìœ¼ë¡œ ì»¤ë„ì„ ì›€ì§ì¼ í•„ìš”ê°€ ì—†ì–´ ê³±ì…ˆê³¼ í•© ì—°ì‚°ë§Œ ì§„í–‰í•˜ë¯€ë¡œ ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤.</li>
</ul>

<h3 id="forward">Forward</h3>
<p>Conv2d ë ˆì´ì–´ì˜ forwardë¥¼ ë¨¼ì € ë³´ê² ìŠµë‹ˆë‹¤.<br />
(3,3)ì¸ X, (2,2)ì¸ Wë¥¼ convolutioní•´ì„œ (2,2)ì¸ Oë¥¼ ê³„ì‚°í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.<br />
<img src="https://drive.google.com/uc?export=view&amp;id=1a6BSuYgNy41-ibtokSbW-q-6qinfs01l" alt="" /></p>
<ul>
  <li>$o_{11} = k_{11}x_{11} + k_{12}x_{12} + k_{21}x_{21} + k_{22}x_{22}$</li>
  <li>$o_{12} = k_{11}x_{12} + k_{12}x_{13} + k_{21}x_{22} + k_{22}x_{23}$</li>
  <li>$o_{21} = k_{11}x_{21} + k_{12}x_{22} + k_{21}x_{31} + k_{22}x_{32}$</li>
  <li>$o_{22} = k_{11}x_{22} + k_{12}x_{23} + k_{21}x_{32} + k_{22}x_{33}$</li>
</ul>

<h3 id="backward">Backward</h3>
<p>ì´ì œ Conv2d ë ˆì´ì–´ì˜ backwardë¥¼ ê³„ì‚°í•´ë³´ê² ìŠµë‹ˆë‹¤. doutì€ ë’¤ì˜ ë ˆì´ì–´ì—ì„œ ë“¤ì–´ì˜¤ëŠ” gradientë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.<br />
Backward ì—°ì‚°ë¶€í„°ëŠ” Fowardì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë³´ë©´ì„œ ì´í•´í•˜ì‹œëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.</p>

<p>ê°€ì¥ ë¨¼ì €, forwardì—ì„œ ê³„ì‚°í•œ ëª¨ë“  ì‹ì„ weightì— ëŒ€í•´ í¸ë¯¸ë¶„ì„ ì‹œë„í•©ë‹ˆë‹¤.</p>
<ul>
  <li>$\frac{do_{11}}{dk_{11}}=x_{11} \quad \frac{do_{11}}{dk_{12}}=x_{12} \quad \frac{do_{11}}{dk_{21}}=x_{21} \quad \frac{do_{11}}{dk_{22}}=x_{22}$</li>
  <li>$\frac{do_{12}}{dk_{11}}=x_{12} \quad \frac{do_{12}}{dk_{12}}=x_{13} \quad \frac{do_{12}}{dk_{21}}=x_{22} \quad \frac{do_{12}}{dk_{22}}=x_{23}$</li>
  <li>$\frac{do_{21}}{dk_{11}}=x_{21} \quad \frac{do_{21}}{dk_{12}}=x_{22} \quad \frac{do_{21}}{dk_{21}}=x_{31} \quad \frac{do_{21}}{dk_{22}}=x_{32}$</li>
  <li>$\frac{do_{22}}{dk_{11}}=x_{22} \quad \frac{do_{22}}{dk_{12}}=x_{23} \quad \frac{do_{22}}{dk_{21}}=x_{32} \quad \frac{do_{22}}{dk_{22}}=x_{33}$</li>
</ul>

<p>ì´ì œ weightì˜ gradient í•˜ë‚˜ë§Œ ê³„ì‚°í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<ul>
  <li>$\frac{dL}{dk_{11}} = \frac{dL}{do_{11}} \cdot \frac{do_{11}}{dk_{11}} + \frac{dL}{do_{12}} \cdot \frac{do_{12}}{dk_{11}} + \frac{dL}{do_{21}} \cdot \frac{do_{21}}{dk_{11}} + \frac{dL}{do_{22}} \cdot \frac{do_{22}}{dk_{11}}  \quad \text{by the chain rule}$</li>
  <li>$\frac{dL}{dk_{11}} = d_{11} \cdot \frac{do_{11}}{dk_{11}} + d_{12} \cdot \frac{do_{12}}{dk_{11}} + d_{21} \cdot \frac{do_{21}}{dk_{11}} + d_{22} \cdot \frac{do_{22}}{dk_{11}} $</li>
</ul>

<p>ìœ„ì—ì„œ ê³„ì‚°í•œ ê°’ì„ ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<ul>
  <li>$\frac{dL}{dk_{11}} = d_{11} \cdot x_{11} + d_{12} \cdot x_{12} + d_{21} \cdot x_{21} + d_{22} \cdot x_{22} $</li>
</ul>

<p>ë”°ë¼ì„œ, <strong>weightì˜ gradientëŠ” doutê³¼ ì…ë ¥ Xì™€ì˜ convolution ì—°ì‚°ê³¼ ê°™ìŠµë‹ˆë‹¤.</strong><br />
biasëŠ” forwardë•Œ ë§ì…ˆìœ¼ë¡œ ê³„ì‚°ë˜ë¯€ë¡œ í¸ë¯¸ë¶„ ê°’ì´ 1ì…ë‹ˆë‹¤. ê·¸ë˜ì„œ biasì˜ gradientëŠ” doutì˜ í•©ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>ì´ì œ conv layerì—ì„œ ë‚˜ì˜¤ëŠ” gradientë¥¼ ì…ë ¥ ë ˆì´ì–´ ë°©í–¥ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•œ ê³„ì‚°ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.</p>

<p>ì´ë²ˆì—ëŠ” Oë¥¼ ê³„ì‚°í•˜ëŠ” forward ì‹ì—ì„œ xì— ëŒ€í•´ í¸ë¯¸ë¶„ì„ ê³„ì‚°í•´ë‘ê² ìŠµë‹ˆë‹¤.</p>
<ul>
  <li>$\frac{do_{11}}{dx_{11}}=k_{11} \quad \frac{do_{11}}{dx_{12}}=k_{12} \quad \frac{do_{11}}{dx_{21}}=k_{21} \quad \frac{do_{11}}{dx_{22}}=k_{22}$</li>
  <li>$\frac{do_{12}}{dx_{12}}=k_{11} \quad \frac{do_{12}}{dx_{13}}=k_{12} \quad \frac{do_{12}}{dx_{22}}=k_{21} \quad \frac{do_{12}}{dx_{23}}=k_{22}$</li>
  <li>$\frac{do_{21}}{dx_{21}}=k_{11} \quad \frac{do_{21}}{dx_{22}}=k_{12} \quad \frac{do_{21}}{dx_{31}}=k_{21} \quad \frac{do_{21}}{dx_{32}}=k_{22}$</li>
  <li>$\frac{do_{22}}{dx_{22}}=k_{11} \quad \frac{do_{22}}{dx_{23}}=k_{12} \quad \frac{do_{22}}{dx_{32}}=k_{21} \quad \frac{do_{22}}{dx_{33}}=k_{22}$</li>
</ul>

<p>ë‹¤ìŒ ì…ë ¥ê°’ ê°ê°ì— ëŒ€í•œ gradientì¸ $\frac{dL}{dX}$ê°’ì„ ë‹¤ìŒê³¼ ê°™ì´ ìœ ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<ul>
  <li>$\frac{dL}{dx_{11}} = \frac{dL}{do_{11}} \cdot \frac{do_{11}}{dx_{11}}$</li>
  <li>$\frac{dL}{dx_{12}} = \frac{dL}{do_{12}} \cdot \frac{do_{12}}{dx_{12}} + \frac{dL}{do_{11}} \cdot \frac{do_{11}}{dx_{12}}$</li>
  <li>$\frac{dL}{dx_{13}} = \frac{dL}{do_{12}} \cdot \frac{do_{12}}{dx_{13}}$</li>
  <li>$\frac{dL}{dx_{21}} = \frac{dL}{do_{21}} \cdot \frac{do_{21}}{dx_{21}} + \frac{dL}{do_{11}} \cdot \frac{do_{11}}{dx_{21}}$</li>
  <li>$\frac{dL}{dx_{22}} = \frac{dL}{do_{22}} \cdot \frac{do_{22}}{dx_{22}} + \frac{dL}{do_{21}} \cdot \frac{do_{21}}{dx_{22}} + \frac{dL}{do_{12}} \cdot \frac{do_{12}}{dx_{22}} + \frac{dL}{do_{11}} \cdot \frac{do_{11}}{dx_{22}}$</li>
  <li>$\frac{dL}{dx_{23}} = \frac{dL}{do_{22}} \cdot \frac{do_{22}}{dx_{23}} + \frac{dL}{do_{12}} \cdot \frac{do_{12}}{dx_{23}}$</li>
  <li>$\frac{dL}{dx_{31}} = \frac{dL}{do_{21}} \cdot \frac{do_{21}}{dx_{31}}$</li>
  <li>$\frac{dL}{dx_{32}} = \frac{dL}{do_{22}} \cdot \frac{do_{22}}{dx_{32}} + \frac{dL}{do_{21}} \cdot \frac{do_{21}}{dx_{32}}$</li>
  <li>$\frac{dL}{dx_{33}} = \frac{dL}{do_{22}} \cdot \frac{do_{22}}{dx_{33}}$</li>
</ul>

<p>ìœ„ì˜ ì‹ì— ë‹¤ì‹œ ì •ë¦¬í•´ë‘” ê°’ì„ ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<ul>
  <li>$\frac{dL}{dx_{11}} = d_{11} \cdot k_{11}$</li>
  <li>$\frac{dL}{dx_{12}} = d_{12} \cdot k_{11} + d_{11} \cdot k_{12}$</li>
  <li>$\frac{dL}{dx_{13}} = d_{12} \cdot k_{12}$</li>
  <li>$\frac{dL}{dx_{21}} = d_{21} \cdot k_{11} + d_{11} \cdot k_{21}$</li>
  <li>$\frac{dL}{dx_{22}} = d_{22} \cdot k_{11} + d_{21} \cdot k_{12} + d_{12} \cdot k_{21} + d_{11} \cdot k_{22}$</li>
  <li>$\frac{dL}{dx_{23}} = d_{22} \cdot k_{12} + d_{12} \cdot k_{22}$</li>
  <li>$\frac{dL}{dx_{31}} = d_{21} \cdot k_{21}$</li>
  <li>$\frac{dL}{dx_{32}} = d_{22} \cdot k_{21} + d_{21} \cdot k_{22}$</li>
  <li>$\frac{dL}{dx_{33}} = d_{22} \cdot k_{22}$</li>
</ul>

<p>ì‹ë§Œ ë³´ê³ ì„œëŠ” ì–´ë–¤ì‹ìœ¼ë¡œ ê³„ì‚°ë˜ì–´ì•¼ í•˜ëŠ”ì§€ ê°ì´ ì˜ ì•ˆì˜µë‹ˆë‹¤. ì´ê²ƒì„ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í–ˆì„ ë•Œ ì •ë§ ì‰½ê²Œ ê³„ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. íŒŒë€ìƒ‰ í…Œë‘ë¦¬ê°€ weight, ë¹¨ê°„ìƒ‰ í…Œë‘ë¦¬ê°€ dout, ë…¸ë€ìƒ‰ í…Œë‘ë¦¬ëŠ” ê³„ì‚°ì— ì°¸ì—¬í•˜ëŠ” cell ì…ë‹ˆë‹¤.</p>
<ul>
  <li>
    <p>$\frac{dL}{dx_{11}} = d_{11} \cdot k_{11}$
<img src="https://drive.google.com/uc?export=view&amp;id=18KIBdo2AbiexxzKaL15siJAFc5KtVpbW" alt="" width="300" /></p>
  </li>
  <li>
    <p>$\frac{dL}{dx_{12}} = d_{12} \cdot k_{11} + d_{11} \cdot k_{12}$
<img src="https://drive.google.com/uc?export=view&amp;id=1_5_BHHzLyqCyAJaaincxoM0fSg0jVJ2V" alt="" width="300" /></p>
  </li>
  <li>
    <p>$\frac{dL}{dx_{13}} = d_{12} \cdot k_{12}$
<img src="https://drive.google.com/uc?export=view&amp;id=1RdI042LLWwvWM6g64K8RBDiYfLhGfl04" alt="" width="300" /></p>
  </li>
</ul>

<h2 id="crossentropyloss">CrossEntropyLoss</h2>
<p>NLL(Negative Log Likelihood)ë¥¼ ì´ìš©í•œ Loss functionì…ë‹ˆë‹¤. Pytorchì˜ CrossEntropyLossëŠ” Softmaxì™€ NLLLossë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.</p>

<p>ì´ì „ì— êµ¬í˜„í–ˆë˜ CrossEntropyLossëŠ” logë¥¼ ì·¨í•˜ëŠ” ê³¼ì •ì—ì„œ ì…ë ¥ê°’ì´ ìŒìˆ˜ê°€ ë“¤ì–´ì˜¤ëŠ” ê²½ìš° ì—ëŸ¬ê°€ ì¼ì–´ë‚©ë‹ˆë‹¤. ë°˜ë©´ì— Softmaxì™€ NLLLossë¡œ êµ¬í˜„í•˜ê²Œ ë˜ë©´ ìŒìˆ˜ ì…ë ¥ì— ëŒ€í•´ì„œë„ cross entropy ê°’ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">NLLLoss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span>\
        <span class="p">[</span><span class="o">-</span><span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">label</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="p">)])</span> <span class="o">/</span> <span class="n">batch</span>

<span class="k">def</span> <span class="nf">NLLLoss_deriv</span><span class="p">(</span><span class="n">y_pred</span><span class="p">):</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>\
        <span class="p">[</span><span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">/</span><span class="n">batch</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span> \
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">math</span>
    <span class="n">r_</span> <span class="o">=</span> <span class="n">softmax_</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>\
        <span class="p">[</span><span class="n">math</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">r_</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r_</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span> \
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r_</span><span class="p">))]</span>

<span class="k">def</span> <span class="nf">log_softmax_deriv</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">math</span>
    <span class="n">r_</span> <span class="o">=</span> <span class="n">softmax_</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>\
        <span class="p">[</span><span class="n">r_</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">label</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">==</span><span class="n">j</span> <span class="k">else</span> <span class="n">r_</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> \
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r_</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span> \
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r_</span><span class="p">))]</span>
</code></pre></div></div>
<p>NLLLoss í•¨ìˆ˜ì˜ ì…ë ¥ìœ¼ë¡œëŠ” log_softmaxë¥¼ í†µê³¼í•œ ê°’ì´ ë“¤ì–´ê°€ë„ë¡ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.</p>

<h2 id="reference">Reference</h2>
<ul>
  <li><a href="https://stackoverflow.com/a/43087771">Convolve2d(StackOverflow)</a></li>
  <li><a href="https://ratsgo.github.io/deep%20learning/2017/04/05/CNNbackprop/">https://ratsgo.github.io/deep%20learning/2017/04/05/CNNbackprop/</a></li>
  <li><a href="https://velog.io/@changdaeoh/backpropagationincnn">https://velog.io/@changdaeoh/backpropagationincnn</a></li>
  <li><a href="https://towardsdatascience.com/backpropagation-in-a-convolutional-layer-24c8d64d8509">https://towardsdatascience.com/backpropagation-in-a-convolutional-layer-24c8d64d8509</a></li>
  <li><a href="https://towardsdatascience.com/forward-and-backward-propagation-of-pooling-layers-in-convolutional-neural-networks-11e36d169bec">https://towardsdatascience.com/forward-and-backward-propagation-of-pooling-layers-in-convolutional-neural-networks-11e36d169bec</a></li>
</ul>
:ET