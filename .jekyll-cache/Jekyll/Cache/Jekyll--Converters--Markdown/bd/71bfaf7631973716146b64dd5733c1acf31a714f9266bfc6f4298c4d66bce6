I"´7<h2 id="generative-models">Generative Models</h2>

<h3 id="learning-a-generative-model">Learning a Generative Model</h3>
<ul>
  <li>ê°œì˜ ì´ë¯¸ì§€ê°€ ìˆë‹¤ê³  ê°€ì •í•˜ë©´ ê°œì˜ ì´ë¯¸ì§€ì— ëŒ€í•œ í™•ë¥ ë¶„í¬ $p(x)$ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.
    <ul>
      <li>Generation : ë§Œì•½ $p(x)$ì—ì„œ $x_{new}$ë¥¼ sampleí•  ìˆ˜ ìˆë‹¤ë©´ $x_{new}$ ë˜í•œ ê°œì˜ ì´ë¯¸ì§€ì´ë‹¤. (ìƒ˜í”Œë§, sampling)</li>
      <li>Density estimation : $x$ê°€ ê°œì¸ì§€ ê³ ì–‘ì¸ì§€ ì•Œì•„ ë‚¼ ìˆ˜ ìˆë‹¤. (ì´ìƒ íƒì§€, anomaly detection)
        <ul>
          <li>ì…ë ¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ ì…ë ¥ì— ëŒ€í•œ í™•ë¥  ê°’ì„ ì–»ì–´ë‚¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ explicit modelì´ë¼ í•œë‹¤.
            <ul>
              <li>ì˜ˆë¥¼ë“¤ì–´, ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë‚´ê³  ê·¸ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë§í•œë‹¤.</li>
            </ul>
          </li>
          <li>ë°˜ëŒ€ë¡œ generationë§Œ í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ implicit modelì´ë¼ í•œë‹¤.</li>
        </ul>
      </li>
      <li>Unsupervised representation learning : ì´ë¯¸ì§€ê°€ ê°€ì§€ê³  ìˆëŠ” íŠ¹ì§•ë“¤ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. (feature learning)
        <ul>
          <li>ì˜ˆë¥¼ë“¤ë©´ ê°œì˜ ê¼¬ë¦¬, ê·€ì™€ ê°™ì€ ê²ƒë“¤</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>how can we represent $p(x)$?</li>
</ul>

<h3 id="basic-discrete-distributions">Basic Discrete Distributions</h3>
<ul>
  <li>ë² ë¥´ëˆ„ì´ ë¶„í¬ : (biased) coin flip -&gt; 0 or 1
    <ul>
      <li>D = {Heads, Tails}</li>
      <li>Specify $P(X=Heads) = p$. Then $P(X=Tails) = 1 - p$</li>
      <li>$X \sim Ber(p)$</li>
      <li>ì´ í™•ë¥ ë¶„í¬ë¥¼ í‘œí˜„í•˜ëŠ” ìˆ«ìê°€ í•˜ë‚˜ í•„ìš”í•˜ë‹¤.</li>
    </ul>
  </li>
  <li>ì¹´í…Œì½”ë¦¬ ë¶„í¬ : (biased) m-sided dice
    <ul>
      <li>D = {1,â€¦,m}</li>
      <li>Specify $P(Y=i) = p_i$, such that $\sum_{i=1}^m p_i=1$</li>
      <li>$Y \sim Cat(p_1,â€¦,p_m)$</li>
      <li>6ë©´ì˜ ì£¼ì‚¬ìœ„ë¥¼ ë˜ì§€ëŠ” ë°ì—ëŠ” 5ê°œì˜ íŒŒë¼ë¯¸í„°ë§Œ ìˆìœ¼ë©´ ëœë‹¤. ì™œëƒí•˜ë©´ í•©ê³„ë¥¼ í†µí•´ ë‚˜ë¨¸ì§€ í•œ ë©´ì„ ì•Œ ìˆ˜ ìˆê¸° ë•Œë¬¸</li>
    </ul>
  </li>
</ul>

<h3 id="example">Example</h3>
<ul>
  <li>RGB joint distributionì„ ëª¨ë¸ë§</li>
  <li>$(r,g,b) \sim p(R,G,B)$</li>
  <li>Number of case : 256 $\times$ 256 $\times$ 256</li>
  <li>ì´ë•Œ íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ëŠ” 256 $\times$ 256 $\times$ 256 - 1</li>
  <li>íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ í•™ìŠµì€ ì–´ë ¤ì›Œì§€ê¸° ë•Œë¬¸ì— íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ë…¸ë ¥ì´ í•„ìš”í•˜ë‹¤</li>
</ul>

<h3 id="structure-through-independence">Structure Through Independence</h3>
<ul>
  <li>ë§Œì•½, $X_1, â€¦, X_n$ì´ ëª¨ë‘ independentí•˜ë‹¤ë©´ $p(x_1,â€¦,x_n)=p(x_1)p(x_2)â€¦p(x_n)$ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</li>
  <li>stateì˜ ìˆ˜ëŠ” $2^n$ê°œê°€ ëœë‹¤.</li>
  <li>$p(x_1,â€¦,x_n)$ì„ í‘œí˜„í•˜ê¸° ìœ„í•œ íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ëŠ” $n$ê°œì´ë‹¤.</li>
  <li>$2^n$ê°œì˜ ì—”íŠ¸ë¦¬ë¥¼ ë‹¨ì§€ $n$ê°œì˜ ìˆ˜ë¡œ í‘œí˜„ê°€ëŠ¥í•˜ë‹¤. ê·¸ëŸ¬ë‚˜ independentí•˜ë‹¤ëŠ” ê°€ì •ì´ ë„ˆë¬´ ì–´ë µë‹¤</li>
</ul>

<h3 id="conditional-independence">Conditional Independence</h3>
<ul>
  <li>3ê°€ì§€ì˜ ì¤‘ìš”í•œ ë£°
    <ul>
      <li>Chain rule:
        <ul>
          <li>$p(x_1,â€¦,x_n)=p(x_1)p(x_2|x_1)..p(x_n|x_1,â€¦,x_{n-1})$</li>
          <li>$p(x_1)$ : 1 parameter</li>
          <li>$p(x_2|x_1)$ : 2 parameter ($p(x_2|x_1)=0$ì´ë¼ë©´ $p(x_2|x_1)=1$)</li>
          <li>$p(x_3|x_1,x_2)$ : 4 parameter</li>
          <li>ë”°ë¼ì„œ, $1 + 2 + 2^2 + â€¦ + 2^{n-1} = 2^n-1$ì´ë¯€ë¡œ ì´ì „ê³¼ ê°™ë‹¤.</li>
        </ul>
      </li>
      <li>Bayesâ€™ rule:
        <ul>
          <li>$p(x|y) = \frac{p(x,y)}{p(y)} = \frac{p(y|x)p(x)}{p(y)}$</li>
        </ul>
      </li>
      <li>Conditional independence:
        <ul>
          <li>if $x \bot y |z$, then $p(x |y,z) = p(x|z)$</li>
          <li>$z$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ $x$ì™€ $y$ê°€ independentí•  ë•Œ, $p(x|y,z)$ëŠ” $p(x|z)$ì™€ ê°™ê¸° ë•Œë¬¸ì— $y$ëŠ” ìƒê´€ì´ ì—†ë‹¤</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ì´ì œ $X_{i+1} \bot X_1,â€¦, X_{i-1}|X_i$ (Markov assumption)ì´ë¼ê³  í•  ë•Œ
    <ul>
      <li>$p(x_1,â€¦,x_n) = p(x_1)p(x_2|x_1)â€¦p(x_n|x_{n-1})$</li>
      <li>ì´ì œ íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” $2n-1$ì´ ë˜ì—ˆë‹¤.</li>
      <li>ë”°ë¼ì„œ Markov assumptionì„ í†µí•´ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆì—ˆë‹¤.</li>
    </ul>
  </li>
</ul>

<h3 id="auto-regressive-model">Auto-regressive Model</h3>
<ul>
  <li>28 x 28 ë°”ì´ë„ˆë¦¬ í”½ì…€ì´ ìˆë‹¤ê³  ê°€ì •í•˜ì</li>
  <li>$p(x) = p(x_1,â€¦,x_{784}), x \in [0,1] ^{784}$</li>
  <li>ì´ê²ƒì„ $p(x)$ë¡œ ì–´ë–»ê²Œ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ê°€?
    <ul>
      <li>chain ruleì„ ì‚¬ìš©</li>
      <li>$p(x_{1:784})=p(x_1)p(x_2|x_1)p(x_3|x_{1:2})â€¦$</li>
      <li>ì´ëŸ¬í•œ ê²ƒì„ autoregressive modelì´ë¼ í•œë‹¤.
        <ul>
          <li>autoregressive modelì€ ì–´ë–¤ í•˜ë‚˜ì˜ ì´ì „ ì •ë³´ê°€ ì´ì „ ì •ë³´ë“¤ì— ëŒ€í•´ dependentí•œ ê²ƒì„ ë§í•œë‹¤.</li>
          <li>Markov assumptionì„ í†µí•´ ië²ˆì§¸ í”½ì…€ì´ i-1ë²ˆì§¸ í”½ì…€ì—ë§Œ dependentí•œ ê²ƒë„ autoregressvie modelì´ë‹¤.</li>
          <li>ië²ˆì§¸ í”½ì…€ì´ 1ë¶€í„° i-1ë²ˆì§¸ ëª¨ë‘ì— dependentí•œ ê²ƒë„ autoregressive modelì´ë‹¤.</li>
        </ul>
      </li>
      <li>ë˜í•œ, ì´ë¯¸ì§€ ë„ë©”ì¸ê³¼ ê°™ì´ ìˆœì„œë¥¼ ì •í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤.
        <ul>
          <li>ìˆœì„œë¥¼ ë§¤ê¸°ëŠ” ê²ƒì— ë”°ë¼ ì„±ëŠ¥ê³¼ ë°©ë²•ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>markov assumptionì„ ì£¼ê±°ë‚˜ ì–´ë–¤ ì„ì˜ì˜ conditional independenceë¥¼ ì£¼ëŠ” ê²ƒì´ chain ruleì…ì¥ì—ì„œ joint distributionì„ ìª¼ê°œëŠ” ê±°ì— ì–´ë–¤ ê´€ê³„ê°€ ìˆëŠ” ê±´ì§€ ì˜ ìƒê°í•´ë´ì•¼ í•œë‹¤.</li>
</ul>

<h3 id="nade-neural-autoregressive-density-estimator">NADE: Neural Autoregressive Density Estimator</h3>
<ul>
  <li>ië²ˆì§¸ í”½ì…€ì„ 1ë¶€í„° i-1ì—ë§Œ dependentí•˜ê²Œ ë§Œë“ ê²ƒ</li>
  <li>$p(x_1|x_{1:i-1}) = \sigma(\alpha_ih_i+b_i)$ where $h_i = \sigma(W{&lt;i}x_{1:i-1}+c)$</li>
  <li>NADEëŠ” explicit modelì´ê³  ì…ë ¥ì˜ densityë¥¼ ê³„ì‚°í•œë‹¤.
    <ul>
      <li>784ê°œì˜ ë°”ì´ë„ˆë¦¬ í”½ì…€ì´ ìˆë‹¤ê³  ê°€ì •í•˜ë©´</li>
      <li>$p(x_1,..,x_{784})=p(x_1)p(x_2|x_1)â€¦p(x_{784}|x_{1:784})$</li>
      <li>ê° conditional probability $p(x_i|x_{1:i-1})$ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°í•œë‹¤.</li>
    </ul>
  </li>
  <li>continuous random variableì„ ëª¨ë¸ë§í•˜ëŠ” ê²½ìš° ê°€ìš°ì‹œì•ˆ ë¯¹ìŠ¤ì³ ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤.</li>
</ul>

<h3 id="pixel-rnn">Pixel RNN</h3>
<ul>
  <li>í”½ì…€ë“¤ì„ ë§Œë“¤ì–´ ë‚´ê³  ì‹¶ì–´í•˜ëŠ” generative modelì´ë‹¤.</li>
  <li>ì˜ˆë¥¼ë“¤ì–´, n x n RGB ì´ë¯¸ì§€ë¥¼ ì›í•œë‹¤ë©´
    <ul>
      <li>$P(x) = \Pi_{i=1}^{n^2}p(x_{i,R}|x_{&lt;i})p(x_{i,G}|x_{&lt;i},x_{i,R})p(x_{i,B}|x_{&lt;i},x_{i,R},x_{i,G})$</li>
      <li>$p(x_{i,R}|x_{&lt;i})$ : Prob. i-th Red</li>
      <li>$p(x_{i,G}|x_{&lt;i},x_{i,R})$ : Prob. i-th Green</li>
      <li>$p(x_{i,B}|x_{&lt;i},x_{i,R},x_{i,G})$ : Prob. i-th Blue</li>
    </ul>
  </li>
  <li>orderingì— ë”°ë¼ ë‘ ê°€ì§€ ëª¨ë¸ë¡œ ë‚˜ë‰œë‹¤.
    <ul>
      <li>Row LSTM : ìœ„ìª½ì˜ ìˆëŠ” ì •ë³´ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•</li>
      <li>Diagonal BiLSTM : ìê¸° ì´ì „ì •ë³´ë“¤ì„ ëª¨ë‘ í™œìš©í•œ ë°©ë²•</li>
    </ul>
  </li>
</ul>

<h2 id="latent-variable-models">Latent Variable Models</h2>
<hr />
<ul>
  <li>autoencoderê°€ generative modelì¸ê°€?</li>
</ul>

<h3 id="variational-auto-encoder-va">Variational Auto-encoder (VA)</h3>
<ul>
  <li>Variational inference (VI)
    <ul>
      <li>Variational distributionì„ ì°¾ëŠ” ëª©ì ì€ posterior distributionì„ ì°¾ê¸° ìœ„í•¨ì´ë‹¤.</li>
      <li>Posterior distribution : $p_{\theta}(z|x)$
        <ul>
          <li>Posterior distributionì´ë€ observationì´ ì£¼ì–´ì¡Œì„ ë•Œ ê´€ì‹¬ìˆì–´í•˜ëŠ” random variableì˜ í™•ë¥ ë¶„í¬ì´ë‹¤.</li>
          <li>ì—¬ê¸°ì„œ zëŠ” Latent Variableì´ ëœë‹¤.</li>
        </ul>
      </li>
      <li>Variational distribution : $q_{\phi}(z|x)$
        <ul>
          <li>Posterior distributionì„ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ë„ˆë¬´ í˜ë“¤ê¸° ë•Œë¬¸ì— ê·¼ì‚¬í•œ ë¶„í¬ë¥¼ Variatoinal distributionì´ë¼ í•œë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>ë”°ë¼ì„œ ê´€ì‹¬ìˆëŠ” posteriorì‚¬ì´ì˜ KL divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ”(ê·¼ì‚¬í•˜ëŠ”) variational distributionì„ ì°¾ê³  ì‹¶ì€ ê²ƒì„ Variational inferenceë¼ í•œë‹¤.</p>
  </li>
  <li>posteriorê°€ ë­”ì§€ë„ ëª¨ë¥´ëŠ”ë° ì´ê²ƒì„ ê·¼ì‚¬í•˜ëŠ” variational distributionì„ ì°¾ëŠ” ë‹¤ëŠ” ê²ƒì€ ë§ì´ ì•ˆëœë‹¤.</li>
  <li>ê·¸ëŸ¬ë‚˜ targetì´ ë­”ì§€ ëª¨ë¥´ëŠ”ë° loss functionì„ ì¤„ì¼ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê²ƒì´ VIì˜ ELBO(Evidence Lower Bound)ë‹¤.
    <ul>
      <li>ELBOë¥¼ í‚¤ì›€ìœ¼ë¡œì¨ intractable objectiveë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤.</li>
      <li>In $P_{\theta}(D) = E_{q_{\phi}(z|x)} [In \frac{p_{\phi}(x,z)}{q_{\phi}(z|x)}](ELBO)+ D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x)) (Objective)$</li>
    </ul>
  </li>
  <li>ë‹¤ì‹œ ELBOë¥¼ ë‚˜ëˆ„ê²Œ ë˜ë©´
    <ul>
      <li>$E_{q_{\phi}(z|x)} [In \frac{p_{\phi}(x,z)}{q_{\phi}(z|x)}] = \int In\frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}q_{\phi}(z|x)dz$</li>
      <li>= $E_{q_{\phi}(z|x)}[p_{\theta}(x|z)] - D_{KL}(q_{\theta}(z|x)||p(z))$</li>
      <li>= Reconstruction Term - Prior Fitting Term
        <ul>
          <li>ì¸ì½”í„°ë¥¼ í†µí•´ì„œ xë¼ëŠ” ì…ë ¥ì„ latent spaceë¡œ ë³´ëƒˆë‹¤ê°€ ë‹¤ì‹œ ë””ì½”ë”ë¡œ ëŒì•„ì˜¤ëŠ” reconstruction lossë¥¼ ì¤„ì´ëŠ” ê²ƒì´ reconstruction termì´ë‹¤.</li>
          <li>Prior Fitting Termì€ ì´ë¯¸ì§€ë“¤ì„ latent spaceë¡œ ì˜¬ë¦¬ë©´ ì´ë¯¸ì§€ë“¤ì´ ì´ë£¨ëŠ” ë¶„í¬ì™€ ë‚´ê°€ ê°€ì •í•˜ê³  ìˆëŠ” latent spaceì—ì„œì˜ prior distributionì„ ë¹„ìŠ·í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒì´ ë‘ ê°€ì§€ ë¶„í¬ë¥¼ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ê²°êµ­ Variational Auto-encoderëŠ” ì–´ë–¤ ì…ë ¥ì´ ì£¼ì–´ì§€ê³  latent spaceë¡œ ë³´ë‚´ì„œ ë¬´ì–¸ê°€ë¥¼ ì°¾ê³  ë‹¤ì‹œ reconstructioní•˜ëŠ” termìœ¼ë¡œ ë§Œë“¤ì–´ì§€ëŠ”ë° generative modelì´ ë˜ê¸° ìœ„í•´ì„œëŠ” latent spaceëœ prior distribution zë¥¼ ìƒ˜í”Œë§ í•˜ê³  ë””ì½”ë”ì— ë„£ì–´ì„œ ë‚˜ì˜¤ëŠ” ì–´ë–¤ output ë„ë©”ì¸ë“¤ì˜ ê°’ë“¤ì´ ë°”ë¡œ generation resultë¼ê³  ë³¸ë‹¤.</li>
  <li>
    <p>í•˜ì§€ë§Œ ì—„ë°€í•œ ì˜ë¯¸ì—ì„œ ì…ë ¥ì„ ë„£ì–´ì„œ ì•„ì›ƒí’‹ì´ ë‚˜ì˜¤ëŠ”  auto-encoderëŠ” generative modelì´ ì•„ë‹ˆë‹¤.</p>
  </li>
  <li>Key limitation:
    <ul>
      <li>explicit ëª¨ë¸ì´ ì•„ë‹ˆë‹¤. ì–´ë–¤ ì…ë ¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ ì–¼ë§ˆë‚˜ likelihoodí•œì§€ ì•Œê¸° ì–´ë µë‹¤.</li>
      <li>prior fitting termì—ì„œ KL divergenceëŠ” ê°€ìš°ì‹œì•ˆì„ ì œì™¸í•˜ê³  closed formì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ê°€ ê±°ì˜ ì—†ë‹¤.</li>
      <li>ì¼ë°˜ì ìœ¼ë¡œëŠ” isotropic Gaussianì„ ì‚¬ìš©í•œë‹¤.
        <ul>
          <li>ëª¨ë“  output dimensionì´ independentí•œ ë¶„í¬</li>
          <li>$D_{KL}(q_{\phi}(z|x)||N(0,I)) = \frac{1}{2}\sum_{i=1}^D(\sigma_{z_i}^2+\mu_{z_i}^2-ln(\sigma_{z_i}^2)-1)$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="adversarial-auto-encoder-aa">Adversarial Auto-encoder (AA)</h3>
<ul>
  <li>auto encoderì˜ ê°€ì¥ í° ë‹¨ì ì€ ê°€ìš°ì‹œì•ˆì„ í™œìš©í•´ì•¼ í•œë‹¤ëŠ” ì ì´ë‹¤.
    <ul>
      <li>ê°€ìš°ì‹œì•ˆì´ ì•„ë‹Œ ë‹¤ë¥¸ ë¶„í¬ë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ adversarial auto-encoderë¥¼ ì‚¬ìš©í•œë‹¤.</li>
    </ul>
  </li>
  <li>VAë³´ë‹¤ AAê°€ ì„±ëŠ¥ì´ ì¢‹ì€ ê²½ìš°ë„ ë§ë‹¤.</li>
</ul>

<h3 id="generative-adversarial-network-gan">Generative Adversarial Network (GAN)</h3>
<ul>
  <li>generatorëŠ” discriminatorë¥¼ ì†ì´ê¸°ìœ„í•´ fake dataë¥¼ ìƒì„±í•œë‹¤.</li>
  <li>discriminatorëŠ” real dataì™€ fake dataë¥¼ íŒë³„í•œë‹¤.</li>
  <li>disciminatorì˜ ì„±ëŠ¥ì´ ì˜¬ë¼ê°ˆìˆ˜ë¡ generator ë˜í•œ ì„±ëŠ¥ì´ ê°™ì´ ì˜¬ë¼ê°„ë‹¤.</li>
  <li>$min_G max_D V(D,G) = E_{x\sim P_{data}(x)}[log D(x)] + E_{z \sim p_z(z)}[log(1-D(G(z)))]$</li>
  <li>implicit ëª¨ë¸ì´ë‹¤.</li>
</ul>

<h3 id="gan-vs-vae">GAN vs. VAE</h3>
<ul>
  <li>VAE : xë¼ëŠ” ì´ë¯¸ì§€ í˜¹ì€ ì…ë ¥ ë„ë©”ì¸ì´ ë“¤ì–´ì˜¤ë©´ ì¸ì½”ë”ë¥¼ í†µí•´ latent vectorë¡œ ê°”ë‹¤ê°€ zë¥¼ ìƒ˜í”Œë§í•´ì„œ ë””ì½”ë”ë¥¼ í†µí•´ xê°€ generative ê²°ê³¼ê°€ ëœë‹¤.</li>
  <li>GAN : zë¥¼ í†µí•´ì„œ fakeê°€ ìƒì„±ëœë‹¤. DiscriminatorëŠ” ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµí•˜ê³  generatorëŠ” discriminator ì…ì¥ì—ì„œ trueê°€ ë‚˜ì˜¤ë„ë¡ ë‹¤ì‹œ ì—…ë°ì´íŠ¸í•˜ê³  discriminatorëŠ” ì—…ë°ì´íŠ¸ëœ generatorì˜ ìƒì„± ê²°ê³¼ë¬¼ì„ ë¶„ë¥˜í•  ìˆ˜ ìˆë„ë¡ ì„±ì¥í•œë‹¤.</li>
</ul>

<h3 id="gan-objective">GAN Objective</h3>
<ul>
  <li>generatorì™€ discriminator ì‚¬ì´ì˜ minmax ê²Œì„ì´ë‹¤.
    <ul>
      <li>minmax ê²Œì„ì€ í•œìª½ì€ ë†’ì´ê³  ì‹¶ì–´í•˜ê³  í•œìª½ì€ ë‚®ì¶”ê³  ì‹¶ì–´í•˜ëŠ” ê²Œì„</li>
    </ul>
  </li>
  <li>For discriminator
    <ul>
      <li>$max_D V(G,D) = E_{x \sim p_{data}}[log D(x)] + E_{x \sim p_G} [log(1-(D(x))$</li>
      <li>where the optimal discriminator is
        <ul>
          <li>$D_G^*(x) = \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>For generator
    <ul>
      <li>$min_G V(G,D) = E_{x \sim p_{data}}[log D(x)] + E_{x \sim p_G} [log(1-(D(x))$</li>
      <li>Plugging in the optimal discriminator,
        <ul>
          <li>$V(G, D_G^*(x)) = D_{KL}[P_{data},\frac{P_{data}+P_G}{2}] + D_{KL}[P_G,\frac{P_{data}+P_G}{2}] - log 4$</li>
        </ul>
      </li>
      <li>2 x Jenson-Shannon Divergence (JSD) = $2D_{JSD}[P_{data},P_G] - log 4$</li>
      <li>GANì˜ Objectiveê°€ ë§ì€ ê²½ìš° true data distributionë¥¼ real data distributionì— ê°€ê¹Œì›Œì§€ê²Œ í•˜ëŠ” ê²ƒ
        <ul>
          <li>ì¦‰, Jenson-Shannon Divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="dcgan">DCGAN</h2>

<h3 id="info-gan">Info-GAN</h3>
<ul>
  <li>zì™€ classë¥¼ ì˜ë¯¸í•˜ëŠ” cë¥¼ í†µí•´ í•œ ê³³ì— ì§‘ì¤‘ì‹œì¼œì£¼ëŠ” ë°©ë²•</li>
</ul>

<h3 id="text2image">Text2Image</h3>
<ul>
  <li>ë¬¸ì¥ì´ ì£¼ì–´ì§€ë©´ ì´ë¯¸ì§€ë¥¼ ìƒì„±</li>
</ul>

<h3 id="puzzle-gan">Puzzle-GAN</h3>
<ul>
  <li>ì´ë¯¸ì§€ ì•ˆì— subpatchê°€ ìˆê³  subpatchë¥¼ ê°€ì§€ê³  ì›ë˜ ì´ë¯¸ì§€ë¡œ ë³µì›</li>
</ul>

<h3 id="cyclegan">CycleGAN</h3>
<ul>
  <li>Cycle-consistency loss</li>
  <li>GAN êµ¬ì¡°ê°€ ë‘ ê°œ ë“¤ì–´ê°€ ìˆëŠ” êµ¬ì¡°</li>
</ul>

<h3 id="star-gan">Star-GAN</h3>
<ul>
  <li>ì´ë¯¸ì§€ë¥¼ ë‹¤ë¥¸ ë„ë©”ì¸ìœ¼ë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ ì•„ë‹Œ íŠ¹ì • ì˜ì—­ì„ ì»¨íŠ¸ë¡¤ í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°</li>
</ul>

<h3 id="progressive-gan">Progressive-GAN</h3>
<ul>
  <li>4x4ì—ì„œ ì‹œì‘í•´ì„œ 1024x1024ë¡œ ì ì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ” êµ¬ì¡°</li>
  <li>í•œë²ˆì— ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ë§Œë“œëŠ” ê²ƒì€ ë¬´ë¦¬ê°€ ê°„ë‹¤.</li>
</ul>
:ET