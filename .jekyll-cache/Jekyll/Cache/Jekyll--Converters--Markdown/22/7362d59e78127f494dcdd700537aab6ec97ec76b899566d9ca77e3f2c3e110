I"<4<h2 id="astract">Astract</h2>

<p>대표적인 문장번역모델들은 인코더와 디코더를 포함한 rnn 혹은 cnn에 기반하고 있습니다.</p>

<p>또한, 가장 성능이 좋은 모델들은 인코더와 디코더가 attention 매커니즘에 의해 연결되어있습니다.</p>

<p>우리는 recurrent와 컨볼루션한 것들을 모두 배제하고 트랜스포머라는 새로운 간단한 네트워크 구조를 제안합니다다.</p>

<p>두번의 기계 번역에서의 실험은 이러한 모델들이 더 병렬화가 가능하고 훈련하는데 시간이 적게 소요됨과 동시에 퀄리티가 좋다는 점을 보여줍니다다.</p>

<p>WMT 2014에서 영어 → 독어에서 28.4 BLEU를 얻으면서 최고기록을 경신하고 영어→불어에서 41.0 BLEU를 얻고 SOTA(State-Of-The-Art)에 올라섰습니다. (8 GPU와 3.5일간의 학습 - 최고 성능의 모델이 필요한 자원보다 매우 적다)</p>

<h2 id="introduction">Introduction</h2>

<p>LSTM이나 GRU와 같이 RNN이 sequence 모델링과 언어 모델링과 기계 번역 문제에서 강자였습니다. RNN 모델은 일반적으로 input과 output sequence의 위치에 따라 계산합니다.</p>

<p>연산 step위치에 따라, 모델은 현재 위치 $t$에서의 입력과 이전 hidden state $h_{t-1}$을 사용하여 $h_t$인 hidden state의 sequence를 생성합니다.</p>

<p>이 sequential한 성질은 학습에서의 병렬화를 하지 못하게 하고 긴 문장에서는 메모리 제약으로 인해 일괄 처리가 되지 않는 치명적인 단점이됩니다.</p>

<p>최근 연구에서 factorization trick과 conditional computation을 통해 계산에서의 상당한 개선이 이루어졌습니다.</p>

<p>그러나 sequential 계산에서의 단점(fundamental constraint of sequential computation)이 아직 남아있습니다.</p>

<p>Attention 매커니즘은 compelling sequence modeling과 transduction model에서 빠지지 않는 부분이 되었습니다.</p>

<p>input과 output sequence에서 거리를 고려하지 않아도 종속성을 모델링할 수 있습니다.</p>

<p>하지만 몇가지 경우를 제외하고, Attention 매커니즘은 Recurrent network를 사용합니다.</p>

<p>input과 output 사이의 dependencies를 설계하기 위한 Recurrence는 Transformer에서는 사용하지 않는다. 대신 attention 매커니즘에서 사용합니다.</p>

<p>Transformer는 더 많은 병렬화가 가능하고 8대의 P100 GPU와 12시간의 학습으로 번역분야에서의 SOTA에 도달했습니다.</p>

<h2 id="model-architecture">Model Architecture</h2>

<p>Transformer는 인코더와 디코더 모두 self-attention이 쌓여있고 fully connected layer를 사용하여 아키텍쳐를 구성하고 있습니다.</p>

<h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h3>

<p><img src="https://onedrive.live.com/embed?resid=502FD124B305BA80%213300&amp;authkey=%21ABkYMVUyN8vebHA&amp;width=490&amp;height=718" alt="" /></p>

<ul>
  <li><strong>Encoder</strong>
    <ul>
      <li>인코더는 $N=6$의 identical 레이어의 스택으로 구성되어있습니다. 각 레이어는 두 개의 sub-layer들을 가지고 있습니다.</li>
      <li>sub-layer의 첫번째는 Multi-Head Attention이고 두번째는 position-wise fully connected feed-forward network입니다.</li>
      <li>sub-layer들은 residual connection을 가지고 있고 이후에 Layer normalization을 사용합니다.</li>
      <li>sub-layer에 들어가는 입력을 $x$라 할때, sub-layer의 출력은 LayerNorm($x$ + Sublayer($x$))로 표기할 수 있습니다.</li>
    </ul>
  </li>
  <li><strong>Decoder</strong>
    <ul>
      <li>디코더 또한 $N=6$의 identical 레이어의 스택으로 구성되어있습니다. 디코더는 3개의 sub-layer로 구성되어 있는데 인코더의 출력에 multi-head attention을 수행할 sub-layer가 추가되었습니다.</li>
      <li>인코더와 비슷하게 각각의 sub-layer에 residual connection을 채용하고 그 후에 layer normalization을 수행합니다.</li>
      <li>디코더에서는 subsequent position을 위해 self-attention을 변형했는데 masking이라는 것을 사용합니다.</li>
      <li>masking은 position $i$보다 이후에 있는 position에 attention을 주지 못하게 합니다. 이를 통해 position $i$에 대해 미래 정보를 활용하지 못하도록 하는 것입니다.</li>
    </ul>
  </li>
</ul>

<h3 id="attention">Attention</h3>

<p>attention은 query와 key-value pair들을 output에 맵핑해주는 함수입니다. 출력은 values들의 weighted sum으로 계산되는데 value에 할당된 weight는 query와 대응되는 key의 compatibility function에 의해 계산됩니다.</p>

<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>

<p><img src="https://lh3.google.com/u/0/d/1-tli8qOupke7v5-XB97E-COUc6XH42eI" alt="" /></p>

<p>여기서 사용하는 attention을 Scaled Dot-Product Attention(SDPA)라 부르는데, input은 dimension이 $d_k$인 query와 key, dimension이 $d_v$인 value들로 이루어집니다.</p>

<p><img src="https://lh3.google.com/u/0/d/1xrZKQfNWlXCNM5738iO8B0TdLW_u7hB6" alt="" /></p>

<p>모든 query와 모든 key들에 대해 dot product로 계산되는데 각각의 결과에 $\sqrt{d_k}$로 나누어진다. 다음 value의 가중치를 얻기 위해 softmax 함수를 적용합니다.</p>

<p>attention 함수는 additive attention과 dot-product attention이 사용됩니다.</p>

<ul>
  <li>additive attention은 single hidden layer와 함께 feed-forward network에 compatibility function을 계산하는데 사용됩니다.</li>
  <li>dot-product attention이 좀 더 빠르고 실제로 space-efficient합니다. 왜냐하면 optimized matrix multiplication code를 사용해서 구현되었기 때문입니다.</li>
</ul>

<p>$d_k$가 작은 경우 additive attention이 dot product attention보다 성능이 좋습니다.</p>

<p>그러나 $d_k$가 큰 값인 경우 softmax 함수에서 기울기 변화가 거의 없는 region으로 이동하기 때문에 dot product를 사용하면서 $\sqrt{d_k}$으로 나누어 scaling을 적용했습니다.</p>

<h3 id="multi-head-attention">Multi-Head Attention</h3>

<p><img src="https://lh3.google.com/u/0/d/18yEv3-etuUHyzdplbfG-yV_ruLBPOtzy" alt="" /></p>

<p>$d_{model}$ dimension의 query, key, value 들로 하나의 attention을 수행하는 대신, query, key, value들에 각각 학습된 linear projection을 $h$번 수행하는 것이 더 좋습니다.</p>

<ul>
  <li>즉, $Q, K, V$에 각각 다른 weight를 곱해주는 것입니다.</li>
  <li>parameter matrix : $W_i^Q \in R^{d_{model}\times d_k}, \space w_i^K \in R^{d_{model}\times d_k}, \space W_i^V \in R^{d_{model}\times d_v}, \space W^O \in R^{hd_v \times d_{model}}$</li>
</ul>

<p><img src="https://lh3.google.com/u/0/d/1GyI80NxaXxZDUruxYBGCYgLtjV6LF2UZ" alt="" /></p>

<p>이때, projection이라 하는 이유는 각각의 값들이 parameter matrix와 곱해졌을 때, $d_k, d_v, d_{model}$차원으로 project되기 때문입니다. query, key, value들을 병렬적으로 attention function을 거쳐 dimension이 $d_v$인 output 값으로 나오게 됩니다.</p>

<p>이후 여러개의 head를 concatenate하고 다시 $W^O$와 projection하여 dimension이 $d_{model}$인 output 값으로 나오게 됩니다.</p>

<h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h3>

<p>인코더와 디코더는 fully connected feed-forward network를 가지고 있습니다.</p>

<p>또한, 두 번의 linear transformations과 activation function ReLU로 구성되어집니다.</p>

<p><img src="https://lh3.google.com/u/0/d/1aZhqVcl5FEpX-CZqG96r__WemGt_9JcH" alt="" /></p>

<p>각각의 position마다 같은 $W, b$를 사용하지만 layer가 달라지면 다른 parameter를 사용합니다.</p>

<h3 id="positional-encoding">Positional Encoding</h3>

<p>모델이 recurrence와 convolution을 사용하지 않기 때문에 문장안에 상대적인 혹은 절대적인 위치의 token들에 대한 정보를 주입해야만 했습니다.</p>

<p>이후 positional encoding이라는 것을 encoder와 decoder stack 밑 input embedding에 더해줬습니다.</p>

<p>positional encoding은 $d_{model}$인 dimension을 가지고 있기 때문에 둘을 더할 수 있습니다.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=11EUTF3UWb4pqpiEjTl4vjOODVWjRPVRN" alt="" /></p>

<ul>
  <li>$pos$는 position, $i$는 dimension</li>
</ul>

<p>$pos$는 sequence에서 단어의 위치이고 해당 단어는 $i$에 $0$부터 $\frac{d_{model}}{2}$까지 대입해 dimension이 $d_{model}$인 positional encoding vector를 얻을 수 있습니다.</p>

<h2 id="why-self-attention">Why Self-Attention</h2>

<p>Self-Attention을 사용하는 첫 번째 이유는 layer마다 total computational complexity가 작기 때문입니다.</p>

<p>두 번째 이유는 computation의 양이 parallelized하기때문에 sequential operation의 minimum으로 측정되기 때문입니다.</p>

<p><img src="https://lh3.google.com/u/0/d/1zSom0Z9VTnveWpCw-_LQeFEafPhk5r9t" alt="" /></p>

<p>세 번째 이유로는 네트워크에서의 long-range dependencies사이의 path length때문입니다. long-range dependencies를 학습하는 것은 많은 문장 번역 분야에서의 key challenge가 됩니다.</p>

<p>input sequence와 output sequence의 길이가 길어지면 두 position간의 거리가 멀어져 long-range dependencies를 학습하는데 어려워집니다.</p>

<p>테이블을 보면 Recurrent layer의 경우 Sequential operation에서 $O(n)$이 필요하지만 Self-Attention의 경우 상수시간에 실행될 수 있습니다.</p>

<p>또한 Self-Attention은 interpretable(설명가능한) model인 것이 이점입니다.</p>

<h2 id="traning">Traning</h2>
<h3 id="optimizer">Optimizer</h3>

<p>Adam optimizer와 $\beta_1=0.9$, $\beta_2=0.98$, $\epsilon=10^{-9}$를 사용했습니다.</p>

<p>학습동안 아래의 공식을 통해 learning rate를 변화시켰습니다.</p>

<p><img src="https://lh3.google.com/u/0/d/1aQqVdboUkvGkAqrxOeuirWmtQerdXRCo" alt="" /></p>

<p>이는 warmup_step에 따라 linear하게 증가시키고 step number에 따라 square root한 값을 통해 점진적으로 줄여갔습니다. 그리고 warmup_step = 4000을 사용했습니다.</p>

<h3 id="residual-dropout">Residual Dropout</h3>

<p>각 sub-layer에서 input을 더하는 것과 normalization을 하기전에 output에 dropout을 설정했습니다. 또한 encoder와 decoder stacks에 embedding의 합계와 positional encoding에도 dropout을 설정했습니다.</p>

<p>그리고 rate $P_{drop}=0.1$을 사용했습니다.</p>

<h3 id="label-smoothing">Label Smoothing</h3>

<p>학습하는 동안 label smoothing value $\epsilon_{ls}=0.1$를 적용했습니다.</p>

<h2 id="result">Result</h2>
<h3 id="machine-translation">Machine Translation</h3>

<p><img src="https://lh3.google.com/u/0/d/1GlZ6NFZ3XS1s63Xz7zP8f2aJY7NV21_d" alt="" /></p>

<p>영어→독일어 번역에서는 기존 모델들보다 높은 점수가 나왔고 영어→프랑스어 번역에서는 single 모델보다 좋고 ensemble 모델들과 비슷한 성능을 내주는 것을 볼 수 있습니다.</p>

<p>여기서 중요한 점은 Training Cost인데 기존 모델들보다 훨씬 적은 Cost가 들어가는 것을 볼 수 있습니다.</p>

<h3 id="model-variations">Model Variations</h3>

<p><img src="https://lh3.google.com/u/0/d/1W4prVn3P9rgL_SBWPH3M9dXhH8g7Q25v" alt="" /></p>

<ul>
  <li>테이블에 row (A)를 보면 single-head attention은 head=16일때보다 0.9 BLEU 낮고 head=32로 늘렸을 때도 head=16일때보다 BLEU가 낮습니다.</li>
  <li>row (B)를 보면 $d_k$를 낮추는 것이 model quality를 낮추게 합니다.</li>
  <li>row (C), (D)를 보면 더 큰 모델일수록 좋고, dropout이 overfitting을 피하는데 도움이 되는 것을 볼 수 있습니다.</li>
  <li>row (E)를 보면 sinusoidal position대신 learned positional embeddings를 넣었을 때의 결과가 base model과 동일한 결과인 것을 볼 수 있습니다.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>우리는 완전히 attention에 기반한 첫번째 문장번역 모델인 Transformer를 소개했다. encoder-decorder 아키텍쳐에서 자주 사용되는 recurrent 레이어들을 multi-headed self-attention으로 대체합니다.</p>

<p>번역 분야에서, Transformer는 recurrent 또는 convolutional 레이어 기반 아키텍쳐들보다 상당히 빠르게 학습할 수 있습니다.</p>

<p>WMT 2014 영어→독어, 영어→불어 번역 분야에서 우리는 새로운 SOTA를 얻을 수 있었다. Transformer는 기존 모든 앙상블 모델들을 능가하는 성능을 낼 수 있습니다.</p>

<p>우리는 attention-based models의 미래에 기대하고 있고 다른 분야에 적용할 계획이 있습니다.</p>

<p>Transformer를 텍스트 이외에 입력 및 출력 형식과 관련된 문제로 확장하고 이미지, 오디오, 비디오와 같은 대규모 입력과 출력을 효율적으로 다루기 위한 local, restricted attention 매커니즘을 조사할 계획입니다.</p>

<p>generation을 덜 순차적으로 만드는 것도 또 다른 연구 목표입니다.</p>
:ET