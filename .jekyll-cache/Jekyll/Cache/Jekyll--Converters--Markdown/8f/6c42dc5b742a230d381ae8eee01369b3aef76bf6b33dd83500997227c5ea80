I"q<<h2 id="rag">RAG</h2>
<p>RAG (Retrieval Augmented Generation)란, LLM이 학습한 데이터로부터 답변하지 않고 외부 문서(Context)를 참조할 수 있도록 하는 시스템을 말합니다. RAG는 LLM이 자신이 학습한 데이터가 아닌 외부의 최신 정보를 참조하여 답변의 부정확성이나 환각(Hallucination)을 줄일 수 있습니다.</p>

<h2 id="retriever">Retriever</h2>
<p>Retriever는 유저의 질문에 관련이 높은 문서를 가져오는 역할을 담당하는 구성요소입니다. 관련 문서를 가져오는 전략으로는 BM25, Bi-Encoder 등의 방식이 있습니다.</p>

<h3 id="lexical-search-bm25">Lexical Search: BM25</h3>
<p>BM25는 키워드 매칭을 통해 점수를 내는 방식을 말합니다. TF-IDF를 활용해 질의와 문서간의 유사도를 계산하여 빠르고 간단한 질문에 적합합니다. 하지만, 질의의 문맥을 이용하지 못해 관련된 문서가 검색되는 경우도 있습니다.</p>

<h3 id="semantic-search-bi-encoder">Semantic Search: Bi-Encoder</h3>
<p>Bi-Encoder는 질의와 문서간의 임베딩 벡터 유사도를 계산하여 점수를 내는 방식입니다. BERT와 같은 언어모델을 이용해 문맥적 의미가 담긴 임베딩 벡터를 생성하여 질의와 문서의 의미적인 유사성을 잘 찾아낼 수 있습니다. 미리 벡터로 변환하여 저장할 수 있어 검색 시 유사도만 계산하면 되므로 크게 느리지 않습니다. 하지만, 질의나 문서가 길어질수록 벡터로 표현할 때 정보가 손실될 수 있고 Query와 Candidates간의 관계성을 고려하지 않기 때문에 Cross-Encoder 대비 정확도가 떨어집니다.</p>

<h3 id="hybrid-search-lexical-search--semantic-search">Hybrid Search: Lexical Search + Semantic Search</h3>
<p>Hybrid Search는 유저 질의의 키워드를 중점으로 찾는 BM25와 유저 질의와 의미적으로 유사한 문서를 찾는 Bi-Encoder를 같이 사용하는 방법을 말합니다. Hybrid Search의 검색 결과는 Lexcial Search 혹은 Semantic Search를 단독으로 사용한 결과보다 정확한 문서를 검색할 수 있습니다. 검색 결과에 가중치를 부여하면 도메인 특성에 따라 최적화할 수 있습니다. 보통 자주 사용하는 계산방식으로는 CC(Convex Combination)과 RRF(Reciprocal Rank Fusion)이 있습니다.</p>

<p><strong>Convex Combination</strong></p>
<ul>
  <li>$score(doc) = \alpha * score_{lex}(doc) + \beta * score_{sem}(doc)$</li>
</ul>

<p>CC는 alpha와 beta을 각각 스코어에 곱해 최종 스코어를 계산하는 방식입니다. 두 스코어 값을 합치기 위해 각 스코어마다 Normalization 과정이 필요합니다.</p>

<p><strong>Reciprocal Rank Fusion</strong></p>
<ul>
  <li>$score(doc) = \frac{1}{k + rank_{lex}(doc)} + \frac{1}{k + rank_{sem}(doc)}$</li>
</ul>

<p>RRF는 가중치가 아닌 문서의 순위를 이용하는 방식입니다. 가중치를 이용하지 않기 때문에 Normalization 과정이 필요하지 않고 순위가 낮은 문서를 위해 조정하는 임의의 값입니다.</p>

<h3 id="cross-encoder">Cross-Encoder</h3>
<p>Cross-Encoder는 하나의 언어모델에 Query와 Candidate를 같이 넣어 유사도를 계산하는 방식입니다. Bi-Encoder 대비 예측 성능이 괜찮지만 Cross-Encoder를 Retreiver로 사용한다면 매 질의마다 모든 문서와의 유사도를 계산해야 하기 때문에 단독으로 사용하기 어렵습니다.</p>

<h2 id="reranking-전략">Reranking 전략</h2>
<p>Reranking 전략은 적절한 문서 후보군을 선별하고 Cross-Encoder로 순위를 재조정하는 전략을 말합니다. 이렇게 구성하면, Cross-Encoder만 사용했을 때보다 계산 비용이 줄고 정확도를 확보할 수 있는 방법입니다.</p>

<figure>
    <img src="https://framerusercontent.com/images/4atMCrE67i6XDQdSySevR8sVhM.png" />
    <figcaption>https://dify.ai/blog/hybrid-search-rerank-rag-improvement</figcaption>
</figure>

<p>그렇다면 RAG 시스템에서 Reranking 전략이 왜 필요한지 생각해볼 수 있는데 LLM의 답변 정확도가 Context의 순서와 관계가 있기 때문입니다.
<a href="https://arxiv.org/abs/2307.03172">Lost in the Middle: How Language Models Use Long Contexts</a> 논문에는 정답과 관련된 문서가 가운데 위치할 수록 답변 성능이 떨어지는 것을 확인할 수 있습니다. 따라서, 이러한 이유 때문에 RAG 시스템에서 Hybrid Search의 문서 순위를 재조정하는 Reranking 전략이 필요함을 알 수 있습니다.</p>

<h2 id="rag-구현">RAG 구현</h2>
<h3 id="pipeline">Pipeline</h3>
<figure style="text-align:center;">
    <a href="https://1drv.ms/i/c/502fd124b305ba80/IQTHCBiTNjXJSrDfn8ZtMHRJAXPh5Xy2ex78zUYezRLVn8s?width=1024" onclick="return false;" data-lightbox="gallery">
        <img src="https://1drv.ms/i/c/502fd124b305ba80/IQTHCBiTNjXJSrDfn8ZtMHRJAXPh5Xy2ex78zUYezRLVn8s?width=1024" alt="01" style="max-width: 80%; height: auto;" />
    </a>
</figure>

<p>RAG 파이프라인 순서는 다음과 같습니다.</p>
<ol>
  <li>유저가 질문을 보내면 <code class="language-plaintext highlighter-rouge">text-embedding-004</code> 모델로부터 임베딩 벡터를 받습니다.</li>
  <li>유저 쿼리와 임베딩 벡터를 이용해 Vector Store인 <code class="language-plaintext highlighter-rouge">Opensearch</code>에 Hybrid Search로 후보 문서들을 가져옵니다.</li>
  <li>후보 문서들을 Opensearch ML 기능을 이용해 Reranking합니다.</li>
  <li>최종 Context와 유저 질의를 LLM(<code class="language-plaintext highlighter-rouge">Gemini-2.0-flash</code>)에 전달해 답변을 받아옵니다.</li>
  <li>LLM의 답변을 유저에게 전달합니다.</li>
</ol>

<h3 id="opensearch">Opensearch</h3>
<p>Vector Store로 Opensearch를 선택했는데 그 이유는 lexical search와 semantic search 모두 지원하기 때문입니다. Semantic search는 knn(K-Nearest Neighbor) 알고리즘을 지원하고 있어 빠른 검색 결과를 받아볼 수 있습니다. 또한, Opensearch 클러스터에 ML모델을 배포하고 REST API로 사용할 수 있는 기능이 있어 이를 이용해 Reranking 전략을 사용할 수 있었습니다.</p>

<h3 id="gemini">Gemini</h3>
<p>LLM은 Google의 Gemini 모델을 선택했습니다. 그 이유는 1M의 토큰 컨텍스트를 지원하고 있어 프롬프트에 충분히 많은 Context들을 넣을 수 있다는 장점이 있기 때문입니다.</p>

<blockquote>
  <p>Gemini 2.0 Flash의 무료 등급은 시간당 1백만개의 토큰 제한이 걸려있습니다. 제품 개선에 사용 내역이 사용되기는 하지만 토이 프로젝트로 사용하기에 충분하다고 생각합니다.</p>
</blockquote>

<h3 id="demo-app">Demo APP</h3>
<p>많이 사용하는 프레임워크인 Streamlit을 사용해서 앞단을 구성했습니다.</p>
<figure>
    <a href="https://1drv.ms/i/c/502fd124b305ba80/IQRhkcrRacG5QoIRDWhpuPswAUx8mqDtVT69w05eslUP6sI?width=2828&amp;height=1342" onclick="return false;" data-lightbox="gallery">
        <img src="https://1drv.ms/i/c/502fd124b305ba80/IQRhkcrRacG5QoIRDWhpuPswAUx8mqDtVT69w05eslUP6sI?width=2828&amp;height=1342" alt="02" style="max-width: 100%; height: auto'" />
    </a>
    <figcaption>Frontend</figcaption>
</figure>

<p>사용자가 PDF파일을 업로드하면 파싱 후 Vector Store에 문서를 적재하고 아래 조건에 따라 유저 질의에 따라 문서를 가져올 수 있도록 구성했습니다.</p>

<h3 id="pdf-parse">PDF Parse</h3>
<p>PDF 문서 내에서 정보를 추출하는 라이브러리 중 하나인 Unstructured를 이용했습니다. 클라우드 서비스를 제공하고 있고 Langchain과 연동이 되어 있어 쉽게 사용가능했습니다. 하지만, PDF의 표를 추출하는 과정에서 문제가 발생했습니다.</p>

<p><strong>Llamaindex</strong><br />
PDF 문서를 Unstructured 라이브러리로 바로 파싱하는 경우, 테이블 형식이 제대도 파싱되지 않는 문제가 발생했습니다. 이를 해결하기 위해 중간 변환 과정이 필요했고 마크다운과 이미지 형식 두 가지를 고려했습니다. 이미지 변환 방식도 LLM이 이미지도 잘 해석해주기 때문에 가능한 방법이지만 그 만큼 비용이 들어가기 때문에 마크다운 변환을 선택하게 되었습니다. 마크다운 변환을 이용할 때는 <a href="https://www.llamaindex.ai/">Llamaindex</a>라는 상용 서비스를 이용했고 괜찮은 결과물을 반환해주었습니다.</p>

<blockquote>
  <p>Llamaindex의 Free Plan은 하루에 1000페이지 변환이 가능합니다. 이 정도면 토이 프로젝트로 사용할 때 충분하다고 생각됩니다.</p>
</blockquote>

<blockquote>
  <p>Llamaindex 말고도 Upstage에서도 <a href="https://www.upstage.ai/blog/en/let-llms-read-your-documents-with-speed-and-accuracy">Document Parse</a> 서비스를 제공중에 있습니다.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">llama_cloud_services</span> <span class="kn">import</span> <span class="n">LlamaParse</span>

<span class="n">llamaparse</span> <span class="o">=</span> <span class="n">LlamaParse</span><span class="p">(</span>
        <span class="n">result_type</span><span class="o">=</span><span class="s">"markdown"</span><span class="p">,</span>
        <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'LLAMA_CLOUD_API_KEY'</span><span class="p">)</span>
    <span class="p">).</span><span class="n">load_data</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Unstructured.io</strong><br />
Llamaindex로 변화된 마크다운들로부터 정보를 추출해야 합니다. Langchain에서 제공하는 UnstructuredMarkdownLoader를 이용해 문서를 파싱하고 Vector Store에 저장할 준비를 합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">UnstructuredMarkdownLoader</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">UnstructuredMarkdownLoader</span><span class="p">(</span>
        <span class="n">file_path</span><span class="o">=</span><span class="n">markdown_path</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s">"elements"</span><span class="p">,</span>
        <span class="n">chunking_strategy</span><span class="o">=</span><span class="s">"by_title"</span><span class="p">,</span>
        <span class="n">strategy</span><span class="o">=</span><span class="s">"hi_res"</span><span class="p">,</span>
        <span class="n">max_characters</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
        <span class="n">new_after_n_chars</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span>
        <span class="n">combine_text_under_n_chars</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="n">load</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>OpenSearchVectorSearch</strong><br />
Langchain에서 Opensearch를 벡터 스토어로 사용할 수 있도록 구현체를 제공합니다. 이 클래스를 이용하면 docs를 쉽게 Opensearch에 적재할 수 있습니다. 단, Opensearch에 문서들을 적재하기 전에 반드시 인덱스를 생성해야 합니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">OpenSearchVectorSearch</span>

<span class="n">vector_db</span> <span class="o">=</span> <span class="n">OpenSearchVectorSearch</span><span class="p">(</span>
        <span class="n">index_name</span><span class="o">=</span><span class="n">index_name</span><span class="p">,</span>
        <span class="n">opensearch_url</span><span class="o">=</span><span class="n">opensearch_url</span><span class="p">,</span>
        <span class="n">embedding_function</span><span class="o">=</span><span class="n">llm_emb</span><span class="p">,</span>
        <span class="n">http_auth</span><span class="o">=</span><span class="p">(</span><span class="n">opensearch_id</span><span class="p">,</span> <span class="n">opensearch_password</span><span class="p">),</span>
        <span class="n">use_ssl</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">verify_certs</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">is_aoss</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">engine</span><span class="o">=</span><span class="s">"faiss"</span><span class="p">,</span>
        <span class="n">space_type</span><span class="o">=</span><span class="s">"l2"</span><span class="p">,</span>
        <span class="n">bulk_size</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
        <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
        <span class="n">ssl_show_warn</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>

<span class="n">vector_db</span><span class="p">.</span><span class="n">add_documents</span><span class="p">(</span>
        <span class="n">documents</span><span class="o">=</span><span class="n">docs</span><span class="p">,</span>
        <span class="n">vector_field</span><span class="o">=</span><span class="s">'vector_field'</span><span class="p">,</span>
        <span class="n">bulk_size</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div>

<h3 id="retrievals">Retrievals</h3>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://aws.amazon.com/ko/blogs/tech/korean-reranker-rag/">한국어 Reranker를 활용한 검색 증강 생성(RAG) 성능 올리기</a></li>
  <li><a href="https://www.elastic.co/search-labs/blog/hybrid-search-elasticsearch">hyrbrid-search-elasticsearch</a></li>
  <li><a href="https://medium.com/@nuatmochoi/%ED%95%98%EC%9D%B4%EB%B8%8C%EB%A6%AC%EB%93%9C-%EA%B2%80%EC%83%89-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0-feat-ensembleretriever-knowledge-bases-for-amazon-bedrock-d6ef1a0daaf1">하이브리드 검색 구현하기 (feat. EnsembleRetriever, Knowledge Bases for Amazon Bedrock)</a></li>
  <li><a href="https://heygeronimo.tistory.com/101">[논문이해] Lost in the Middle: How Language Models Use Long Contexts</a></li>
</ul>
:ET