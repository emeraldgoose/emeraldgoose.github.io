I"º<h2 id="introduction-to-dense-embedding">Introduction to Dense Embedding</h2>

<h3 id="limitations-of-sparse-embedding">Limitations of sparse embedding</h3>

<ol>
  <li>ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì˜ ê²½ìš° ì‚¬ì‹¤ìƒ 0ì´ ë˜ë©´ì„œ 90%ì´ìƒì˜ ë²¡í„° ë””ë©˜ì…˜ë“¤ì´ 0ì´ ë˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•œë‹¤.</li>
  <li>ì°¨ì›ì˜ ìˆ˜ê°€ ë§¤ìš° í¬ë‹¤ â†’ compressed formatìœ¼ë¡œ ê·¹ë³µê°€ëŠ¥</li>
  <li>ê°€ì¥ í° ë‹¨ì ì€ ìœ ì‚¬ì„±ì„ ê³ ë ¤í•˜ì§€ ëª»í•œë‹¤.</li>
</ol>

<h3 id="dense-embedding">Dense Embedding</h3>

<blockquote>
  <p>Complementary to sparse representations by design</p>
</blockquote>

<ul>
  <li>ë” ì‘ì€ ì°¨ì›ì˜ ê³ ë°€ë„ ë²¡í„°(length = 50 ~ 1000)</li>
  <li>ê° ì°¨ì› íŠ¹ì • termì— ëŒ€ì‘ë˜ì§€ ì•ŠìŒ</li>
  <li>ëŒ€ë¶€ë¶„ì˜ ìš”ì†Œê°€ nonzeroê°’</li>
</ul>

<h3 id="retrieval-sparse-vs-dense">Retrieval: Sparse vs. Dense</h3>

<ul>
  <li>Sparse Embedding
    <ul>
      <li>ì¤‘ìš”í•œ Termë“¤ì´ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•˜ëŠ” ê²½ìš° ì„±ëŠ¥ì´ ë›°ì–´ë‚¨</li>
      <li>ì„ë² ë”©ì´ êµ¬ì¶•ë˜ê³  ë‚˜ì„œëŠ” ì¶”ê°€ì ì¸ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•¨</li>
    </ul>
  </li>
  <li>Dense Embedding
    <ul>
      <li>ë‹¨ì–´ì˜ ìœ ì‚¬ì„± ë˜ëŠ” ë§¥ë½ì„ íŒŒì•…í•´ì•¼ í•˜ëŠ” ê²½ìš° ì„±ëŠ¥ì´ ë›°ì–´ë‚¨</li>
      <li>í•™ìŠµì„ í†µí•´ ì„ë² ë”©ì„ ë§Œë“¤ë©° ì¶”ê°€ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•¨</li>
    </ul>
  </li>
  <li>ìµœê·¼ ì‚¬ì „í•™ìŠµ ëª¨ë¸, ê²€ìƒ‰ ê¸°ìˆ  ë“±ì˜ ë°œì „ ë“±ìœ¼ë¡œ ì¸í•´ Dense Embeddingì„ í™œë°œíˆ ì´ìš©</li>
</ul>

<h2 id="training-dense-encoder">Training Dense Encoder</h2>

<h3 id="what-can-be-dense-encoder">What can be Dense Encoder?</h3>

<p>BERTì™€ ê°™ì€ Pre-trained language model (PLM)ì´ ìì£¼ ì‚¬ìš©ë˜ê³  ê·¸ ì™¸ ë‹¤ì–‘í•œ neural network êµ¬ì¡°ë„ ê°€ëŠ¥í•˜ë‹¤.<br />
Questionê³¼ Passageë¥¼ Encoderì— ë„£ì–´ ì¸ì½”ë”©í•œ í›„ ë‘˜ì„ dot productí•˜ì—¬ ìœ ì‚¬ë„ Scoreë¥¼ ë§¤ê¸´ë‹¤. ì´ ì¤‘ ê°€ì¥ í° ì ìˆ˜ë¥¼ ê°€ì§„ Passageë¥¼ ì„ íƒí•˜ì—¬ ì¿¼ë¦¬ì— ë‹µì„ í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.</p>

<h3 id="dense-encoder-í•™ìŠµ-ëª©í‘œì™€-í•™ìŠµ-ë°ì´í„°">Dense Encoder í•™ìŠµ ëª©í‘œì™€ í•™ìŠµ ë°ì´í„°</h3>

<ul>
  <li>í•™ìŠµëª©í‘œ : ì—°ê´€ëœ questionê³¼ passage dense embedding ê°„ì˜ ê±°ë¦¬ë¥¼ ì¢íˆëŠ” ê²ƒ(ë˜ëŠ” inner productë¥¼ ë†’ì´ëŠ” ê²ƒ) = higher similarity</li>
  <li>Challenge : ì—°ê´€ëœ questionê³¼ passageë¥¼ ì–´ë–»ê²Œ ì°¾ì„ ê²ƒì¸ê°€?
    <ul>
      <li>ê¸°ì¡´ MRC ë°ì´í„°ì…‹ì„ í™œìš©</li>
    </ul>
  </li>
</ul>

<h3 id="negative-sampling">Negative Sampling</h3>

<ul>
  <li>ì—°ê´€ëœ questionê³¼ passageê°„ì˜ dense embedding ê±°ë¦¬ë¥¼ ì¢íˆëŠ” ê²ƒ (higher similarity) = Positive</li>
  <li>ì—°ê°„ë˜ì§€ ì•ŠëŠ” embeddingê°„ì˜ ê±°ë¦¬ëŠ” ë©€ì–´ì•¼ í•¨ = Negative</li>
  <li>Chosing negative examples:
    <ul>
      <li>Corpus ë‚´ì—ì„œ ëœë¤í•˜ê²Œ ë½‘ê¸°</li>
      <li>ì¢€ ë” í—·ê°ˆë¦¬ëŠ” negative ìƒ˜í”Œë“¤ ë½‘ê¸° (ex. ë†’ì€ TF-IDF ìŠ¤ì½”ì–´ë¥¼ ê°€ì§€ì§€ë§Œ ë‹µì„ í¬í•¨í•˜ì§€ ì•ŠëŠ” ìƒ˜í”Œ)</li>
    </ul>
  </li>
</ul>

<h3 id="objective-function">Objective function</h3>

<p>Positive passageì— ëŒ€í•œ negative log likelihood (NLL) loss ì‚¬ìš©</p>

<h3 id="evalution-metric-for-dense-encoder">Evalution Metric for Dense Encoder</h3>

<p>Top-k retrieval accuracy: retrieveëœ passage ì¤‘ì—ì„œ ë‹µì„ í¬í•¨í•˜ëŠ” passageì˜ ë¹„ìœ¨</p>

<h2 id="passage-retrieval-with-dense-encoder">Passage Retrieval with Dense Encoder</h2>

<h3 id="from-dense-encoding-to-retrieval">From dense encoding to retrieval</h3>

<p>Inference: Passageì™€ queryë¥¼ ê°ê° embeddingí•œ í›„, queryë¡œë¶€í„° ê°€ê¹Œìš´ ìˆœì„œëŒ€ë¡œ passageì˜ ìˆœìœ„ë¥¼ ë§¤ê¹€</p>

<h3 id="from-retrieval-to-open-domain-question-answering">From retrieval to open-domain question answering</h3>

<p>Retrieverë¥¼ í†µí•´ ì°¾ì•„ë‚¸ Passageì„ í™œìš©, MRC(Machine Reading Comprehension) ëª¨ë¸ë¡œ ë‹µì„ ì°¾ìŒ</p>

<h3 id="how-to-make-better-dense-encoding">How to make better dense encoding</h3>

<ul>
  <li>í•™ìŠµ ë°©ë²• ê°œì„  (e.g. DPR)</li>
  <li>ì¸ì½”ë” ëª¨ë¸ ê°œì„  (BERTë³´ë‹¤ í°, ì •í™•í•™ pretrained ëª¨ë¸)</li>
  <li>ë°ì´í„° ê°œì„  (ë” ë§ì€ ë°ì´í„°, ì „ì²˜ë¦¬, ë“±)</li>
</ul>
:ET