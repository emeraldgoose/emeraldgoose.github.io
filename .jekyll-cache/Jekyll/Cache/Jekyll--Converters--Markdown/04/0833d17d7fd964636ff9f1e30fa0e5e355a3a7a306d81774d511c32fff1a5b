I"%‰<h3 id="motivation">Motivation</h3>
<blockquote>
  <p><strong>ë¹…ë°ì´í„°ë¥¼ ì§€íƒ±í•˜ëŠ” ê¸°ìˆ </strong>ì„ ì½ë‹¤ê°€ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì— ì‚¬ìš©ë˜ëŠ” í”Œë«í¼ë“¤ì„ ì „ì²´ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ êµ¬ì¶•í•´ë³´ê³  ì‹¶ì–´ì„œ
ì´ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/50171632/205239615-69152b4b-112b-492e-ae90-ef752b436f6b.png" alt="" width="700" /></p>

<h3 id="data">Data</h3>
<p>ë¨¼ì €, ìˆ˜ì§‘í•  ë°ì´í„°ëŠ” nginxë¡œë¶€í„° ë‚˜ì˜¤ëŠ” ë¡œê·¸ë¥¼ ìƒê°í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë§ì€ ì–‘ì˜ ë¡œê·¸ë¥¼ ìƒì‚°í•˜ë ¤ë©´ nginxë¡œë¶€í„° ë‚˜ì˜¤ê²Œ í•˜ê¸°ëŠ” ì–´ë ¤ì›Œì„œ python ì½”ë“œë¡œ ë¹„ìŠ·í•œ nginx ë¡œê·¸ë¥¼ ìƒì„±í•˜ê³  /var/log/httpd/access_log/*.logì— logging ëª¨ë“ˆë¡œ ê¸°ë¡í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ë¡œê·¸ë¥¼ ìƒì‚°í–ˆìŠµë‹ˆë‹¤.</p>

<p>ìƒì‚°ë˜ëŠ” ë¡œê·¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>206.176.215.237 - - [02/Dec/2022:18:57:34 +0900] "GET /api/items HTTP/1.1" 200 3456 477 "https://www.dummmmmy.com" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Mobile/15E148 Safari/604.1"
</code></pre></div></div>

<h3 id="producerfilebeat">Producer(FileBeat)</h3>
<p>ì„œë²„ ì ‘ì† ê¸°ë¡ì„ ë¡œê¹…í•˜ëŠ” ì„œë²„ì—ì„œ ë¡œê·¸ë¥¼ ì™¸ë¶€ë¡œ ë³´ë‚´ì£¼ëŠ” ë¬´ì–¸ê°€ í•„ìš”í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ íŒŒì¼ì„ ELK ìŠ¤íƒì˜ logstashë¡œ ì½ëŠ” ë°©ë²•ì´ ìˆì§€ë§Œ Elasticsearchì™€ HDFSì— ì ì¬í•˜ë ¤ë©´ losgtashë¥¼ ë°–ìœ¼ë¡œ ë¹¼ë‚´ ìˆ˜ì§‘ ì„œë²„ë¥¼ ë”°ë¡œ ë‘ê³  ì„œë²„ì—ëŠ” logstashì™€ ì˜ ë§ëŠ” FileBeatë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë§ë‹¤ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤.</p>

<p>FileBeatëŠ” Logstashì˜ ë¬´ê²ë‹¤ëŠ” ë‹¨ì ì„ ë³´ì™„í•˜ì—¬ ê°œë°œëœ ë¡œê·¸ ìˆ˜ì§‘ê¸°ì…ë‹ˆë‹¤. ë¡œê·¸íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì„¤ì •í•˜ë©´ offsetì„ ê¸°ì–µí•´ ì¶”ê°€ë˜ëŠ” ë¡œê·¸ë¥¼ ì™¸ë¶€ë¡œ ì „ë‹¬í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.</p>

<p>FileBeatëŠ” /var/log/httpd/access_log/*.log íŒŒì¼ì„ ì½ì–´ Logstash ì„œë²„ë¡œ ì¶”ê°€ëœ ë¡œê·¸ë¥¼ ì „ë‹¬í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. FileBeatë¥¼ ì‚¬ìš©í•˜ë©´ Logstashì—ì„œ ë³„ë‹¤ë¥¸ ì„¤ì • ì—†ì´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì ë„ ì„ íƒì— ì˜í–¥ì´ ìˆì—ˆìŠµë‹ˆë‹¤.</p>

<h3 id="logstash">Logstash</h3>
<p>LogstashëŠ” ì „ë‹¬ë°›ì€ ë¡œê·¸ë¥¼ Elasticsearchë‚˜ ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. Logstashë¥¼ ì‚¬ìš©í•œ ì´ìœ ëŠ” ëŒë‹¤ ì•„í‚¤í…ì²˜ê°™ì€ íŒŒì´í”„ë¼ì¸ì„ ìƒê°í•˜ê³  ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                      â”Œ-- Elasticsearch -- Kibana
FileBeat -- Logstash -| 
                      â””-- HDFS ------- Postgresql
</code></pre></div></div>

<p>ëŒë‹¤ ì•„í‚¤í…ì²˜ì²˜ëŸ¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘ë˜ì–´ ë³´ì—¬ì£¼ëŠ” ë·°ì™€ ë°°ì¹˜ ì²˜ë¦¬ë˜ì–´ ë³´ì—¬ì£¼ëŠ” ë·°ë¥¼ ì œê³µí•˜ëŠ” êµ¬ì¡°ì¸ë° logstashëŠ” ì—¬ëŸ¬ ê²½ë¡œì˜ Outputì„ ì§€ì›í•˜ê³  ìˆê¸° ë•Œë¬¸ì— ì í•©í•˜ë‹¤ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤.</p>

<p>logstashëŠ” *.conf íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë°ì´í„° ê°€ê³µì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì €ëŠ” ê° í•­ëª©ê³¼ ipì˜ ìœ„ì¹˜ì£¼ì†Œ, User Agent ì •ë³´ë¥¼ íŒŒì‹±í•˜ëŠ” í•„í„°ë¥¼ ë„£ì–´ íŒŒì‹±í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ íŒŒì‹±í• ë•ŒëŠ” grokì„ ì‚¬ìš©í–ˆê³  ë‹¤ìŒê³¼ ê°™ì€ ì„¤ì •ê°’ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. geoipì™€ useragent í”ŒëŸ¬ê·¸ì¸ì„ ì‚¬ìš©í•˜ë©´ IPì˜ ìœ„ì¹˜(êµ­ê°€, ë„ì‹œ)ì™€ ì ‘ì†í•œ ë¸Œë¼ìš°ì €, OS ë“±ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>filter {
  grok {
    match =&gt; {
      "message" =&gt; "%{IPORHOST:remote_addr} - %{USER:remote_user} \[%{HTTPDATE:time_local}\] \"%{WORD:method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})\" %{NUMBER:status} (?:%{NUMBER:body_bytes_sent}|-) (?:%{NUMBER:response_time}|-) \"%{GREEDYDATA:referrer}\" \"%{GREEDYDATA:UA}\""
    }
  }
  geoip {
    source =&gt; "remote_addr"
    target =&gt; "clientgeoip"
  }
  useragent {
    source =&gt; "UA"
  }
}
</code></pre></div></div>

<h3 id="elasticsearch-kibana">Elasticsearch, Kibana</h3>
<p>ElasticsearchëŠ” logstashë¡œë¶€í„° ì „ë‹¬ë°›ì€ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” DBì—­í• ì„ í•©ë‹ˆë‹¤. KibanaëŠ” Elasticsearchì˜ ë°ì´í„°ë¥¼ ë³´ì—¬ì£¼ëŠ” ëŒ€ì‹œë³´ë“œ ì—­í• ì„ í•©ë‹ˆë‹¤.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FileBeat -- Logstash -- Elasticsearch -â”¬- Kibana
                                       |
                                       â””- HDFS
</code></pre></div></div>
<p>ìœ„ì™€ ê°™ì€ íŒŒì´í”„ë¼ì¸ì„ ìƒê°í•´ë³´ê¸´ í–ˆëŠ”ë° ë‚®ì€ ì‚¬ì–‘ì˜ ES ì„œë²„ì— ë°ì´í„° ì €ì¥ê³¼ ì¶”ì¶œì„ í•˜ëŠ” ê²ƒì´ ë„ˆë¬´ ë¶€ë‹´ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ESëŠ” ë”°ë¡œ ë‘ê³  Logstashì—ì„œ HDFSë¡œ ì „ë‹¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.</p>

<p>Dockerfileì„ ë”°ë¡œ ì‘ì„±í•˜ì§€ ì•Šì•˜ëŠ”ë° Elasticsearchì™€ Kibanaê¹Œì§€ ë„ì»¤ë¡œ ì˜¬ë¦¬ë©´ ë§¥ë¶ì´ ê°ë‹¹í•˜ì§€ ëª»í•  ê²ƒ ê°™ì•„ì„œ ì„œë²„ë¥¼ ë¹Œë ¤ì£¼ëŠ” í”Œë«í¼ì„ ì•Œì•„ë³´ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.</p>

<p>ì²˜ìŒì—ëŠ” GCP í”„ë¦¬í‹°ì–´ë¥¼ ìƒê°í–ˆë‹¤ê°€ <a href="https://ide.goorm.io">êµ¬ë¦„</a>ì´ ìƒê°ë‚˜ì„œ ì´ê³³ì— ì„¤ì¹˜í–ˆìŠµë‹ˆë‹¤. êµ¬ë¦„ideê°€ ë¹Œë ¤ì£¼ëŠ” ì„œë²„ ìì›ì´ ì¢‹ì§€ëŠ” ì•Šì§€ë§Œ í•­ìƒ ì¼œë‘˜ ìˆ˜ ìˆê³  *.run.goorm.ioë¼ëŠ” ë„ë©”ì¸ë„ ì œê³µë˜ì–´ ì‚¬ìš©í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ì£¼ì†Œë¡œ Kibanaì— ì ‘ì†í•  ìˆ˜ ìˆì§€ë§Œ ë™ì‘ì´ ëŠë¦¬ë¯€ë¡œ ì¡°ê¸ˆ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.</p>
<ul>
  <li><a href="https://dashboard-kibana.run.goorm.io">dashboard-kibana.run.goorm.io</a></li>
  <li>ë§Œì•½, logstashë¡œ êµ¬ë¦„ì— ìˆëŠ” elasticsearchë¡œ ì—°ê²°í•˜ë ¤ë©´ í¬íŠ¸í¬ì›Œë”© ì„¸íŒ…ì„ í•˜ê³  portëŠ” 443ìœ¼ë¡œ ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤.</li>
</ul>

<p>Kibanaë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ì‹œë³´ë“œë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1fzCVL6zVHpJLPep1bzde5kxO-KfAgEjj" alt="" width="800" /></p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=126_Mpra87ei2ONdB7K_LcnXMsr8XY3of" alt="" width="800" /></p>

<h3 id="hdfs">HDFS</h3>
<p>ë¡œê·¸ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë°°ì¹˜ì²˜ë¦¬í•˜ë ¤ë©´ ë¨¼ì € ì €ì¥ë  ê³µê°„ì´ í•„ìš”í–ˆìŠµë‹ˆë‹¤. logstashë¡œë¶€í„° ì˜¨ ë°ì´í„°ë“¤ì€ ë¨¼ì € hdfsì— ì €ì¥ë˜ê³  ë°°ì¹˜ì²˜ë¦¬ë¥¼ í†µí•´ RDBë¡œ ì €ì¥ë˜ëŠ” ê³¼ì •ì„ ìƒê°í–ˆìŠµë‹ˆë‹¤.</p>

<p>í•˜ë‘¡ì´ ì„¤ì¹˜ë˜ëŠ” ë„ì»¤ë¥¼ ë” ëŠ˜ë¦¬ê¸°ëŠ” ì–´ë ¤ì›Œì„œ ë‹¨ì¼ ë…¸ë“œë¡œ ì‚¬ìš©í•˜ì§€ë§Œ ì„¤ì •ì€ ë¶„ì‚° ì„¤ì •ì´ ë˜ì–´ìˆëŠ” ëª¨ë“œì¸ Pseudo Distribute ëª¨ë“œë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.</p>

<p>HDFSëŠ” íŠ¹ì„±ìƒ íŒŒì¼ì„ ì—¬ëŸ¬ë²ˆ ìˆ˜ì •í•˜ëŠ”ë° ì¢‹ì§€ ì•Šì•„ ë°ì´í„°ë¥¼ ëª¨ì•„ í° íŒŒì¼ì„ í•œë²ˆì— ì ì¬í–ˆìŠµë‹ˆë‹¤. Logstashë„ Outputìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‚´ë³´ë‚´ì–´ ì¤‘ê°„ì— ì €ì¥í•  ê³µê°„ì´ í•„ìš”í–ˆìŠµë‹ˆë‹¤. ì €ëŠ” ì´ ê³µê°„ì´ In-memory DBê°€ ì ë‹¹í•˜ë‹¤ê³  ìƒê°ë˜ì–´ì„œ Redisë¥¼ ë‘ì–´ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  Spark ìŠ¤í¬ë¦½íŠ¸ë¡œ HDFSì— ì ì¬í–ˆìŠµë‹ˆë‹¤.</p>

<h3 id="redis">Redis</h3>
<p>Logstashë¡œë¶€í„° í•˜ë£¨ ê°„ê²©ì˜ ë°ì´í„°ë¥¼ ë°›ì•„ hdfsë¡œ í•œë²ˆì— ì ì¬í•˜ê¸° ìœ„í•´ logstashì™€ HDFSì‚¬ì´ì— ì„ì‹œ ë°ì´í„° ì €ì¥ì†Œê°€ í•„ìš”í–ˆìŠµë‹ˆë‹¤. kafkaëŠ” í˜„ì—…ì—ì„œ ìì£¼ ì“°ì´ëŠ” í”Œë«í¼ì´ì§€ë§Œ zookeeperê°€ ì¶”ê°€ë¡œ ì„¤ì¹˜ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ë„ì»¤ë¥¼ ì¶”ê°€ë¡œ ì˜¬ë¦¬ëŠ”ë° ë¶€ë‹´ë˜ì–´ ì œì™¸í–ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ redisë¥¼ ì„ íƒí•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.</p>

<p>LogstashëŠ” redisë¡œ ë³´ë‚¼ë•Œ keyë¥¼ ì§€ì •í•´ì•¼í•©ë‹ˆë‹¤. keyëŠ” ê·¸ë‚  ë‚ ì§œë¡œ ì§€ì •í•˜ì—¬ ì—°ì†ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ redisë¡œ ì „ë‹¬í•˜ì—¬ í•˜ë£¨ ê°„ê²©ì˜ ë°°ì¹˜ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë  ë•Œ ì–´ì œ ë‚ ì§œë¡œ key ì ‘ê·¼í•˜ì—¬ ë°ì´í„°ë¥¼ ëª¨ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>output {
    redis {
        host =&gt; ["redis"]
        port =&gt; 6379
        data_type =&gt; "list"
        key =&gt; "%{+YYYYMMdd}"
    }
}
</code></pre></div></div>

<h3 id="spark">Spark</h3>
<p>SparkëŠ” í•˜ë‘¡ì´ ì„¤ì¹˜ëœ ë„ì»¤ì— ê°™ì´ ì„¤ì¹˜í–ˆìŠµë‹ˆë‹¤. ì²˜ìŒì—ëŠ” í•˜ë‘¡ì˜ Yarnì˜ ê´€ë¦¬ë¥¼ ë°›ê²Œ í•˜ë ¤ê³  ì„¤ì¹˜í–ˆì§€ë§Œ ë‹¨ì¼ ë…¸ë“œë¡œ ëŒë¦¬ëŠë¼ localê³¼ yarnì˜ ì°¨ì´ê°€ ë‚˜ì§€ëŠ” ì•Šì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ pyspark ìŠ¤í¬ë¦½íŠ¸ë¥¼ spark-submit ëª…ë ¹ì–´ë¡œ ì‹¤í–‰í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span>
<span class="kn">import</span> <span class="nn">json</span><span class="p">,</span> <span class="n">argparse</span><span class="p">,</span> <span class="n">datetime</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">redis</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="k">def</span> <span class="nf">get_logs_from_redis</span><span class="p">(</span><span class="n">key</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rq</span> <span class="o">=</span> <span class="n">redis</span><span class="p">.</span><span class="n">Redis</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s">'redis'</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">6379</span><span class="p">,</span> <span class="n">db</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">rq</span><span class="p">.</span><span class="n">llen</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">rq</span><span class="p">.</span><span class="n">brpop</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">json_normalize</span><span class="p">(</span><span class="n">value</span><span class="p">).</span><span class="n">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s">'records'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">r</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">start_date</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--start_date'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">start_date</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">strptime</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">start_date</span><span class="p">,</span> <span class="s">'%Y-%m-%d'</span><span class="p">)</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">start_date</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y%m%d'</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">get_logs_from_redis</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">'Warehouse'</span><span class="p">).</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">df</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="n">parquet</span><span class="p">(</span><span class="sa">f</span><span class="s">'hdfs://hadoop-spark:9000/warehouse/</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">.parquet'</span><span class="p">,</span><span class="n">mode</span><span class="o">=</span><span class="s">'append'</span><span class="p">)</span>
    <span class="n">spark</span><span class="p">.</span><span class="n">stop</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="airflow">Airflow</h3>
<p>ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë„ë¡ Airflowë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. í•˜ë‘¡ ë„ì»¤ì—ì„œ spark-submitì„ ì‹¤í–‰í•˜ëŠ” ì»¤ë§¨ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ SSHOperatorê°€ í¬í•¨ëœ íƒœìŠ¤í¬ì™€ hdfsì—ì„œ DBë¡œ ì ì¬í•˜ëŠ” ë°°ì¹˜ì²˜ë¦¬í•˜ëŠ” íƒœìŠ¤í¬ë¥¼ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° ì–‘ë„ ì ê³  ë¹ ë¥´ê²Œ í™•ì¸í•˜ê¸° ìœ„í•´ ëª¨ë‘ @dailyë¡œ ì‚¬ìš©í•˜ì—¬ í•˜ë£¨ ê°„ê²©ìœ¼ë¡œ ì‹¤í–‰í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤.</p>

<p>spark-submitì„ ì‚¬ìš©í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">pendulum</span>
<span class="kn">from</span> <span class="nn">airflow.decorators</span> <span class="kn">import</span> <span class="n">dag</span>
<span class="kn">from</span> <span class="nn">airflow.providers.ssh.operators.ssh</span> <span class="kn">import</span> <span class="n">SSHOperator</span>
<span class="kn">from</span> <span class="nn">airflow.providers.ssh.hooks.ssh</span> <span class="kn">import</span> <span class="n">SSHHook</span>

<span class="n">kst</span> <span class="o">=</span> <span class="n">pendulum</span><span class="p">.</span><span class="n">timezone</span><span class="p">(</span><span class="s">'Asia/Seoul'</span><span class="p">)</span>
<span class="n">now</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d'</span><span class="p">)</span>
<span class="n">one_day_ago</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">(</span><span class="n">tz</span><span class="o">=</span><span class="n">kst</span><span class="p">)</span> <span class="o">-</span> <span class="n">datetime</span><span class="p">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="o">@</span><span class="n">dag</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s">'logs_redis_to_hdfs'</span><span class="p">,</span> 
    <span class="n">schedule_interval</span><span class="o">=</span><span class="s">'@daily'</span><span class="p">,</span> 
    <span class="n">start_date</span><span class="o">=</span><span class="n">one_day_ago</span><span class="p">,</span> 
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s">'batch'</span><span class="p">,</span><span class="s">'redis'</span><span class="p">,</span><span class="s">'hdfs'</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">parquet_to_hdfs_from_logstash</span><span class="p">():</span>
    <span class="n">hook</span> <span class="o">=</span> <span class="n">SSHHook</span><span class="p">(</span>
        <span class="n">remote_host</span><span class="o">=</span><span class="s">'hadoop-spark'</span><span class="p">,</span>
        <span class="n">username</span><span class="o">=</span><span class="s">'root'</span><span class="p">,</span>
        <span class="n">key_file</span><span class="o">=</span><span class="s">'/root/.ssh/id_rsa.pub'</span>
    <span class="p">)</span>

    <span class="n">run_script</span> <span class="o">=</span> <span class="n">SSHOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s">'run_script'</span><span class="p">,</span>
        <span class="n">ssh_hook</span><span class="o">=</span><span class="n">hook</span><span class="p">,</span>
        <span class="n">command</span><span class="o">=</span><span class="sa">f</span><span class="s">'/spark/bin/spark-submit /spark/logs_redis_to_hdfs.py --start_date </span><span class="si">{</span><span class="n">now</span><span class="si">}</span><span class="s">'</span><span class="p">,</span>
    <span class="p">)</span>
    
    <span class="n">run_script</span>
    
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">parquet_to_hdfs_from_logstash</span><span class="p">()</span>
</code></pre></div></div>
<p>sshë¡œ í•˜ë‘¡ì´ ì„¤ì¹˜ëœ ë„ì»¤ë¡œ ì ‘ì†í•˜ì—¬ SSHOperatorë¡œ commandë¥¼ ì‹¤í–‰í•˜ëŠ” DAGì…ë‹ˆë‹¤. sshë¡œ ì ‘ì†í•˜ê¸° ìœ„í•´ airflow ë„ì»¤ì™€ í•˜ë‘¡ ë„ì»¤ì˜ <code class="language-plaintext highlighter-rouge">~/.ssh/</code> í´ë”ë¥¼ ê³µìœ ì‹œì¼œ í•˜ë‘¡ì—ì„œ ìƒì„±ëœ key íŒŒì¼ì„ airflowì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í–ˆìŠµë‹ˆë‹¤.</p>

<p>RDBë¡œ ì ì¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">pyspark</span>
<span class="kn">import</span> <span class="nn">sqlalchemy</span>
<span class="kn">import</span> <span class="nn">pendulum</span>
<span class="kn">from</span> <span class="nn">airflow.decorators</span> <span class="kn">import</span> <span class="n">dag</span><span class="p">,</span> <span class="n">task</span>

<span class="n">kst</span> <span class="o">=</span> <span class="n">pendulum</span><span class="p">.</span><span class="n">timezone</span><span class="p">(</span><span class="s">"Asia/Seoul"</span><span class="p">)</span>
<span class="n">yesterday</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">(</span><span class="n">tz</span><span class="o">=</span><span class="n">kst</span><span class="p">)</span> <span class="o">-</span> <span class="n">datetime</span><span class="p">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="o">@</span><span class="n">dag</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s">'store_to_postgres'</span><span class="p">,</span> 
    <span class="n">schedule_interval</span><span class="o">=</span><span class="s">'@daily'</span><span class="p">,</span> 
    <span class="n">start_date</span><span class="o">=</span><span class="n">yesterday</span><span class="p">,</span> 
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s">'batch'</span><span class="p">,</span><span class="s">'hdfs'</span><span class="p">,</span><span class="s">'rdb'</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">batch_to_rdb</span><span class="p">():</span>
    <span class="o">@</span><span class="n">task</span>
    <span class="k">def</span> <span class="nf">get_logs_from_hdfs</span><span class="p">():</span>
        <span class="n">sc</span> <span class="o">=</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">SparkContext</span><span class="p">(</span><span class="n">master</span><span class="o">=</span><span class="s">'local'</span><span class="p">,</span> <span class="n">conf</span><span class="o">=</span><span class="n">pyspark</span><span class="p">.</span><span class="n">SparkConf</span><span class="p">())</span>
        <span class="n">sqlContext</span> <span class="o">=</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">sql</span><span class="p">.</span><span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">parquet</span><span class="p">(</span>
            <span class="sa">f</span><span class="s">'hdfs://hadoop-spark:9000/warehouse/</span><span class="si">{</span><span class="n">yesterday</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d"</span><span class="p">)</span><span class="si">}</span><span class="s">.parquet'</span><span class="p">)</span>
        <span class="n">df</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">'data.parquet'</span><span class="p">)</span>
    
    <span class="o">@</span><span class="n">task</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">():</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s">'data.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span>
            <span class="n">columns</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s">"clientgeoip.geo.country_name"</span> <span class="p">:</span> <span class="s">"country_name"</span><span class="p">,</span>
                <span class="s">"clientgeoip.geo.region_name"</span> <span class="p">:</span> <span class="s">"region_name"</span><span class="p">,</span>
                <span class="s">"clientgeoip.geo.city_name"</span> <span class="p">:</span> <span class="s">"city_name"</span><span class="p">,</span>
                <span class="s">"user_agent.name"</span> <span class="p">:</span> <span class="s">"browser"</span><span class="p">,</span>
                <span class="s">"user_agent.device.name"</span> <span class="p">:</span> <span class="s">"device"</span><span class="p">,</span>
                <span class="s">"user_agent.os.name"</span> <span class="p">:</span> <span class="s">"os_name"</span><span class="p">,</span>
                <span class="s">"user_agent.os.version"</span> <span class="p">:</span> <span class="s">"os_version"</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="n">df</span><span class="p">[</span><span class="s">'timestamp'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span>
            <span class="n">df</span><span class="p">[</span><span class="s">'time_local'</span><span class="p">],</span> 
            <span class="nb">format</span><span class="o">=</span><span class="s">'%d/%b/%Y:%H:%M:%S +0900'</span><span class="p">).</span><span class="n">dt</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%dT%H:%M:%S'</span><span class="p">))</span>

        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span>
            <span class="p">[</span><span class="s">'timestamp'</span><span class="p">,</span><span class="s">'UA'</span><span class="p">,</span><span class="s">'body_bytes_sent'</span><span class="p">,</span><span class="s">'country_name'</span><span class="p">,</span><span class="s">'httpversion'</span><span class="p">,</span><span class="s">'message'</span><span class="p">,</span><span class="s">'method'</span><span class="p">,</span>
            <span class="s">'referrer'</span><span class="p">,</span><span class="s">'remote_addr'</span><span class="p">,</span><span class="s">'remote_user'</span><span class="p">,</span><span class="s">'request'</span><span class="p">,</span><span class="s">'response_time'</span><span class="p">,</span><span class="s">'status'</span><span class="p">,</span><span class="s">'device'</span><span class="p">,</span>
            <span class="s">'browser'</span><span class="p">,</span><span class="s">'os_name'</span><span class="p">,</span><span class="s">'os_version'</span><span class="p">,</span><span class="s">'city_name'</span><span class="p">,</span><span class="s">'region_name'</span><span class="p">]</span>
        <span class="p">]</span>

        <span class="n">df_yesterday</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span>
            <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'timestamp'</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">yesterday</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d'</span><span class="p">)</span><span class="o">+</span><span class="s">'T0:0:0'</span><span class="p">)</span> <span class="o">&amp;</span> \
            <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'timestamp'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">yesterday</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d'</span><span class="p">)</span><span class="o">+</span><span class="s">'T23:59:59'</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">df_today</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span>
            <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'timestamp'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">yesterday</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d'</span><span class="p">)</span><span class="o">+</span><span class="s">'T23:59:59'</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">df_yesterday</span><span class="p">.</span><span class="n">to_parquet</span><span class="p">(</span><span class="s">'yesterday.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">df_today</span><span class="p">.</span><span class="n">to_parquet</span><span class="p">(</span><span class="s">'today.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="o">@</span><span class="n">task</span>
    <span class="k">def</span> <span class="nf">store_to_postgres</span><span class="p">():</span>
        <span class="n">engine</span> <span class="o">=</span> <span class="n">sqlalchemy</span><span class="p">.</span><span class="n">create_engine</span><span class="p">(</span><span class="s">'postgresql://root:root@postgres/mart'</span><span class="p">)</span>
        <span class="n">df_yesterday</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s">'yesterday.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">)</span>
        <span class="n">df_today</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s">'today.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">)</span>

        <span class="n">df_yesterday</span><span class="p">.</span><span class="n">to_sql</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s">'mart_</span><span class="si">{</span><span class="n">yesterday</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d"</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">,</span>
            <span class="n">con</span><span class="o">=</span><span class="n">engine</span><span class="p">,</span> 
            <span class="n">if_exists</span><span class="o">=</span><span class="s">'append'</span><span class="p">,</span> 
            <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        
        <span class="n">df_today</span><span class="p">.</span><span class="n">to_sql</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s">'mart_</span><span class="si">{</span><span class="p">(</span><span class="n">yesterday</span> <span class="o">+</span> <span class="n">datetime</span><span class="p">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">)).</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d"</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">,</span>
            <span class="n">con</span><span class="o">=</span><span class="n">engine</span><span class="p">,</span>
            <span class="n">if_exists</span><span class="o">=</span><span class="s">'append'</span><span class="p">,</span>
            <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">engine</span><span class="p">.</span><span class="n">dispose</span><span class="p">()</span>

    <span class="n">get_logs_from_hdfs</span><span class="p">()</span> <span class="o">&gt;&gt;</span> <span class="n">transform</span><span class="p">()</span> <span class="o">&gt;&gt;</span> <span class="n">store_to_postgres</span><span class="p">()</span>
    
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">batch_to_rdb</span><span class="p">()</span>
</code></pre></div></div>
<p>ë°ì´í„° ì²˜ë¦¬ëŠ” Pandasë¥¼ ì´ìš©í–ˆìŠµë‹ˆë‹¤. Airflow ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë°ì´í„° ì²˜ë¦¬ë¥¼ í•˜ë ¤ê³  í–ˆì§€ë§Œ Spark DataFrameì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ SparkSessionì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ë°(ì œê°€ ì˜ ëª°ë¼ì„œ ê·¸ëŸ°ê²ƒì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.) ì—ëŸ¬ ë•Œë¬¸ì— ì‚¬ìš©í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. HDFSì—ì„œ parquet íŒŒì¼ì„ ê°€ì ¸ì™€ pyarrowë¥¼ ì´ìš©í•˜ì—¬ pandas DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">transform()</code> ê³¼ì •ì—ì„œ df_yesterdayì™€ df_today ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì½”ë“œê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ê²ƒì€ logstashê°€ UTCë¡œ ë™ì‘í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê¸°ë¡ë˜ëŠ” ë‚ ì´ UTC ê¸°ì¤€ì´ë¼ì„œ í•œêµ­ ì‹œê°„ê³¼ 9ì‹œê°„ ì°¨ì´ê°€ ë‚˜ê¸° ë•Œë¬¸ì— Redisì— ê°™ì€ í‚¤ì— ë‹¤ë¥¸ ë‚ ì§œì˜ ë¡œê·¸ê°€ ë“¤ì–´ì˜¤ê²Œ ë©ë‹ˆë‹¤.</p>

<p>ì˜ˆë¥¼ë“¤ë©´, UTC   2022-11-30ì˜ ë°ì´í„°ëŠ” KSTê¸°ì¤€ 2022-11-30 09:00:00ë¶€í„° 2022-12-01 08:59:59ê¹Œì§€ì´ë¯€ë¡œ Redisì—ëŠ” <code class="language-plaintext highlighter-rouge">20221130</code>í‚¤ë¡œ ì ‘ê·¼í–ˆì„ ë•Œ 2022-12-01 ë°ì´í„°ê°€ ë“¤ì–´ì™€ ìˆê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ë‚˜ëˆ  DBì— ì ì¬í•˜ëŠ” ì½”ë“œê°€ í•„ìš”í–ˆìŠµë‹ˆë‹¤. DBì— ë‚˜ëˆ  ì ì¬í•˜ì§€ ì•Šìœ¼ë©´ ë°°ì¹˜ ì²˜ë¦¬í• ë•Œ ê·¸ ì´ì „ ë‚ ì§œë“¤ì˜ í…Œì´ë¸”ê¹Œì§€ ëª¨ë‘ ì¡°íšŒí•´ì•¼í•  ê°€ëŠ¥ì„±ì´ ìˆê¸° ë•Œë¬¸ì— ì´ë¥¼ ë°©ì§€í•˜ëŠ” ì´ìœ ë˜í•œ ìˆìŠµë‹ˆë‹¤.</p>

<p>airflowì— SparkSessionìœ¼ë¡œ ì„¸ì…˜ì„ ìƒì„±í•˜ë©´ 30ì´ˆì˜ timeoutìœ¼ë¡œ DAGê°€ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.(ì •í™•í•œ ì´ìœ ê°€ ë§ëŠ”ì§€ëŠ” ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.) ê·¸ë˜ì„œ SparkContextë¥¼ í†µí•´ hdfsì— ì ‘ê·¼í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ë°°ì¹˜ íŒŒì¼ì„ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.</p>

<h3 id="postgresql-jupyter-notebook">PostgreSQL, Jupyter Notebook</h3>
<p>RDBë¡œ PostgreSQLì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. PostgreSQLì„ MySQLë³´ë‹¤ ìì£¼ ì‚¬ìš©í•´ì„œ ìµìˆ™í•˜ê¸° ë•Œë¬¸ì— ì„ íƒí–ˆìŠµë‹ˆë‹¤. BIë¡œ Jupyter Notebookì„ ì‚¬ìš©í–ˆëŠ”ë° ë˜‘ê°™ì´ Apache Zeplinë³´ë‹¤ ìµìˆ™í•˜ê¸° ë•Œë¬¸ì— ì„ íƒí–ˆìŠµë‹ˆë‹¤.</p>

<p>Jupyter Notebookì€ ë¡œê·¸ì¸ ì‹œ íŒ¨ìŠ¤ì›Œë“œë¥¼ ë¬¼ì–´ë³´ì§€ ì•Šê²Œ í•˜ë©´ í† í°ì„ ì…ë ¥í•´ì•¼ í•˜ëŠ”ë° Notebook ì ‘ì† ì‹œ Jupyter Notebook ë„ì»¤ì˜ ë¡œê·¸ì— ìˆëŠ” URLë¡œ ì ‘ì†í•´ì•¼ í•˜ëŠ” ë¶ˆí¸í•¨ì´ ìˆìŠµë‹ˆë‹¤.</p>
:ET