I"߃<h2 id="rag">RAG</h2>
<p>RAG (Retrieval Augmented Generation)란, LLM이 학습한 데이터로부터 답변하지 않고 외부 문서(Context)를 참조할 수 있도록 하는 시스템을 말합니다. RAG는 LLM이 자신이 학습한 데이터가 아닌 외부의 최신 정보를 참조하여 답변의 부정확성이나 환각(Hallucination)을 줄일 수 있습니다.</p>

<h2 id="retriever">Retriever</h2>
<p>Retriever는 유저의 질문에 관련이 높은 문서를 가져오는 역할을 담당하는 구성요소입니다. 관련 문서를 가져오는 전략으로는 BM25, Bi-Encoder 등의 방식이 있습니다.</p>

<h3 id="lexical-search-bm25">Lexical Search: BM25</h3>
<p>BM25는 키워드 매칭을 통해 점수를 내는 방식을 말합니다. TF-IDF를 활용해 질의와 문서간의 유사도를 계산하여 빠르고 간단한 질문에 적합합니다. 하지만, 질의의 문맥을 이용하지 못해 관련된 문서가 검색되는 경우도 있습니다.</p>

<h3 id="semantic-search-bi-encoder">Semantic Search: Bi-Encoder</h3>
<p>Bi-Encoder는 질의와 문서간의 임베딩 벡터 유사도를 계산하여 점수를 내는 방식입니다. BERT와 같은 언어모델을 이용해 문맥적 의미가 담긴 임베딩 벡터를 생성하여 질의와 문서의 의미적인 유사성을 잘 찾아낼 수 있습니다. 미리 벡터로 변환하여 저장할 수 있어 검색 시 유사도만 계산하면 되므로 크게 느리지 않습니다. 하지만, 질의나 문서가 길어질수록 벡터로 표현할 때 정보가 손실될 수 있고 Query와 Candidates간의 관계성을 고려하지 않기 때문에 Cross-Encoder 대비 정확도가 떨어집니다.</p>

<h3 id="hybrid-search-lexical-search--semantic-search">Hybrid Search: Lexical Search + Semantic Search</h3>
<p>Hybrid Search는 유저 질의의 키워드를 중점으로 찾는 BM25와 유저 질의와 의미적으로 유사한 문서를 찾는 Bi-Encoder를 같이 사용하는 방법을 말합니다. Hybrid Search의 검색 결과는 Lexcial Search 혹은 Semantic Search를 단독으로 사용한 결과보다 정확한 문서를 검색할 수 있습니다. 검색 결과에 가중치를 부여하면 도메인 특성에 따라 최적화할 수 있습니다. 보통 자주 사용하는 계산방식으로는 CC(Convex Combination)과 RRF(Reciprocal Rank Fusion)이 있습니다.</p>

<p><strong>Convex Combination</strong></p>
<ul>
  <li>$score(doc) = \alpha * score_{lex}(doc) + \beta * score_{sem}(doc)$</li>
</ul>

<p>CC는 alpha와 beta을 각각 스코어에 곱해 최종 스코어를 계산하는 방식입니다. 두 스코어 값을 합치기 위해 각 스코어마다 Normalization 과정이 필요합니다.</p>

<p><strong>Reciprocal Rank Fusion</strong></p>
<ul>
  <li>$score(doc) = \frac{1}{k + rank_{lex}(doc)} + \frac{1}{k + rank_{sem}(doc)}$</li>
</ul>

<p>RRF는 가중치가 아닌 문서의 순위를 이용하는 방식입니다. 가중치를 이용하지 않기 때문에 Normalization 과정이 필요하지 않고 순위가 낮은 문서를 위해 조정하는 임의의 값입니다.</p>

<h3 id="cross-encoder">Cross-Encoder</h3>
<p>Cross-Encoder는 하나의 언어모델에 Query와 Candidate를 같이 넣어 유사도를 계산하는 방식입니다. Bi-Encoder 대비 예측 성능이 괜찮지만 Cross-Encoder를 Retreiver로 사용한다면 매 질의마다 모든 문서와의 유사도를 계산해야 하기 때문에 단독으로 사용하기 어렵습니다.</p>

<h2 id="reranking-전략">Reranking 전략</h2>
<p>Reranking 전략은 적절한 문서 후보군을 선별하고 Cross-Encoder로 순위를 재조정하는 전략을 말합니다. 이렇게 구성하면, Cross-Encoder만 사용했을 때보다 계산 비용이 줄고 정확도를 확보할 수 있는 방법입니다.</p>

<figure>
    <img src="https://framerusercontent.com/images/4atMCrE67i6XDQdSySevR8sVhM.png" />
    <figcaption>https://dify.ai/blog/hybrid-search-rerank-rag-improvement</figcaption>
</figure>

<p>그렇다면 RAG 시스템에서 Reranking 전략이 왜 필요한지 생각해볼 수 있는데 LLM의 답변 정확도가 Context의 순서와 관계가 있기 때문입니다.
<a href="https://arxiv.org/abs/2307.03172">Lost in the Middle: How Language Models Use Long Contexts</a> 논문에는 정답과 관련된 문서가 가운데 위치할 수록 답변 성능이 떨어지는 것을 확인할 수 있습니다. 따라서, 이러한 이유 때문에 RAG 시스템에서 Hybrid Search의 문서 순위를 재조정하는 Reranking 전략이 필요함을 알 수 있습니다.</p>

<h2 id="rag-구현">RAG 구현</h2>
<h3 id="pipeline">Pipeline</h3>
<figure style="text-align:center;">
    <a href="https://1drv.ms/i/c/502fd124b305ba80/IQTHCBiTNjXJSrDfn8ZtMHRJAXPh5Xy2ex78zUYezRLVn8s?width=1024" onclick="return false;" data-lightbox="gallery">
        <img src="https://1drv.ms/i/c/502fd124b305ba80/IQTHCBiTNjXJSrDfn8ZtMHRJAXPh5Xy2ex78zUYezRLVn8s?width=1024" alt="01" style="max-width: 80%; height: auto;" />
    </a>
</figure>

<p>RAG 파이프라인 순서는 다음과 같습니다.</p>
<ol>
  <li>유저가 질문을 보내면 <code class="language-plaintext highlighter-rouge">text-embedding-004</code> 모델로부터 임베딩 벡터를 받습니다.</li>
  <li>유저 쿼리와 임베딩 벡터를 이용해 Vector Store인 <code class="language-plaintext highlighter-rouge">Opensearch</code>에 Hybrid Search로 후보 문서들을 가져옵니다.</li>
  <li>후보 문서들을 Opensearch ML 기능을 이용해 Reranking합니다.</li>
  <li>최종 Context와 유저 질의를 LLM(<code class="language-plaintext highlighter-rouge">Gemini-2.0-flash</code>)에 전달해 답변을 받아옵니다.</li>
  <li>LLM의 답변을 유저에게 전달합니다.</li>
</ol>

<h3 id="opensearch">Opensearch</h3>
<p>Vector Store로 Opensearch를 선택했는데 그 이유는 lexical search와 semantic search 모두 지원하기 때문입니다. Semantic search는 knn(K-Nearest Neighbor) 알고리즘을 지원하고 있어 빠른 검색 결과를 받아볼 수 있습니다. 또한, Opensearch 클러스터에 ML모델을 배포하고 REST API로 사용할 수 있는 기능이 있어 이를 이용해 Reranking 전략을 사용할 수 있었습니다.</p>

<h3 id="gemini">Gemini</h3>
<p>LLM은 Google의 Gemini 모델을 선택했습니다. 그 이유는 1M의 토큰 컨텍스트를 지원하고 있어 프롬프트에 충분히 많은 Context들을 넣을 수 있다는 장점이 있기 때문입니다.</p>

<blockquote>
  <p>Gemini 2.0 Flash의 무료 등급은 시간당 1백만개의 토큰 제한이 걸려있습니다. 제품 개선에 사용 내역이 사용되기는 하지만 토이 프로젝트로 사용하기에 충분하다고 생각합니다.</p>
</blockquote>

<h3 id="demo-app">Demo APP</h3>
<p>많이 사용하는 프레임워크인 Streamlit을 사용해서 앞단을 구성했습니다.</p>
<figure>
    <a href="https://1drv.ms/i/c/502fd124b305ba80/IQRhkcrRacG5QoIRDWhpuPswAUx8mqDtVT69w05eslUP6sI?width=2828&amp;height=1342" onclick="return false;" data-lightbox="gallery">
        <img src="https://1drv.ms/i/c/502fd124b305ba80/IQRhkcrRacG5QoIRDWhpuPswAUx8mqDtVT69w05eslUP6sI?width=2828&amp;height=1342" alt="02" style="max-width: 100%; height: auto'" />
    </a>
    <figcaption>Frontend</figcaption>
</figure>

<p>사용자가 PDF파일을 업로드하면 파싱 후 Vector Store에 문서를 적재하고 아래 조건에 따라 유저 질의에 따라 문서를 가져올 수 있도록 구성했습니다.</p>

<h3 id="pdf-parse">PDF Parse</h3>
<p>PDF 문서 내에서 정보를 추출하는 라이브러리 중 하나인 Unstructured를 이용했습니다. 클라우드 서비스를 제공하고 있고 Langchain과 연동이 되어 있어 쉽게 사용가능했습니다. 하지만, PDF의 표를 추출하는 과정에서 문제가 발생했습니다.</p>

<p><strong>Llamaindex</strong><br />
PDF 문서를 Unstructured 라이브러리로 바로 파싱하는 경우, 테이블 형식이 제대도 파싱되지 않는 문제가 발생했습니다. 이를 해결하기 위해 중간 변환 과정이 필요했고 마크다운과 이미지 형식 두 가지를 고려했습니다. 이미지 변환 방식도 LLM이 이미지도 잘 해석해주기 때문에 가능한 방법이지만 그 만큼 비용이 들어가기 때문에 마크다운 변환을 선택하게 되었습니다. 마크다운 변환을 이용할 때는 <a href="https://www.llamaindex.ai/">Llamaindex</a>라는 상용 서비스를 이용했고 괜찮은 결과물을 반환해주었습니다.</p>

<blockquote>
  <p>Llamaindex의 Free Plan은 하루에 1000페이지 변환이 가능합니다. 이 정도면 토이 프로젝트로 사용할 때 충분하다고 생각됩니다.</p>
</blockquote>

<blockquote>
  <p>Llamaindex 말고도 Upstage에서도 <a href="https://www.upstage.ai/blog/en/let-llms-read-your-documents-with-speed-and-accuracy">Document Parse</a> 서비스를 제공중에 있습니다.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">llama_cloud_services</span> <span class="kn">import</span> <span class="n">LlamaParse</span>

<span class="n">llamaparse</span> <span class="o">=</span> <span class="n">LlamaParse</span><span class="p">(</span>
        <span class="n">result_type</span><span class="o">=</span><span class="s">"markdown"</span><span class="p">,</span>
        <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'LLAMA_CLOUD_API_KEY'</span><span class="p">)</span>
    <span class="p">).</span><span class="n">load_data</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Unstructured.io</strong><br />
Llamaindex로 변화된 마크다운들로부터 정보를 추출해야 합니다. Langchain에서 제공하는 UnstructuredMarkdownLoader를 이용해 문서를 파싱하고 Vector Store에 저장할 준비를 합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">UnstructuredMarkdownLoader</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">UnstructuredMarkdownLoader</span><span class="p">(</span>
        <span class="n">file_path</span><span class="o">=</span><span class="n">markdown_path</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s">"elements"</span><span class="p">,</span>
        <span class="n">chunking_strategy</span><span class="o">=</span><span class="s">"by_title"</span><span class="p">,</span>
        <span class="n">strategy</span><span class="o">=</span><span class="s">"hi_res"</span><span class="p">,</span>
        <span class="n">max_characters</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
        <span class="n">new_after_n_chars</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span>
        <span class="n">combine_text_under_n_chars</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="n">load</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>OpenSearchVectorSearch</strong><br />
Langchain에서 Opensearch를 벡터 스토어로 사용할 수 있도록 구현체를 제공합니다. 이 클래스를 이용하면 docs를 쉽게 Opensearch에 적재할 수 있습니다. OpenSearchVectorSearch의 embedding_function에는 embedding 모델을 넣어 Semantic search를 위해 문서의 임베딩 벡터를 추가합니다.</p>

<p>단, Opensearch에 문서들을 적재하기 전에 반드시 인덱스를 생성해야 합니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">langchain_google_genai</span> <span class="kn">import</span> <span class="n">GoogleGenerativeAIEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">OpenSearchVectorSearch</span>

<span class="n">llm_emb</span> <span class="o">=</span> <span class="n">GoogleGenerativeAIEmbeddings</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s">'models/text-embedding-004'</span><span class="p">,</span>
        <span class="n">google_api_key</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'GOOGLE_API_KEY'</span><span class="p">)</span>
    <span class="p">)</span>

<span class="n">vector_db</span> <span class="o">=</span> <span class="n">OpenSearchVectorSearch</span><span class="p">(</span>
        <span class="n">index_name</span><span class="o">=</span><span class="n">index_name</span><span class="p">,</span>
        <span class="n">opensearch_url</span><span class="o">=</span><span class="n">opensearch_url</span><span class="p">,</span>
        <span class="n">embedding_function</span><span class="o">=</span><span class="n">llm_emb</span><span class="p">,</span>
        <span class="n">http_auth</span><span class="o">=</span><span class="p">(</span><span class="n">opensearch_id</span><span class="p">,</span> <span class="n">opensearch_password</span><span class="p">),</span>
        <span class="n">use_ssl</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">verify_certs</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">is_aoss</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">engine</span><span class="o">=</span><span class="s">"faiss"</span><span class="p">,</span>
        <span class="n">space_type</span><span class="o">=</span><span class="s">"l2"</span><span class="p">,</span>
        <span class="n">bulk_size</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
        <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
        <span class="n">ssl_show_warn</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>

<span class="n">vector_db</span><span class="p">.</span><span class="n">add_documents</span><span class="p">(</span>
        <span class="n">documents</span><span class="o">=</span><span class="n">docs</span><span class="p">,</span>
        <span class="n">vector_field</span><span class="o">=</span><span class="s">'vector_field'</span><span class="p">,</span>
        <span class="n">bulk_size</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div>

<h3 id="retrievals">Retrievals</h3>
<p>Opensearch를 이용할 수 있는 Python 라이브러리인 opensearchpy를 사용할 수 있습니다. 라이브러리를 이용해 lexical search와 semantic search를 구현하고 이를 이용해 hybrid search를 구현합니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">opensearchpy</span> <span class="kn">import</span> <span class="n">OpenSearch</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenSearch</span><span class="p">(</span>
        <span class="n">hosts</span><span class="p">,</span>
        <span class="p">...</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">get_document</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">index_name</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="n">body</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">index_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_lexical_search</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">index_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"index_name"</span><span class="p">)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"query"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s">"match"</span><span class="p">:</span> <span class="p">{</span>
                <span class="s">"text"</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s">"query"</span><span class="p">:</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'query'</span><span class="p">)</span><span class="si">}</span><span class="s">"</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">},</span>
        <span class="s">"size"</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"k"</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">get_document</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">index_name</span><span class="o">=</span><span class="n">index_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>

<span class="k">def</span> <span class="nf">get_semantic_search</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">index_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"index_name"</span><span class="p">)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"query"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s">"knn"</span><span class="p">:</span> <span class="p">{</span>
                <span class="s">"vector_field"</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s">"vector"</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"vector"</span><span class="p">),</span>
                    <span class="s">"k"</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"k"</span><span class="p">)</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">get_document</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">index_name</span><span class="o">=</span><span class="n">index_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>
</code></pre></div></div>
<p>각 search 함수의 response에는 문서와 스코어가 튜플 형태로 저장되어 있습니다. 스코어를 normalization하는 함수와 CC 혹은 RRF를 적용하는 함수를 작성하여 최종 문서를 반환하도록 합니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">normalization</span><span class="p">(</span><span class="n">response</span><span class="p">):</span>
    <span class="n">hits</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s">'hits'</span><span class="p">][</span><span class="s">'hits'</span><span class="p">]</span>
    <span class="n">max_score</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s">'hits'</span><span class="p">][</span><span class="s">'max_score'</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">hit</span> <span class="ow">in</span> <span class="n">hits</span><span class="p">:</span>
        <span class="n">hit</span><span class="p">[</span><span class="s">'_score'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">hit</span><span class="p">[</span><span class="s">'_score'</span><span class="p">])</span> <span class="o">/</span> <span class="n">max_score</span>
    <span class="n">response</span><span class="p">[</span><span class="s">'hits'</span><span class="p">][</span><span class="s">'max_score'</span><span class="p">]</span> <span class="o">=</span> <span class="n">hits</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s">'_score'</span><span class="p">]</span> <span class="c1"># 1.0
</span>    <span class="n">response</span><span class="p">[</span><span class="s">'hits'</span><span class="p">][</span><span class="s">'hits'</span><span class="p">]</span> <span class="o">=</span> <span class="n">hits</span>
    <span class="k">return</span> <span class="n">response</span>

<span class="k">def</span> <span class="nf">get_ensemble_results</span><span class="p">(</span><span class="n">doc_lists</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]],</span> <span class="n">weights</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">60</span><span class="p">):</span>
    <span class="c1"># RRF
</span>    <span class="n">documents</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span> <span class="c1"># unique docs
</span>    <span class="n">content_to_document</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">doc_list</span> <span class="ow">in</span> <span class="n">doc_lists</span><span class="p">:</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="n">doc_list</span><span class="p">:</span>
            <span class="n">documents</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="p">)</span>
            <span class="n">content_to_document</span><span class="p">[</span><span class="n">content</span><span class="p">]</span> <span class="o">=</span> <span class="n">doc</span>
    
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span><span class="n">content</span><span class="p">:</span> <span class="p">.</span><span class="mi">0</span> <span class="k">for</span> <span class="n">content</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">}</span>
    
    <span class="k">for</span> <span class="n">doc_list</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">doc_lists</span><span class="p">,</span><span class="n">weights</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">doc_list</span><span class="p">,</span><span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">content</span> <span class="o">=</span> <span class="n">doc</span><span class="p">.</span><span class="n">page_content</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="n">c</span><span class="p">))</span>
            <span class="n">results</span><span class="p">[</span><span class="n">content</span><span class="p">]</span> <span class="o">+=</span> <span class="n">score</span>

    <span class="n">sorted_results</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">results</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">sorted_docs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">content_to_document</span><span class="p">[</span><span class="n">content</span><span class="p">],</span> <span class="n">score</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span> <span class="ow">in</span> <span class="n">sorted_results</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">sorted_docs</span>
</code></pre></div></div>

<p>Rerank를 수행할 때는 opensearchpy의 REST API를 따로 호출하도록 구현했습니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">rerank_documents</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">retrievals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Document</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]):</span>
    <span class="n">model_id</span> <span class="o">=</span> <span class="s">""</span> <span class="c1"># Opensearch Rerank Model id
</span>    <span class="n">text_docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">retrievals</span><span class="p">]</span>

    <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"query_text"</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span>
        <span class="s">"text_docs"</span><span class="p">:</span> <span class="n">text_docs</span>
    <span class="p">}</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">transport</span><span class="p">.</span><span class="n">perform_request</span><span class="p">(</span>
        <span class="n">method</span><span class="o">=</span><span class="s">"POST"</span><span class="p">,</span>
        <span class="n">url</span><span class="o">=</span><span class="sa">f</span><span class="s">"/_plugins/_ml/models/</span><span class="si">{</span><span class="n">model_id</span><span class="si">}</span><span class="s">/_predict"</span><span class="p">,</span>
        <span class="n">body</span><span class="o">=</span><span class="n">payload</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s">'inference_results'</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">response</span>
</code></pre></div></div>

<h3 id="question-answering">Question Answering</h3>
<p>QA 단계에서는 Retriever가 가져온 문서와 유저 질문을 묶어 프롬프트를 구성하여 답변을 받아오면 됩니다. ChatGPT처럼 한글자씩 나타나도록 하고 싶다면 아래 코드처럼 stream을 사용하여 geneator 함수로 구성하면 됩니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain.schema</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">HumanMessagePromptTemplate</span><span class="p">,</span> <span class="n">SystemMessagePromptTemplate</span><span class="p">,</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain_google_genai</span> <span class="kn">import</span> <span class="n">GoogleGenerativeAI</span>

<span class="k">def</span> <span class="nf">get_answer</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">instruction</span> <span class="o">=</span> <span class="s">''</span> <span class="c1"># system prompt
</span>    <span class="n">human_prompt</span> <span class="o">=</span> <span class="s">''</span> <span class="c1"># user prompt
</span>
    <span class="n">system_template</span> <span class="o">=</span> <span class="n">SystemMessagePromptTemplate</span><span class="p">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">instruction</span><span class="p">)</span>
    <span class="n">human_template</span> <span class="o">=</span> <span class="n">HumanMessagePromptTemplate</span><span class="p">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">human_prompt</span><span class="p">)</span>

    <span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="n">from_messages</span><span class="p">([</span><span class="n">system_template</span><span class="p">,</span> <span class="n">human_template</span><span class="p">])</span>

    <span class="n">invoke_args</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># user prompt의 f-string 요소가 있다면 해당 위치에 값을 넣을 수 있습니다
</span>
    <span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="n">stream</span><span class="p">(</span><span class="n">invoke_args</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">chunk</span>
</code></pre></div></div>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://aws.amazon.com/ko/blogs/tech/korean-reranker-rag/">한국어 Reranker를 활용한 검색 증강 생성(RAG) 성능 올리기</a></li>
  <li><a href="https://www.elastic.co/search-labs/blog/hybrid-search-elasticsearch">hyrbrid-search-elasticsearch</a></li>
  <li><a href="https://medium.com/@nuatmochoi/%ED%95%98%EC%9D%B4%EB%B8%8C%EB%A6%AC%EB%93%9C-%EA%B2%80%EC%83%89-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0-feat-ensembleretriever-knowledge-bases-for-amazon-bedrock-d6ef1a0daaf1">하이브리드 검색 구현하기 (feat. EnsembleRetriever, Knowledge Bases for Amazon Bedrock)</a></li>
  <li><a href="https://heygeronimo.tistory.com/101">[논문이해] Lost in the Middle: How Language Models Use Long Contexts</a></li>
</ul>
:ET