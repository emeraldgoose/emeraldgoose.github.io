I"Q<h1 id="objective">Objective</h1>
<p>앞에서 구현한 LayerNorm, MultiHeadAttention, GELU를 사용하고 이전에 구현해둔 Linear, Dropout, Softmax 클래스를 사용하여 Transformer 클래스를 구현하여 테스트해봅니다.</p>
<blockquote>
  <p><a href="https://emeraldgoose.github.io/pytorch/transformer-scratch-implementation-1/">Python으로 Transformer 바닥부터 구현하기[1] (MultiHead-Attention, LayerNorm, GELU)</a>
가장 바깥쪽에 위치한 Transformer부터 시작해서 EncoderLayer, DecoderLayer 순으로 설명하고자 합니다.</p>
</blockquote>
:ET