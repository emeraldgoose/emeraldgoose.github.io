I"†<blockquote>
  <p>ì½”ë”©í…ŒìŠ¤íŠ¸ë¡œ Pythonìœ¼ë¡œë§Œ MLPë¥¼ êµ¬í˜„í•˜ëŠ” ë¬¸ì œê°€ ë‚˜ì™”ë˜ ì ì´ ìˆìŠµë‹ˆë‹¤. ë‹¹ì‹œì— ì—­ì „íŒŒ êµ¬í˜„ì„ í•˜ì§€ ëª»í•´ ì½”ë”©í…ŒìŠ¤íŠ¸ì—ì„œ ë–¨ì–´ì¡Œì—ˆê³  ì™„ì „íˆ ë°”ë‹¥ì—ì„œë¶€í„° êµ¬í˜„í•´ë³´ê³ ì ì‹œì‘í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.</p>
</blockquote>

<h2 id="multi-layer-perceptron">Multi-Layer Perceptron</h2>
<p>Multi-Layer Perceptron(MLP)ì€ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì¸µ(layer)ë“¤ì´ ìŒ“ì—¬ ì‹ ê²½ë§ì„ ì´ë£¨ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. êµ¬í˜„ì´ ê°„ë‹¨í•˜ê¸° ë•Œë¬¸ì— ë”¥ëŸ¬ë‹ì„ ë°”ë‹¥ë¶€í„° êµ¬í˜„í•˜ëŠ” í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•˜ëŠ”ë° ì¢‹ì€ ëª¨ë¸ì…ë‹ˆë‹¤.
ì €ëŠ” MNISTë¥¼ ë°ì´í„°ì…‹ìœ¼ë¡œ í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê³  classification taskë¥¼ ìˆ˜í–‰í•´ë³¼ ê²ƒì…ë‹ˆë‹¤.</p>

<p>êµ¬í˜„í•˜ë ¤ê³  í•˜ëŠ” ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model(
  (net1): Sequential(
    (0): Linear(in_features=784, out_features=28, bias=True)
    (1): Sigmoid()
  )
  (net2): Sequential(
    (0): Linear(in_features=28, out_features=28, bias=True)
    (1): Sigmoid()
  )
  (fc): Linear(in_features=28, out_features=10, bias=True)
)
</code></pre></div></div>

<h2 id="forward-propagation">Forward Propagation</h2>
<p>ë¨¼ì € ìœ„ì˜ ëª¨ë¸ì˜ ìˆœì „íŒŒë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.</p>

<p>$y_1 = \sigma(z_1) = \sigma(w_1x + b_1), \sigma = \text{sigmoid}$</p>

<p>$y_2 = \sigma(z_2) = \sigma(w_2y_1 + b_2)$</p>

<p>$\hat{y} = w_3y_2 + b_3$</p>

<p>$L_{\text{MSE}} = \sum(\hat{y}-{y})^2$</p>

<h2 id="backward-propagation">Backward Propagation</h2>
<p>ë¨¼ì €, $w_3$ì— ëŒ€í•´ í¸ë¯¸ë¶„ëœ ê°’ì€ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>$\frac{\partial L}{ \partial w_3}=\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{dw_3}$</p>

<p>ë‹¤ìŒ, $w_2$ì— ëŒ€í•´ í¸ë¯¸ë¶„ëœ ê°’ì„ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ë©ë‹ˆë‹¤.</p>

<p>$\frac{\partial L}{ \partial w_2}=\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{dy_2} \cdot \frac{\partial y_2}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_2}$</p>

<p>${\partial y_2 \over \partial z_2} = \sigma_2(y_2)(1 - \sigma_2(y_2)) = y_2(1 - y_2)$</p>

<p>$\frac{\partial L}{\partial w_2} = \frac{2}{m}(\hat{y}-y) \cdot y_2(1 - y_2) \cdot y_1, \ \text{m = batch size}$</p>

<p>$w_1$ì— ëŒ€í•´ í¸ë¯¸ë¶„ëœ ê°’ì„ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ë©ë‹ˆë‹¤.</p>

<p>$\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z_2} \cdot \frac{\partial z_2}{\partial y_1} \cdot \frac{\partial y_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_1}$</p>

<p>${\partial y_1 \over \partial z_1} = \sigma_1(z_1)(1 - \sigma_1(z_1)) = y_1(1 - y_1)$</p>

<p>$\frac{\partial L}{\partial w_1}= \frac{2}{m}(\hat{y}-y) \cdot \sigma_2(z_2)(\delta_{ij} - \sigma_2(z_2)) \cdot w_2 \cdot y_1 \cdot (1 - y_1) \cdot x$</p>

<p>gradientì˜ ê³„ì‚°ì—ì„œ ë§ˆì§€ë§‰ ê³±ì—ëŠ” <strong>ì…ë ¥ê°’</strong>ì— ëŒ€í•´ dot productí•˜ê³  ì…ë ¥ ë ˆì´ì–´ ë°©í–¥ìœ¼ë¡œ <strong>ì´ì „ ë ˆì´ì–´ì˜ weight</strong>ë¥¼ dot productí•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ Linear ë ˆì´ì–´ëŠ” ì…ë ¥ê°’ì„ ì €ì¥í•´ì•¼ backward ê³„ì‚°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
<script src="https://gist.github.com/emeraldgoose/5bbdab6c658bc73da63bbc694bcf5f2a.js"></script></p>

<p>ë˜í•œ, sigmoidë¥¼ í†µê³¼í•œ ì¶œë ¥ê°’ë“¤ì€ ì—­ì „íŒŒë•Œ <strong>element-wise product</strong>ë¥¼ ì§„í–‰í•´ì•¼ í•©ë‹ˆë‹¤. í™œì„±í™”í•¨ìˆ˜ëŠ” ì…ë ¥ê°’ ê°ê°ì— ëŒ€í•´ í•¨ìˆ˜ë¥¼ í†µê³¼ì‹œí‚¤ë¯€ë¡œ ì—­ì „íŒŒë•Œë„ ë˜‘ê°™ì´ ì§„í–‰ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. softmaxëŠ” element-wise independentí•˜ì§€ ì•Šì•„ element-wise productë¥¼ ìˆ˜í–‰í•´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.</p>

<p>ì´ ì—­ì „íŒŒë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ ë ˆì´ì–´ë§ˆë‹¤ backward()í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ì—¬ gradientë¥¼ ê³„ì‚°í•˜ê³  Optimizerë¥¼ ì´ìš©í•˜ì—¬ weightì™€ biasë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br />
<script src="https://gist.github.com/emeraldgoose/f256205e7bed257c9b1c5ecbcfc409e5.js"></script></p>

<h2 id="ê²°ê³¼">ê²°ê³¼</h2>
<p>MNIST 5000ì¥ì„ í›ˆë ¨ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê³  1000ì¥ì„ í…ŒìŠ¤íŠ¸ë°ì´í„°ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.</p>

<p><img src="https://lh3.google.com/u/0/d/1k18xXPI4qMx31qgSTajBwQ6NwjycTpkr" alt="" width="400" />
<img src="https://lh3.google.com/u/0/d/1Pzta5dtXVxduFsIgHGaSqsHOtKsh6jSh" alt="" width="400" /></p>

<p>10 ì—í¬í¬ì—ë„ lossê°€ ì˜ ë–¨ì–´ì§€ê³  Accuracyë„ ì˜ ì¦ê°€í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<h2 id="ì½”ë“œ">ì½”ë“œ</h2>
<p><a href="https://github.com/emeraldgoose/hcrot">https://github.com/emeraldgoose/hcrot</a></p>

<h2 id="reference">Reference</h2>
<ul>
  <li><a href="http://taewan.kim/post/sigmoid_diff/">http://taewan.kim/post/sigmoid_diff/</a></li>
  <li><a href="https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/">https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/</a></li>
  <li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">https://pytorch.org/docs/stable/generated/torch.nn.Linear.html</a></li>
  <li><a href="https://velog.io/@gjtang/Softmax-with-Loss-%EA%B3%84%EC%B8%B5-%EA%B3%84%EC%82%B0%EA%B7%B8%EB%9E%98%ED%94%84">https://velog.io/@gjtang/Softmax-with-Loss-%EA%B3%84%EC%B8%B5-%EA%B3%84%EC%82%B0%EA%B7%B8%EB%9E%98%ED%94%84</a></li>
  <li><a href="https://aew61.github.io/blog/artificial_neural_networks/1_background/1.b_activation_functions_and_derivatives.html">https://aew61.github.io/blog/artificial_neural_networks/1_background/1.b_activation_functions_and_derivatives.html</a></li>
</ul>
:ET