I"¯,<h2 id="pruning">Pruning</h2>

<ul>
  <li>Pruningì€ ì¤‘ìš”ë„ê°€ ë‚®ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì œê±°í•˜ëŠ” ê²ƒ</li>
  <li>ì–´ë–¤ ë‹¨ìœ„ë¡œ Pruning? â†’ Structured(group) / Unstructured(fine grained)</li>
  <li>ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ Pruning? â†’ ì¤‘ìš”ë„ ì •í•˜ê¸°(Magnitude(L2, L1), BN scaling facgtor, Energy-based, Feature map, â€¦)</li>
  <li>ê¸°ì¤€ì€ ì–´ë–»ê²Œ ì ìš©? â†’ Network ì „ì²´ë¡œ ì¤„ ì„¸ì›Œì„œ(global), Layer ë§ˆë‹¤ ë™ì¼ ë¹„ìœ¨ë¡œ ê¸°ì¤€(local)
    <ul>
      <li>global : ì „ì²´ n%, ì–´ë–¤ layerëŠ” ë§ì´, ì–´ë–¤ layerëŠ” ì ê²Œ</li>
      <li>local : ëª¨ë“  layerë¥¼ ê· ì¼í•˜ê²Œ n%</li>
    </ul>
  </li>
  <li>ì–´ë–¤ phaseì—? â†’ í•™ìŠµëœ ëª¨ë¸ì—(trained model) / initialize ì‹œì ì—(pruning at initialization)</li>
</ul>

<h3 id="structured-pruning">Structured Pruning</h3>

<ul>
  <li>íŒŒë¼ë¯¸í„°ë¥¼ ê·¸ë£¹ ë‹¨ìœ„ë¡œ pruning(ê·¸ë£¹ ë‹¨ìœ„ëŠ” channel/filter/layer level ë“±ì´ ê°€ëŠ¥)</li>
  <li>Masked (0ìœ¼ë¡œ pruningëœ) filter ì œê±° ì‹œ ì‹¤ì§ˆì  ì—°ì‚° íšŸìˆ˜ ê°ì†Œë¡œ ì§ì ‘ì ì¸ ì†ë„ í–¥ìƒ</li>
  <li>BN
    <ul>
      <li>Learning Efficient Convolutional Networks through Network Slimming(ICCV 2017)</li>
      <li>Scaling factor $\gamma$
        <ul>
          <li>BNì˜ scaling factor $\gamma$ëŠ” Convì˜ Out Channelì— ê³±í•´ì§€ëŠ” í˜•íƒœ</li>
          <li>$\gamma$ê°€ í¬ë©´, í•´ë‹¹ Filterì˜ weightí¬ê¸°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ í´ ê²ƒ</li>
          <li>$\gamma$ì˜ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì€ p%ì˜ ì±„ë„ì„ pruning</li>
          <li>$\gamma$ì— L1-normì„ Regularizerë¡œ ì‚¬ìš©, Insignificant Channelsì€ ìì—°ìŠ¤ëŸ½ê²Œ Pruneë˜ë„ë¡ ìœ ë„</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Rank
    <ul>
      <li>Rankë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•œ ë…¼ë¬¸ : HRank: Filter Pruning using High-Rank Feature Map (CVPR 2020)</li>
      <li>Weightê°€ ì•„ë‹Œ ì‹¤ì œ Feature map outputì„ ë³´ì!</li>
      <li>Feature map outputì˜ ê° Filterì— SVDë¥¼ ì ìš©, Rankë¥¼ ê³„ì‚°</li>
      <li>ì´ë¯¸ì§€ì— ë”°ë¼ Feature map outputì€ ë‹¹ì—°íˆ ë‹¬ë¼ì§€ë¯€ë¡œ, ê·¸ë•Œë§ˆë‹¤ SVD rank ê°œìˆ˜ëŠ” ë‹¬ë¼ì§€ëŠ” ê²ƒ ì•„ë‹Œê°€?
        <ul>
          <li>ê° ë‹¤ë¥¸ Batch ì´ë¯¸ì§€ë“¤ë¡œ Feature map outputì„ ê³„ì‚°, Rankë¥¼ êµ¬í–ˆì„ ë•Œ, ì°¨ì´ê°€ ì—†ìŒì„ ì‹¤í—˜ì ìœ¼ë¡œ ë³´ì„</li>
        </ul>
      </li>
      <li>Rank ê³„ì‚°ì„ êµ¬í˜„í•  ë•ŒëŠ” <code class="language-plaintext highlighter-rouge">torch.matrix_rank()</code> í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤.</li>
    </ul>
  </li>
</ul>

<h3 id="unstructured-pruning">Unstructured Pruning</h3>

<ul>
  <li>íŒŒë¼ë¯¸í„° ê°ê°ì„ ë…ë¦½ì ìœ¼ë¡œ Pruning</li>
  <li>Pruningì„ ìˆ˜í–‰í•  ìˆ˜ë¡ ë„¤íŠ¸ì›Œí¬ ë‚´ë¶€ì˜ í–‰ë ¬ì´ ì ì  í¬ì†Œ(Sparse)í•´ì§</li>
  <li>Structured Pruningê³¼ ë‹¬ë¦¬ Sparse Computationì— ìµœì í™”ëœ ì†Œí”„íŠ¸ì›¨ì–´ ë˜ëŠ” í•˜ë“œì›¨ì–´ì— ì ì ˆí•œ ê¸°ë²•
    <ul>
      <li>Sparse computationì„ ì§€ì›í•˜ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ í˜¹ì€ í•˜ë“œì›¨ì–´ì—ì„œë§Œ ì†ë„ í–¥ìƒì´ ìˆê³  ê·¸ ì™¸ì—ëŠ” ì—†ìŒ</li>
      <li>ì™œë‚˜í•˜ë©´, ê° ë ˆì´ì–´ì˜ ì¼ë¶€ ê°€ì¤‘ì¹˜ë§Œ 0ì´ ë  ë¿ì´ê³  ì‹¤ì§ˆì ì€ ë ˆì´ì–´ ê°œìˆ˜ëŠ” ë³€í•¨ì´ ì—†ìŒ</li>
    </ul>
  </li>
  <li>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks(ICLR 2019)
    <ul>
      <li>Pruneëœ sparseí•œ ëª¨ë¸ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ì´ˆê¸°ì¹˜ë¡œë¶€í„° í•™ìŠµì´ ì–´ë ¤ì›€</li>
      <li>Lottery Ticket Hypothesis
        <ul>
          <li>Dense, randomly-initialized, feed-forward netì€ ê¸°ì¡´ì˜ original networkì™€ í•„ì í•˜ëŠ” ì„±ëŠ¥ì„ ê°–ëŠ” sub networks(winning tickets)ë¥¼ ê°–ëŠ”ë‹¤.</li>
        </ul>
      </li>
      <li>ì´ Lottery Ticket(sub network)ì„ ì°¾ëŠ” í•œ ë°©ë²•ì„ ì œì•ˆí•¨</li>
      <li>Identifying winning tickets
        <ol>
          <li>ë„¤íŠ¸ì›Œí¬ ì„ì˜ë¡œ ì´ˆê¸°í™” $f(x;\theta_0)$</li>
          <li>ë„¤íŠ¸ì¿¼ìœ¼ jë²ˆ í•™ìŠµí•˜ì—¬ íŒŒë¼ë¯¸í„° $\theta_j$ë¥¼ ë„ì¶œ</li>
          <li>íŒŒë¼ë¯¸í„° $\theta_j$ë¥¼ $p$%ë§Œí¼ Pruningí•˜ì—¬ mask $m$ì„ ìƒì„±($p$ëŠ” ë³´í†µ 20%)</li>
          <li>Maskë˜ì§€ ì•Šì€ íŒŒë¼ë¯¸í„°ë¥¼ $\theta_0$ë¡œ ë˜ëŒë¦¬ê³ (ì´ˆê¸°í™”í•˜ê³ ), ì´ë¥¼ winning ticketì´ë¼ ì§€ì¹­ $f(x;m \odot\theta_0)$</li>
          <li>Target sparsityì— ë„ë‹¬í•  ë•Œê¹Œì§€ 2-4 ë°˜ë³µ</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>Stabilizing the Lottery Ticket Hypothesis(arXiv 2019): Weight Rewinding
    <ul>
      <li>LTHì˜ ê²½ìš° ë°ì´í„°ì…‹ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ í¬ê¸°ê°€ ì»¤ì¡Œì„ ë•Œ ë¶ˆì•ˆì •í•œ ëª¨ìŠµì„ ë³´ì„</li>
      <li>kë²ˆì§¸ epochì—ì„œ í•™ìŠµí•œ íŒŒë¼ë¯¸í„°ë¡œ ë„¤íŠ¸ì›Œí¬ë¥¼ ì´ˆê¸°í™” í•˜ë©´ í•™ìŠµì´ ì•ˆì •í™” ë¨ì„ ë³´ì„</li>
      <li>ë…¼ë¬¸ì—ì„œëŠ” ì´ iterationì˜ 0.1% ~ 7% ì •ë„ì˜ ì§€ì ì„ rewinding pointë¡œ ì–¸ê¸‰</li>
    </ul>
  </li>
  <li>Comparing Rewinding And Fine-tuning In Neural Network Pruning(ICLR 2020): Learning Rate Rewinding
    <ul>
      <li>Weight rewinding ëŒ€ì‹ , weightëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , í•™ìŠµí–ˆë˜ Learning rate schedulingì„ íŠ¹ì • ì‹œì (k)ë¡œ rewindingí•˜ëŠ” ì „ëµì„ ì œì•ˆ</li>
      <li>ì–´ëŠ ì‹œì ì˜ weightë¥¼ rewindí• ì§€ì— ëŒ€í•œ íŒŒë¼ë¯¸í„° ê³ ë¯¼ ì—†ì´, Learning rateë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•œ í›„ ë‹¤ì‹œ í•™ìŠµí•˜ë©´ ëŒ€ì²´ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„</li>
    </ul>
  </li>
  <li>LTH ê´€ë ¨ ê¸°íƒ€ ì¶”ì²œ ë…¼ë¬¸
    <ul>
      <li>ì™œ ì˜ë˜ëŠ”ì§€ ì•„ì§ ëª…ì¾Œíˆ ì„¤ëª…ë˜ì§€ ì•ŠìŒ(but RL, NLP ë“±ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì ìš©ì´ ê°€ëŠ¥í•¨)</li>
      <li>íŠ¹ì • initializationì— ëŒ€í•´ì„œ, í•™ìŠµ í›„ì— ë„ë‹¬í•˜ëŠ” ê³µê°„ì´ ìœ ì‚¬í•˜ë‹¤? â†’ â€œë„¤íŠ¸ì›Œí¬ í•™ìŠµ ë° ìˆ˜ë ´â€ ê´€ë ¨ ì—°êµ¬ì™€ ì ‘ëª©ë˜ëŠ” íŠ¸ë Œë“œ</li>
    </ul>
  </li>
  <li>Linear Mode Connectivity and the Lottery Ticket Hypothesis(ICML 2020)
    <ul>
      <li>ë„¤íŠ¸ì›Œí¬ì˜ í•™ìŠµ ë° ìˆ˜ë ´ ê´€ë ¨ëœ ì‹¤í—˜</li>
      <li>íŠ¹ì • í•™ìŠµ ì‹œì (epoch at 0, k)ì—ì„œ seedë¥¼ ë³€ê²½í•˜ì—¬ ë‘ê°œì˜ Netì„ í•™ìŠµ â†’ SGDë¥¼ í†µí•œ ê³„ì‚° ê²°ê³¼ê°€ ë‹¤ë¥´ë¯€ë¡œ, ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ìˆ˜ë ´</li>
      <li>ë‘˜ ê°„ì˜ weightë¥¼ Linear interpolationí•˜ì—¬, ì„±ëŠ¥ì„ ë¹„êµ</li>
      <li>ë‘ weight ê³µê°„ ì‚¬ì´ì˜ interpolated netë“¤ì˜ ì„±ëŠ¥ì„ í™•ì¸</li>
      <li>íŠ¹ì • ì‹œì ë¶€í„° ë„¤íŠ¸ì›Œí¬ê°€ ìˆ˜ë ´í•˜ëŠ” ê³µê°„ì€ í•œ plateau ìœ„ì— ìˆëŠ”ê²ƒì´ ì•„ë‹Œê°€?ë¼ëŠ” í•´ì„ì„ ì œì‹œ</li>
    </ul>
  </li>
  <li>Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask(NeurIPS 2019)
    <ul>
      <li>LTHëŠ” $w_f$ì˜ L1 normìœ¼ë¡œ ì¤„ì„ ì„¸ì›Œì„œ maskingí•˜ëŠ”ë° $w_i$ë¥¼ ê³ ë ¤í•˜ë©´?</li>
      <li>$w_i, w_f$ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ë©´?(NLP ìª½ì˜ movement pruningì²˜ëŸ¼)</li>
    </ul>
  </li>
  <li>Pruning at Initialization(unstructured)
    <ul>
      <li>Trainì´ì „ì— â€œì¤‘ìš”ë„â€ë¥¼ ë³´ê³  Pruningì„ ìˆ˜í–‰í•˜ì. ê·¸ í›„ í•™ìŠµí•˜ë©´ ì‹œê°„ì´ í›¨ì”¬ ì ˆì•½ëœë‹¤.</li>
      <li>ì¤‘ìš”ë„ëŠ” ì–´ë–»ê²Œ ê³„ì‚°?
        <ul>
          <li>SNIP(ICLR 2018): Training ë°ì´í„° ìƒ˜í”Œ, Forwardí•´ì„œ Gradientì™€ Weightì˜ ê³±ì˜ ì ˆëŒ“ê°’ìœ¼ë¡œ!</li>
          <li>GraSP(ICLR 2019): Training ë°ì´í„° ìƒ˜í”Œ, Forwardí•´ì„œ Hessian-gradient productì™€ Weightì˜ ê³±ìœ¼ë¡œ!</li>
          <li>SynFlow(NeurIPS 2020): ì „ë¶€ 1ë¡œ ëœ ê°€ìƒ ë°ì´í„°ë¥¼ Forwardí•´ì„œ Gradientì™€ Weightì˜ ê³±ìœ¼ë¡œ!</li>
        </ul>
      </li>
      <li>ë„¤íŠ¸ì›Œí¬ì˜ ê°€ëŠ¥ì„±ì„ ë³¸ë‹¤ â†’ Training Free NAS/AutoML Zero-const proxies for Lightweight NAS(ICLR 2021)
        <ul>
          <li>Pruning at initializationì˜ ê¸°ë²•ë“¤ì„ ì¼ì¢…ì˜ scoreë¡œ networkë¥¼ ì •ë ¬</li>
          <li>ê° ê¸°ë²•ì˜ scoreì™€ ì‹¤ì œ í•™ìŠµ ê²°ê³¼ì˜ ìƒê´€ê´€ê³„(Spearman ìƒê´€ê³„ìˆ˜)ë¥¼ í™•ì¸</li>
          <li>ìƒê°ë³´ë‹¤ scoreì™€ ì‹¤ í•™ìŠµ ê²°ê³¼ì˜ ìƒê´€ ê³„ìˆ˜ê°€ ë†’ê³ , votingì„ í•˜ë©´ ë”ìš± ë†’ë‹¤</li>
          <li>Synflowê°€ CVì™¸ ì—¬ëŸ¬ task(NLP, ASR)ì—ë„ ë†’ì€ ìƒê´€ê³„ìˆ˜ë¥¼ ë³´ì—¬ì¤Œ</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ì•„ì‰¬ìš´ì 
    <ul>
      <li>Structured/Unstructured ëª¨ë‘ ê°„ë‹¨í•œ Architectureë¡œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìˆ˜í–‰(Modern architectureëŠ” ì˜ ë‹¤ë£¨ì§€ ì•ŠìŒ)</li>
      <li>ë‹¤ì–‘í•œ Architectureì— ëŒ€í•´ì„œ ì—¬ëŸ¬ ì˜ˆì™¸ì²˜ë¦¬, ì¶”ê°€ ê²€ì¦ì´ í•„ìš”í•¨</li>
      <li>íŒŒë¼ë¯¸í„° ìˆ˜, FLOPsë¡œ ë¹„êµë¥¼ ë§ì´ í•˜ëŠ” í¸ì´ë‚˜ ì‹¤ì§ˆì  latency í–¥ìƒ ì •ë„ì™€ëŠ” ì°¨ì´ê°€ ìˆìŒ</li>
      <li>UnstructuredëŠ” íŠ¹íˆ ì•„ì§ ê°€ì†ë˜ëŠ” HWê°€ ë§ì§€ ì•ŠìŒ</li>
    </ul>
  </li>
</ul>

<h2 id="knowledge-distillation">Knowledge Distillation</h2>

<h3 id="kdì˜-í•µì‹¬">KDì˜ í•µì‹¬</h3>

<blockquote>
  <p>Teacherì˜ ì •ë³´ë¥¼ ì–´ë–»ê²Œ ë¹¼ë‚´ëŠ”ê°€?</p>

</blockquote>

<h3 id="response-based-knowledge-distillation">Response-Based Knowledge Distillation</h3>

<ul>
  <li>Teacher modelì˜ last output layerë¥¼ í™œìš©í•˜ëŠ” ê¸°ë²•, ì¦‰ ì§ì ‘ì ì¸ final predictionì„ í™œìš©</li>
  <li>ëŒ€í‘œì ìœ¼ë¡œ hinton lossê°€ ìˆìŒ</li>
</ul>

<h3 id="feature-based-knowledge-distillation">Feature-Based Knowledge Distillation</h3>

<ul>
  <li>Teacherì˜ layerì˜ ì¤‘ê°„ ì¤‘ê°„ì˜ intermediate representations(feature)ë¥¼ studentê°€ í•™ìŠµí•˜ë„ë¡ ìœ ë„</li>
  <li>$F_t(x)$,  $F_s(t)$ë¥¼ ê°ê° teacher, studentì˜ feature mapì´ë¼ í•˜ê³  $T_t$, $T_s$ë¥¼ ê°ê° techer, studentì˜ transformation functionì´ë¼ í•˜ì.</li>
  <li>Distance $d$ê°€ ìˆì„ ë•Œ, feature based KDì˜ lossëŠ” ì•„ë˜ì²˜ëŸ¼ ì •ì˜ëœë‹¤.
    <ul>
      <li>$L_{distill} = d(T_t(F_t), T_s(F_s))$</li>
    </ul>
  </li>
  <li>ê°€ì¥ ê°„ë‹¨í•˜ê²ŒëŠ” $d$ì€ L2 norm, $T_t$ëŠ” identity, $T_s$ëŠ” learnableí•œ linear transformation matrixë¡œ ì •ì˜í•œë‹¤.
    <ul>
      <li>feature channel ìˆ˜ê°€ ë³´í†µì€ teacherê°€ ë§ìœ¼ë¯€ë¡œ, ì´ë¥¼ ë§ì¶°ì£¼ê¸° ìœ„í•´ ë„ì…</li>
    </ul>
  </li>
  <li>ë…¼ë¬¸ë“¤ì˜ ì£¼ëœ ë°©í–¥ì€, Feature distillation ê³¼ì •ì—ì„œ ìœ ìš©í•œ ì •ë³´ëŠ” ê°€ì ¸ì˜¤ê³ , ì¤‘ìš”í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ê°€ì ¸ì˜¤ì§€ ì•Šë„ë¡ Transformation functionê³¼ distance, ìœ„ì¹˜ ë“±ì„ ì¡°ì •í•˜ëŠ” ê²ƒ
    <ul>
      <li>ì¤‘ê°„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ë¯€ë¡œ, network êµ¬ì¡°ì— í¬ê²Œ ì˜ì¡´í•¨</li>
    </ul>
  </li>
</ul>

<h3 id="relation-based-knowledge-distillation">Relation-Based Knowledge Distillation</h3>

<ul>
  <li>ë‹¤ë¥¸ ë ˆì´ì–´ë‚˜ Sampleë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ì •ì˜í•˜ì—¬ Knowledge distllationì„ ìˆ˜í–‰</li>
  <li>Relation among examples represented by teacher to student</li>
</ul>

<h3 id="ì£¼ì˜">ì£¼ì˜</h3>

<ul>
  <li>ì¬í˜„ì´ ì˜ ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì£¼ì¥ì„ í•˜ëŠ” ë…¼ë¬¸ì´ ë‹¤ìˆ˜ ì¡´ì¬</li>
  <li>Contrastive representation distillation, ICLR 2019</li>
  <li>NLPì˜ ê²½ìš° Transformer, Transfer Learningì´ main streamì´ì–´ì„œ KD ì—°êµ¬ê°€ í™œë°œ</li>
</ul>

<h3 id="ì¶”ê°€-ë…¼ë¬¸">ì¶”ê°€ ë…¼ë¬¸</h3>

<ul>
  <li>í¬ê¸°ê°€ ë¬¸ì œê°€ ì•„ë‹Œ ì„±ëŠ¥ì´ ë¬¸ì œë¼ë©´?</li>
  <li>Self-training with noisy student improves imagenet classification(CVPR 2019)
    <ul>
      <li>ì•„ë˜ì™€ ê°™ì€ ë°©ì‹ì„ ì‚¬ìš©
        <ul>
          <li>Train a teacher model on labeled images</li>
          <li>Use the teacher to generate pseudo labels on unlabeled images</li>
          <li>Train a student model on the combination of labeled images and pseudo labeled images</li>
        </ul>
      </li>
      <li>Studentë¥¼ ë‹¤ìŒ iterationì—ì„œì˜ teacherë¡œ, studentëŠ” ìœ ì§€í•˜ê±°ë‚˜ ì ì  í‚¤ì›€</li>
      <li>Unlabeled ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë‹¹ì‹œ ImageNet SOTA ê¸°ë¡</li>
      <li>KDì™€ì˜ ì°¨ì´ì ?
        <ul>
          <li>Noiseë¥¼ ì¶”ê°€(input noise: RandAug, Model noise: dropout, stochastic depth function)</li>
          <li>Studentê°€ ì ì  ì»¤ì§€ëŠ” framework</li>
          <li>Soft targetì„ ì‚¬ìš©í•œë‹¤ëŠ” ì ì—ì„œ KDì™€ì˜ ìœ ì‚¬ì„± ì¡´ì¬</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
:ET