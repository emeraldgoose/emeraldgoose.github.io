I"÷H<blockquote>
  <p>ëª¨ê¸°ì—… ì½”ë”©í…ŒìŠ¤íŠ¸ì— íŒŒì´ì¬ ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œë§Œ MLPë¥¼ êµ¬í˜„í•˜ëŠ” ë¬¸ì œê°€ ë‚˜ì™”ë˜ ì ì´ ìˆìŠµë‹ˆë‹¤. ë‹¹ì‹œì— í•™ìŠµì´ ë˜ì§€ ì•Šì•„ ì½”ë”©í…ŒìŠ¤íŠ¸ì—ì„œ ë–¨ì–´ì¡Œì—ˆê³  êµ¬í˜„í•˜ì§€ ëª»í–ˆë˜ ê²ƒì´ ê³„ì† ìƒê°ë‚˜ì„œ êµ¬í˜„í•´ë´¤ìŠµë‹ˆë‹¤.</p>
</blockquote>

<h2 id="ê³„íš">ê³„íš</h2>
<p>ë°ì´í„°ì…‹ì„ MNISTë¡œ ì¡ê³  MLPë¥¼ êµ¬í˜„í•˜ê³ ì í–ˆìŠµë‹ˆë‹¤. ì½”ë”©í…ŒìŠ¤íŠ¸ë•Œë„ ì…ë ¥ìœ¼ë¡œ MNISTì™€ ë¹„ìŠ·í•œ ê°’ì´ ë“¤ì–´ì™”ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</p>

<p>ë ˆì´ì–´ëŠ” ì´ 3ê°œë¡œ input -&gt; (Linear -&gt; Activation) -&gt; (Linear -&gt; Activation) -&gt; (Linear -&gt; Softmax) -&gt; output ìœ¼ë¡œ ìƒê°í•˜ê³  ê° ëª¨ë“ˆ êµ¬í˜„ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.</p>

<h2 id="forward--backward-propagation">Forward &amp; Backward Propagation</h2>
<p>ëª¨ë¸ì´ í•™ìŠµì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ì—­ì „íŒŒ(backpropagation)ê°€ ì§„í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ê° ëª¨ë“ˆë“¤ì˜ ë¯¸ë¶„ê°’ì„ ì¶œë ¥í•˜ê³  chain ruleì— ì˜í•´ ê°’ë“¤ì„ ê³±í•´ê°€ë©´ì„œ Linear ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ì™€ ë°”ì´ì–´ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤.</p>

<p>ë¨¼ì €, ì•„ë˜ì²˜ëŸ¼ êµ¬ì„±ëœ ëª¨ë¸ì´ ìˆë‹¤ê³  ê°€ì •í•˜ê³  ìˆœì „íŒŒ(Feedforward)ë•Œ ê³„ì‚°ë˜ëŠ” ê³¼ì •ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model(
  (layer1): Sequential(
    (0): Linear(in_features=784, out_features=256, bias=True)
    (1): Sigmoid()
  )
  (layer2): Sequential(
    (0): Linear(in_features=256, out_features=10, bias=True)
    (1): Softmax()
  )
)
</code></pre></div></div>
<p>$y_1 = \sigma_{1}(z_1) = \sigma_1(w_1x + b_1), \sigma_{1} = \text{sigmoid}$</p>

<p>$\hat{y} = \sigma_{2}(z_2) = \sigma_{2}(w_2y_1+b_2), \sigma_{2} = \text{softmax}$</p>

<p>$L_{\text{MSE}} = \sum(\hat{y}-{y})^2$</p>

<p>ì´ì œ, ìœ„ ì‹ì„ ê±°ê¾¸ë¡œ ëŒë ¤ê°€ë©´ì„œ ì—­ì „íŒŒë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.</p>

<p>$W_2$ì— ëŒ€í•´ í¸ë¯¸ë¶„ëœ ê°’ì„ ë¨¼ì € êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ë©ë‹ˆë‹¤.</p>

<p>$\frac{\partial L}{ \partial w_2}=\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{dz_2} \cdot \frac{\partial z_2}{\partial w_2}$</p>

<p>${\partial \hat{y} \over \partial z_2} = \sigma_2(z_2)(\delta_{ij} - \sigma_2(z_2)), \ \delta_{ij}=
    \begin{cases}
        1, &amp; i=j \\ 0, &amp; i \ne j
    \end{cases}
$</p>

<p>$\frac{\partial L}{\partial w_2} = \frac{2}{m}(\hat{y}-y) \cdot \sigma_2(z_2)(\delta_{ij} - \sigma_2(z_2)) \cdot y_1$</p>

<p>$W_1$ì— ëŒ€í•´ í¸ë¯¸ë¶„ëœ ê°’ì„ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ë©ë‹ˆë‹¤.</p>

<p>$\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z_2} \cdot \frac{\partial z_2}{\partial y_1} \cdot \frac{\partial y_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_1}$</p>

<p>${\partial y_1 \over \partial z_1} = \sigma_1(z_1)(1 - \sigma_1(z_1)) = y_1(1 - y_1)$</p>

<p>$\frac{\partial L}{\partial w_1}= \frac{2}{m}(\hat{y}-y) \cdot \sigma_2(z_2)(\delta_{ij} - \sigma_2(z_2)) \cdot w_2 \cdot y_1 \cdot (1 - y_1) \cdot x$</p>

<p>gradientì˜ ê³„ì‚°ì—ì„œ ë§ˆì§€ë§‰ ê³±ì—ëŠ” <strong>ì…ë ¥ê°’</strong>ì— ëŒ€í•´ dot productí•˜ê³  ì…ë ¥ ë ˆì´ì–´ ë°©í–¥ìœ¼ë¡œ <strong>ì´ì „ ë ˆì´ì–´ì˜ weight</strong>ë¥¼ dot productí•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ Linear ë ˆì´ì–´ëŠ” ì…ë ¥ê°’ì„ ì €ì¥í•´ì•¼ backward ê³„ì‚°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">.module</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">X</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Pytorch Linear initialization
</span>        <span class="n">sqrt_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">in_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">sqrt_k</span><span class="p">,</span> <span class="n">sqrt_k</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">in_features</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">sqrt_k</span><span class="p">,</span> <span class="n">sqrt_k</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_features</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">mat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mat</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dz</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">dw</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dz</span><span class="p">)</span>
        <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dz</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dz</span><span class="p">)</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dz</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span>
</code></pre></div></div>

<p>ë˜í•œ, sigmoidë¥¼ í†µê³¼í•œ ì¶œë ¥ê°’ë“¤ì€ ì—­ì „íŒŒë•Œ <strong>element-wise product</strong>ë¥¼ ì§„í–‰í•´ì•¼ í•©ë‹ˆë‹¤. í™œì„±í™”í•¨ìˆ˜ëŠ” ì…ë ¥ê°’ ê°ê°ì— ëŒ€í•´ í•¨ìˆ˜ë¥¼ í†µê³¼ì‹œí‚¤ë¯€ë¡œ ì—­ì „íŒŒë•Œë„ ë˜‘ê°™ì´ ì§„í–‰ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. softmaxëŠ” element-wise independentí•˜ì§€ ì•Šì•„ element-wise productë¥¼ ìˆ˜í–‰í•´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.</p>

<p>ì´ ì—­ì „íŒŒë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ ë ˆì´ì–´ë§ˆë‹¤ backward()í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ì—¬ gradientë¥¼ ê³„ì‚°í•˜ê³  Optimizerë¥¼ ì´ìš©í•˜ì—¬ weightì™€ biasë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># optim.py
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">hcrot.layers.module</span> <span class="kn">import</span> <span class="n">Module</span>

<span class="k">class</span> <span class="nc">Optimizer</span><span class="p">:</span>
    <span class="s">"""Gradient Descent"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Net</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">lr_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">modules</span> <span class="o">=</span> <span class="n">Net</span><span class="p">.</span><span class="n">sequential</span> <span class="c1"># ê³„ì‚° ìˆœì„œê°€ ì •ì˜
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">lr_rate</span> <span class="o">=</span> <span class="n">lr_rate</span>
    
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dz</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">modules</span><span class="p">):</span>
            <span class="n">module_name</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">_get_name</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">module_name</span> <span class="o">==</span> <span class="s">"Linear"</span><span class="p">:</span>
                <span class="n">dz</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dz</span><span class="p">)</span>
                <span class="n">module</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight_update</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="nb">id</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="si">}</span><span class="s">.weight'</span><span class="p">,</span> <span class="n">module</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">lr_rate</span><span class="p">)</span>
                <span class="n">module</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight_update</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="nb">id</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="si">}</span><span class="s">.bias'</span><span class="p">,</span> <span class="n">module</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">lr_rate</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dz</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dz</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">weight_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">lr_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="c1"># idëŠ” ë‚˜ì¤‘ì— SGD, Adamì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë³€ìˆ˜ë¡œ ì´ ì½”ë“œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
</span>        <span class="k">return</span> <span class="n">weight</span> <span class="o">-</span> <span class="p">(</span><span class="n">lr_rate</span> <span class="o">*</span> <span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>
<script src="https://gist.github.com/emeraldgoose/f9b399f12abfaa6a653dd31e02241a70.js"></script>

<h2 id="ê²°ê³¼">ê²°ê³¼</h2>
<p>MNIST 5000ì¥ì„ í›ˆë ¨ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê³  1000ì¥ì„ í…ŒìŠ¤íŠ¸ë°ì´í„°ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1k18xXPI4qMx31qgSTajBwQ6NwjycTpkr" alt="" width="400" />
<img src="https://drive.google.com/uc?export=view&amp;id=1Pzta5dtXVxduFsIgHGaSqsHOtKsh6jSh" alt="" width="400" /></p>

<p>10 ì—í¬í¬ì—ë„ lossê°€ ì˜ ë–¨ì–´ì§€ê³  Accuracyë„ ì˜ ì¦ê°€í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>ë²¡í„° ê³„ì‚°ì´ë‚˜ ë‹¤ë¥¸ ìˆ˜ì‹ ê³„ì‚°ì— ë„ì›€ì´ ë˜ëŠ” numpy ì—†ì´ êµ¬í˜„í•˜ë ¤ê³  í•˜ë‹ˆ ì½”ë“œì—ì„œ ì‹¤ìˆ˜ë¥¼ ë§ì´ í–ˆìŠµë‹ˆë‹¤. ê³„ì‚° ê²°ê³¼ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ torchë‚˜ numpyì— ìˆëŠ” ë˜‘ê°™ì€ í•¨ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì €ì˜ ì½”ë“œë¥¼ ë¶ˆëŸ¬ì™€ ê³„ì‚°ê²°ê³¼ê°€ ë§ëŠ”ì§€ ê³„ì† í™•ì¸í–ˆìŠµë‹ˆë‹¤.</p>

<p>torchë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ê³¼ ìµœëŒ€í•œ ìœ ì‚¬í•˜ê²Œ ì‘ì„±í•  ìˆ˜ ìˆë„ë¡ êµ¬í˜„í•˜ê³ ì í–ˆìŠµë‹ˆë‹¤. torchì—ì„œëŠ” autograd ê¸°ëŠ¥ê³¼ í…ì„œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ ì‚¬ìš©ìê°€ ì‰½ê²Œ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.(ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°œë°œìë¶„ë“¤ ì¡´ê²½í•©ë‹ˆë‹¤.) í•˜ì§€ë§Œ ì§ì ‘ êµ¬í˜„í•˜ë ¤ë©´ ì—­ì „íŒŒë¥¼ ìœ„í•´ ë ˆì´ì–´ë§ˆë‹¤ ë¯¸ë¶„ì„ ì§„í–‰í•´ì¤˜ì•¼ í•˜ëŠ” ê³¼ì •ì´ ì¶”ê°€ë˜ì–´ ìƒê°ë³´ë‹¤ êµ¬í˜„ì´ ì–´ë ¤ì› ìŠµë‹ˆë‹¤.</p>

<h3 id="ë¬¸ì œì ">ë¬¸ì œì </h3>
<p>ê°€ì¥ í° ë¬¸ì œì ì€ e^x í•¨ìˆ˜ì˜ Overflow í˜„ìƒì…ë‹ˆë‹¤. ì…ë ¥ê°’ì´ ìŒìˆ˜ì´ë©´ì„œ í° ìˆ˜ì¼ ë•Œ softmaxì™€ sigmoid ê³„ì‚°ì—ì„œ overflow í˜„ìƒì´ ì¼ì–´ë‚¬ìŠµë‹ˆë‹¤.</p>

<h2 id="ì•ìœ¼ë¡œ">ì•ìœ¼ë¡œ</h2>
<p>MLPì— ì‚¬ìš©ë˜ëŠ” ë ˆì´ì–´ë“¤ë§Œ êµ¬í˜„ë˜ì—ˆì§€ë§Œ CNNì´ë‚˜ RNNì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë ˆì´ì–´ë“¤ì„ ì¶”ê°€í•  ìƒê°ì…ë‹ˆë‹¤. êµ¬í˜„í•˜ê¸° ìœ„í•´ì„œ ìˆ˜ì‹ì´ë‚˜ ì—­ì „íŒŒ ê³¼ì •ë“¤ì„ ì°¾ì•„ë³´ë‹ˆ ë‚œì´ë„ê°€ ìˆì–´ ë³´ì—¬ì„œ ì–¸ì œ ì¶”ê°€í•  ìˆ˜ ìˆì„ì§€ëŠ” ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤â€¦</p>

<h2 id="ì½”ë“œ">ì½”ë“œ</h2>
<blockquote>
  <p>êµ¬í˜„ëœ ì½”ë“œëŠ” ê¹ƒí—ˆë¸Œì— ìˆìŠµë‹ˆë‹¤.<br />
<a href="https://github.com/emeraldgoose/hcrot">https://github.com/emeraldgoose/hcrot</a></p>
</blockquote>

<h2 id="reference">Reference</h2>
<ul>
  <li><a href="http://taewan.kim/post/sigmoid_diff/">http://taewan.kim/post/sigmoid_diff/</a></li>
  <li><a href="https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/">https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/</a></li>
  <li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">https://pytorch.org/docs/stable/generated/torch.nn.Linear.html</a></li>
  <li><a href="https://velog.io/@gjtang/Softmax-with-Loss-%EA%B3%84%EC%B8%B5-%EA%B3%84%EC%82%B0%EA%B7%B8%EB%9E%98%ED%94%84">https://velog.io/@gjtang/Softmax-with-Loss-%EA%B3%84%EC%B8%B5-%EA%B3%84%EC%82%B0%EA%B7%B8%EB%9E%98%ED%94%84</a></li>
  <li>[https://aew61.github.io/blog/artificial_neural_networks/1_background/1.b_activation_functions_and_derivatives.html](https://aew61.github.io/blog/artificial_neural_networks/1_background/1.b_activation_functions_and_derivatives.html</li>
</ul>
:ET