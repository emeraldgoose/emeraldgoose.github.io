I"â	<h2 id="neural-networks">Neural Networks</h2>
<blockquote>
  <p>Neural Networks are function approximators that stack affine transformations followed by nonlinear transforms.</p>
</blockquote>

<ul>
  <li>ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ëŠ” ìˆ˜í•™ì ì´ê³  ë¹„ì„ í˜• ì—°ì‚°ì´ ë°˜ë³µì ìœ¼ë¡œ ì¼ì–´ë‚˜ëŠ” ì–´ë–¤ í•¨ìˆ˜ë¥¼ ëª¨ë°©(ê·¼ì‚¬)í•˜ëŠ” ê²ƒì´ë‹¤.</li>
</ul>

<h2 id="linear-neural-networks">Linear Neural Networks</h2>
<ul>
  <li>ê°„ë‹¨í•œ ì˜ˆì œë¥¼ ë“¤ì–´ë³´ì
    <ul>
      <li>Data : $D = (x_i,j_i)_{i=1}^N$</li>
      <li>Model : $\hat{y} = wx + b$</li>
      <li>Loss : loss$= \frac{1}{N}\sum_{i=1}^N (y_i-\hat{y_i})^2$</li>
    </ul>
  </li>
  <li>ê·¸ëŸ¬ë©´ ìµœì í™” ë³€ìˆ˜ì— ëŒ€í•´ì„œ í¸ë¯¸ë¶„ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤
    <ul>
      <li>$\frac{\partial loss}{\partial w} = \frac{\partial}{\partial w}\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y})^2 = \frac{\partial}{\partial w}\frac{1}{N}\sum_{i=1}^N(y_i - wx_i - b)^2 = -\frac{1}{N}\sum_{i=1}^N-2(y_i-wx_i-b)x_i$</li>
    </ul>
  </li>
  <li>ì´ì œ ìµœì í™” ë³€ìˆ˜ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆë‹¤.
    <ul>
      <li>$w \leftarrow w - \eta\frac{\partial loss}{\partial w}$</li>
      <li>$b \leftarrow b - \eta\frac{\partial loss}{\partial b}$</li>
    </ul>
  </li>
  <li>ë¬¼ë¡  multi dimensionì— ëŒ€í•´ì„œë„ í•¸ë“¤í•  ìˆ˜ ìˆë‹¤.
    <ul>
      <li>$y = W^Tx + b$</li>
      <li>ìœ„ì™€ ê°™ì€ ì‹ì„ $x \overset{\underset{\text{{ w,b }}}{}} {\rightarrow} y$</li>
    </ul>
  </li>
</ul>

<h2 id="beyond-linear-neural-networks">Beyond Linear Neural Networks</h2>
<ul>
  <li>What if we stack more?
    <ul>
      <li>$y = W_2^Th = W_2^TW_1^Tx$</li>
    </ul>
  </li>
  <li>ë¹„ì„ í˜• ë°©ì‹ë„ ê°€ëŠ¥
    <ul>
      <li>$y = W_2^Th = W_2^T\rho(W_1^Tx)$, $\rho$ëŠ” Nonlinear transformì´ë‹¤.</li>
    </ul>
  </li>
  <li>Activation functions
    <ul>
      <li>ReLU</li>
      <li>Sigmoid</li>
      <li>Hyperbolic Tangent</li>
    </ul>
  </li>
</ul>

<h2 id="multi-layer-perceptron">Multi-Layer Perceptron</h2>
<ul>
  <li>$y = W_2^Th = W_2^T\rho(W_1^Tx)$</li>
  <li>ì¶œë ¥ì„ ë‹¤ì‹œ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ ë ˆì´ì–´ë¥¼ ìŒ“ì€ ëª¨ë¸ì„ MLPë¼ í•œë‹¤.</li>
  <li>ë¬¼ë¡  ë” ê¹Šê²Œ ìŒ“ì„ ìˆ˜ ìˆë‹¤.
    <ul>
      <li>$y =  W_3^T h = W_3^T\rho(W_2^Th_1)$ $ = W_3^T\rho(W_2^T \rho(W_1^T x))$</li>
    </ul>
  </li>
  <li>loss functionì€ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©ë  ìˆ˜ ìˆì§€ë§Œ ìƒí™©ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.
    <ul>
      <li>Regression Task : MSE</li>
      <li>Classification Task : CE</li>
      <li>Probabilistic Task : MLE</li>
    </ul>
  </li>
</ul>
:ET