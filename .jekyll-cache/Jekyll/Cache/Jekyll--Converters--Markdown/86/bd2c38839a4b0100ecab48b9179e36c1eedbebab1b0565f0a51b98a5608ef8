I"›(<h2 id="related">Related</h2>
<blockquote>
  <p>ì´ì „ í¬ìŠ¤íŠ¸ì—ì„œ MLPë¥¼ êµ¬í˜„í–ˆê³  ì´ë²ˆì—ëŠ” CNNì„ êµ¬í˜„í•˜ëŠ” ì‚½ì§ˆì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.</p>
</blockquote>

<h2 id="cnn">CNN</h2>
<p>CNNì€ [Conv2d + Pooling + (Activation)] ë ˆì´ì–´ê°€ ìˆ˜ì§ìœ¼ë¡œ ìŒ“ì—¬ìˆëŠ” ë‰´ëŸ´ë„·ì„ ë§í•©ë‹ˆë‹¤. 
êµ¬í˜„í•´ë³´ë ¤ëŠ” CNNì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<ul>
  <li>Layer1 : Conv2d(1, 5, 5) -&gt; ReLU -&gt; MaxPool2d(2, 2)</li>
  <li>Layer2 : Conv2d(5, 7, 5) -&gt; ReLU -&gt; MaxPool2d(2, 2)</li>
  <li>Flatten Layer</li>
  <li>Linear Layer</li>
</ul>

<h2 id="conv2d">Conv2d</h2>
<h3 id="convolution">Convolution</h3>
<p>ë‹¨ìˆœí•˜ê²Œ forë¬¸ ì¤‘ì²©ìœ¼ë¡œ Convolutionì„ êµ¬í˜„í•˜ë©´ ì—°ì‚°ì†ë„ê°€ ë„ˆë¬´ ëŠë ¤ì§€ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ ë°©ë²•ë“¤ì„ ì°¾ì•„ë´¤ìŠµë‹ˆë‹¤.</p>
<ul>
  <li>Numpyë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•</li>
  <li>FFTë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•</li>
</ul>

<p>FFT(Fast Fourier Transform)ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ìˆ˜ì‹ê³¼ êµ¬í˜„ë°©ë²•ì´ ì–´ë ¤ì›Œì„œ ë°”ë¡œ í¬ê¸°í•˜ê³  ì²« ë²ˆì§¸ ë°©ë²•ì¸ ìŠ¤íƒ ì˜¤ë²„í”Œë¡œìš°ì˜ ì½”ë“œë¥¼ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">convolve2d</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span><span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">shape</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">f</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">strd</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">lib</span><span class="p">.</span><span class="n">stride_tricks</span><span class="p">.</span><span class="n">as_strided</span>
    <span class="n">subM</span> <span class="o">=</span> <span class="n">strd</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">s</span><span class="p">,</span> <span class="n">strides</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="n">strides</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'ij,ijkl-&gt;kl'</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">subM</span><span class="p">)</span>
</code></pre></div></div>
<p>ë™ì‘ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<ul>
  <li>ì ìš©í•  ì´ë¯¸ì§€ë¥¼ ì»¤ë„ í¬ê¸°ì˜ ë§ê²Œ ì˜ë¼ì„œ ì €ì¥í•©ë‹ˆë‹¤.</li>
  <li>ì˜ë ¤ì§„ ì´ë¯¸ì§€ë“¤ì„ í•˜ë‚˜ì”© êº¼ë‚´ ì»¤ë„ê³¼ element-wise productë¥¼ ì§„í–‰í•˜ì—¬ ë”í•œ ê°’ë“¤ì„ ë¦¬í„´í•©ë‹ˆë‹¤.</li>
  <li>forë¬¸ìœ¼ë¡œ ì»¤ë„ì„ ì›€ì§ì¼ í•„ìš” ì—†ì–´ ê³±ì…ˆê³¼ í•© ì—°ì‚°ë§Œ ì§„í–‰í•˜ë¯€ë¡œ ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤.</li>
</ul>

<h3 id="forward">Forward</h3>
<p>Conv2d ë ˆì´ì–´ì˜ forwardë¥¼ ë¨¼ì € ë³´ê² ìŠµë‹ˆë‹¤.</p>

<h3 id="backward">Backward</h3>
<p>ì´ì œ Conv2d ë ˆì´ì–´ì˜ backwardë¥¼ ê³„ì‚°í•´ë³´ê² ìŠµë‹ˆë‹¤.</p>

<h2 id="crossentropyloss">CrossEntropyLoss</h2>
<p>NLL(Negative Log Likelihood)ë¥¼ ì´ìš©í•œ Loss functionì…ë‹ˆë‹¤. Pytorchì˜ CrossEntropyLossëŠ” Softmaxì™€ NLLLossë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.</p>

<p>ì´ì „ì— êµ¬í˜„í–ˆë˜ CrossEntropyLossëŠ” logë¥¼ ì·¨í•˜ëŠ” ê³¼ì •ì—ì„œ ì…ë ¥ê°’ì´ ìŒìˆ˜ê°€ ë“¤ì–´ì˜¤ëŠ” ê²½ìš° ì—ëŸ¬ë¥¼ ì¼ìœ¼ì¼°ìŠµë‹ˆë‹¤. ë°˜ë©´ì— Softmaxì™€ NLLLossë¡œ êµ¬í˜„í•˜ê²Œ ë˜ë©´ ìŒìˆ˜ ì…ë ¥ì— ëŒ€í•´ì„œë„ cross entropy ê°’ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">NLLLoss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span>\
        <span class="p">[</span><span class="o">-</span><span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">label</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="p">)])</span> <span class="o">/</span> <span class="n">batch</span>

<span class="k">def</span> <span class="nf">NLLLoss_deriv</span><span class="p">(</span><span class="n">y_pred</span><span class="p">):</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>\
        <span class="p">[</span><span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">/</span><span class="n">batch</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span> \
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">math</span>
    <span class="n">r_</span> <span class="o">=</span> <span class="n">softmax_</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>\
        <span class="p">[</span><span class="n">math</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">r_</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r_</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span> \
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r_</span><span class="p">))]</span>

<span class="k">def</span> <span class="nf">log_softmax_deriv</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">math</span>
    <span class="n">r_</span> <span class="o">=</span> <span class="n">softmax_</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>\
        <span class="p">[</span><span class="n">r_</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">label</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">==</span><span class="n">j</span> <span class="k">else</span> <span class="n">r_</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> \
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r_</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span> \
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r_</span><span class="p">))]</span>
</code></pre></div></div>

<h2 id="reference">Reference</h2>
<ul>
  <li><a href="https://stackoverflow.com/a/43087771">Convolve2d(StackOverflow)</a></li>
  <li><a href="https://ratsgo.github.io/deep%20learning/2017/04/05/CNNbackprop/">https://ratsgo.github.io/deep%20learning/2017/04/05/CNNbackprop/</a></li>
  <li><a href="https://velog.io/@changdaeoh/backpropagationincnn">https://velog.io/@changdaeoh/backpropagationincnn</a></li>
  <li><a href="https://towardsdatascience.com/backpropagation-in-a-convolutional-layer-24c8d64d8509">https://towardsdatascience.com/backpropagation-in-a-convolutional-layer-24c8d64d8509</a></li>
  <li><a href="https://towardsdatascience.com/forward-and-backward-propagation-of-pooling-layers-in-convolutional-neural-networks-11e36d169bec">https://towardsdatascience.com/forward-and-backward-propagation-of-pooling-layers-in-convolutional-neural-networks-11e36d169bec</a></li>
</ul>
:ET