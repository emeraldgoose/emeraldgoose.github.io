I"¶B<h2 id="overview">Overview</h2>

<h3 id="cv-ê²½ëŸ‰í™”ì™€-nlp-ê²½ëŸ‰í™”ì˜-ì°¨ì´ì ">CV ê²½ëŸ‰í™”ì™€ NLP ê²½ëŸ‰í™”ì˜ ì°¨ì´ì ?</h3>

<ul>
  <li>Original task â†’ Target taskë¡œì˜ fine tuningí•˜ëŠ” ë°©ì‹ì´ ì£¼ëœ íë¦„</li>
  <li>ê¸°ë³¸ì ìœ¼ë¡œ transforemer êµ¬ì¡°</li>
  <li>ëª¨ë¸ êµ¬ì¡°ê°€ ê±°ì˜ ìœ ì‚¬í•´ì„œ, ë…¼ë¬¸ì˜ ì¬í˜„ ê°€ëŠ¥ì„±ì´ ë†’ê³ , ì½”ë“œ ì¬ì‚¬ìš©ì„±ë„ ë†’ìŒ</li>
</ul>

<h3 id="bert-profiling">BERT Profiling</h3>

<h3 id="profiling-model-size-and-computations">Profiling: Model size and computations</h3>

<ul>
  <li>Embedding layer: look up tableì´ë¯€ë¡œ, FLOPsëŠ” ì—†ìŒ</li>
  <li>Linear before Attn: k, q, v mat ì—°ì‚°ìœ¼ë¡œ linear after attn ëŒ€ë¹„ 3ë°°</li>
  <li>MHA: matmul, softmaxë“±ì˜ ì—°ì‚°, ë³„ë„ íŒŒë¼ë¯¸í„°ëŠ” ì—†ìŒ</li>
  <li>FFN: ê°€ì¥ ë§ì€ íŒŒë¼ë¯¸í„°, ì—°ì‚°íšŸìˆ˜</li>
  <li>íš¨ìœ¨ì ì¸ GPU ì—°ì‚°ì„ ìœ„í•´ ë‹¨ë…ìœ¼ë¡œ CPUì—ì„œ ì‚¬ìš©í•˜ëŠ” ë©”ëª¨ë¦¬ë³´ë‹¤ ì†Œë¹„ëŸ‰ì´ ë” í¼(ì—°ì‚°ì†ë„ë¥¼ ìœ„í•´ CPU, GPUì— ë™ì¼ í…ì„œë¥¼ ë“¤ê³  ìˆê±°ë‚˜ ë“±ë“±)</li>
  <li>MHA íŒŒíŠ¸ëŠ” ì´ë¡  ì—°ì‚° íšŸìˆ˜ ëŒ€ë¹„ ì†ë„ê°€ ë§¤ìš° ëŠë¦°ë°, ì´ëŠ” ì—¬ëŸ¬ ì—°ì‚°ì˜ ì¡°í•©(matmul, softmax) ë•Œë¬¸ì¸ ê²ƒìœ¼ë¡œ ë³´ì—¬ì§</li>
  <li>Linear Layerì—ì„œëŠ” GPUê°€ ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì—¬ì£¼ë‚˜, CPUëŠ” ì´ë¡  ì—°ì‚° íšŸìˆ˜ì™€ ìœ ì‚¬í•œ ê²½í–¥ì„ ë³´ì„</li>
  <li>FFN íŒŒíŠ¸ê°€ ëª¨ë¸ì˜ ì£¼ëœ bottleneck</li>
</ul>

<h2 id="paper">Paper</h2>

<h3 id="are-sixteen-heads-really-better-than-oneneurips-2019">Are Sixteen Heads Really Better than One?(NeurIPS 2019)</h3>

<ul>
  <li>MHAëŠ” êµ¬ì¡°ìƒ ë³µí•©ì ì¸ ì •ë³´ë¥¼ ë‹´ì•„ë‘ê¸° ìœ„í•´ ì œì•ˆë¨.</li>
  <li>ì‹¤í—˜ì—ì„œ ë§ì€ í—¤ë“œê°€ ì§€ì›Œì§€ë”ë¼ë„ ì„±ëŠ¥ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šì•˜ìŒ</li>
  <li>Attnì„ Pruningí•˜ì—¬ 17.5% ì†ë„ í–¥ìƒì„ ì´ë£¸</li>
  <li>Are All Attention Heads Important?
    <ul>
      <li>í—¤ë“œë¥¼ í•˜ë‚˜ë§Œ ë‚¨ê²¨ë‘ê³  ë‚˜ë¨¸ì§€ë¥¼ ì§€ìš´ í›„ ì„±ëŠ¥ ë“œëì„ ê¸°ë¡í•˜ëŠ” ì‹¤í—˜</li>
      <li>ëˆˆì— ëŒ ë§Œí¼ ì„±ëŠ¥ë“œëì´ ì—†ì—ˆìŒ</li>
    </ul>
  </li>
  <li>Are Important Heads the Same Across Datasets?
    <ul>
      <li>í•˜ì§€ë§Œ ì—­í•  ìˆ˜í–‰ì— ê¼­ í•„ìš”í•œ í—¤ë“œê°€ ìˆë‹¤.</li>
      <li>ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œë„ ê°™ì€ í—¤ë“œê°€ ì¤‘ìš”í•œê°€? â†’ ì¤‘ìš”í•œ í—¤ë“œëŠ” ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œë„ ì¤‘ìš”í•œ ê²½ìš°ê°€ ë§ë‹¤.</li>
    </ul>
  </li>
  <li>ì§€ê¸ˆê¹Œì§„ íŠ¹ì •í•œ layerì—ì„œ headë¥¼ ì œê±°í•˜ì—¬ íš¨ê³¼ë¥¼ í™•ì¸í–ˆë‹¤. ì—¬ëŸ¬ layerì— ëŒ€í•œ ì—¬ëŸ¬ headë¥¼ ì œê±°í–ˆì„ ë•ŒëŠ” ì–´ë–¤ í˜„ìƒì´ ë°œìƒí•˜ëŠ”ê°€?</li>
  <li>Iterative Pruning of Attention Heads
    <ul>
      <li>
        <p>Attn í—¤ë“œë¥¼ ì¤‘ìš”ë„ë¥¼ ê°„ì ‘ì ìœ¼ë¡œ ê³„ì‚°í•œë‹¤.</p>

        <p><img src="https://lh3.google.com/u/0/d/1FXNx3_Z0eG4_71XXIGdoKzy4fCphHvci" alt="" /></p>

        <ul>
          <li>$\xi_h$ : mask variables, 0: masked(removed), 1: unmaksed, $X$: data distribution</li>
        </ul>
      </li>
      <li>
        <p>ë°ì´í„°ìƒ˜í”Œì— ëŒ€í•´ì„œ íŠ¹ì • í—¤ë“œë¥¼ ì‚´ë¦° ê²ƒì— ëŒ€í•œ Lossì™€ í•´ë‹¹ í—¤ë“œë¥¼ ì§€ìš´ ê²ƒì— ëŒ€í•œ Lossì˜ ì°¨ì´ë¥¼ ë’¤ì˜ ì‹ìœ¼ë¡œ approximateí•˜ì—¬ ì‚¬ìš©</p>

        <p><img src="https://lh3.google.com/u/0/d/1F2NzJc5-OPCYj16GcsYVlsQ6Rfu3XVJC" alt="" /></p>
      </li>
    </ul>
  </li>
  <li>Effect of Pruning on Efficiency
    <ul>
      <li>ë°°ì¹˜ì‚¬ì´ì¦ˆê°€ ì‘ì€ ê²½ìš° íš¨ê³¼ê°€ ì¢‹ì§€ ì•Šì§€ë§Œ í° ê²½ìš° íš¨ê³¼ê°€ ì¢‹ì•˜ìŒ</li>
    </ul>
  </li>
</ul>

<h3 id="movement-pruning-adaptive-sparsity-by-fine-tuningneurips-2019">Movement Pruning: Adaptive Sparsity by Fine-Tuning(NeurIPS 2019)</h3>

<ul>
  <li>Transfer learningì—ì„œëŠ” Magnitude pruningì˜ íš¨ê³¼ê°€ ì¢‹ì§€ ì•ŠìŒ
    <ul>
      <li>ì™œëƒ? original taskì—ì„œ target taskë¡œ transfer learning ê³¼ì •ì—ì„œ Weight ê°’ì˜ ë³€í™”ê°€ ê·¸ë¦¬ í¬ì§€ ì•ŠìŒ</li>
    </ul>
  </li>
  <li>ê°’ì´ í° Weightì€ ê·¸ëŒ€ë¡œ ê°’ì´ í¼ â†’ Weight ê°’ì€ ë§ì´ ë³€í•˜ì§€ ì•ŠìŒ</li>
  <li>Original modelì—ì„œì˜ í° Weightì€ Original taskì—ì„œ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°–ëŠ” Weightì¼ ê°€ëŠ¥ì„±ì´ í¼
    <ul>
      <li>Fine-tuned modelì—ì„œ í° Weightì€ Target taskì—ì„œ ì¤‘ìš”í•˜ì§€ ì•Šì€ Weightì¼ ìˆ˜ë„ ìˆìŒ</li>
    </ul>
  </li>
  <li>Magnitude Pruningì—ì„œëŠ” Original taskì—ì„œë§Œ ì¤‘ìš”í–ˆë˜ Weightë“¤ì´ ì‚´ì•„ë‚¨ì„ ìˆ˜ ìˆìŒ
    <ul>
      <li>â†’ Movement Pruning : Transfer Learning ê³¼ì •ì—ì„œ, Weightì˜ ì›€ì§ì„ì„ ëˆ„ì í•´ê°€ë©° Pruningí•  Weightë¥¼ ê²°ì •í•˜ì!</li>
    </ul>
  </li>
  <li>
    <p>Background: Score-Based Pruning(Unstructured pruning formulation)</p>

    <p><img src="https://lh3.google.com/u/0/d/10-bcZLGMSOl1y9dQWwZyvf9tg7DQUWm_" alt="" /></p>

    <ul>
      <li>$S = (|W_{i,j}|)_{1&lt;i,j&lt;n}$</li>
    </ul>
  </li>
  <li>Method Interpretation
    <ul>
      <li>0ì—ì„œë¶€í„° ë©€ì–´ì§€ëŠ” weightë¥¼ ê³ ë¥´ëŠ” ë°©ë²•</li>
      <li>Movement Pruningì˜ score ìœ ë„
        <ul>
          <li>Masks are computed using the $M = Top_v(S)$</li>
          <li>Learn both weights $W$, and importance score $S$ during training(fine-tuning)</li>
          <li>Forward pass, we compute all $i, a_i = \sum_{k=1}^n W_{i,k}M_{i,k}x_k$</li>
          <li>Forward ê³¼ì •ì—ì„œ, Topì— ì†í•˜ì§€ ëª»í•œ ë‚˜ë¨¸ì§€ëŠ” maskingì´ 0ì´ ë˜ì–´, gradient ê°’ì´ ì—†ì–´ì§„ë‹¤.</li>
          <li>straight-through estimator(Quantization functionì˜ back propagationì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” Technique)ë¥¼ í†µí•˜ì—¬ gradientë¥¼ ê³„ì‚°</li>
          <li>ë‹¨ìˆœíˆ, gradientê°€ â€œstraight-throughâ€í•˜ê²Œ $S$ë¡œ ì „ë‹¬($M$ â†’ $S$)</li>
          <li>$S$ì˜ ë³€í™”ì— ë”°ë¥¸ Lossì˜ ë³€í™”ëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬</li>
          <li>$\frac{\partial L}{\partial S_{i,j}} = \frac{\partial L}{\partial a_i} \frac{\partial a_i}{\partial S_{i,j}} = W_{i,j} x_{j}$</li>
        </ul>
      </li>
      <li>Movement Pruningì˜ score í•´ì„
        <ul>
          <li>Gradient descentë¥¼ ìƒê°í•´ ë³¼ ë•Œ($w = w - \alpha \frac{\partial L}{\partial w}$, $\alpha$ : lr rate)</li>
          <li>$W_{i,j} &gt; 0, \frac{\partial L}{\partial W_{i,j}} &lt; 0$ì´ë©´, $W_{i,j}$ëŠ” ì¦ê°€í•˜ëŠ” ë°©í–¥(ì´ë¯¸ ì–‘ìˆ˜ì—ì„œ ë” ì»¤ì§)</li>
          <li>$W_{i,j} &lt; 0, \frac{\partial L}{\partial W_{i,j}} &gt; 0$ì´ë©´, $W_{i,j}$ëŠ” ê°ì†Œí•˜ëŠ” ë°©í–¥(ì´ë¯¸ ìŒìˆ˜ì—ì„œ ë” ì‘ì•„ì§)</li>
          <li>ì¦‰, $\frac{\partial L}{\partial S_{i,j}} &lt; 0$ì˜ ê²½ìš°ì˜ ìˆ˜ ë‘ ê°€ì§€ì— ëŒ€í•´ì„œ ëª¨ë‘ $W_{i,j}$ì˜ Magnitudeê°€ ì»¤ì§€ëŠ” ë°©í–¥</li>
          <li>$W_{i,j}$ê°€ 0ì—ì„œ ë©€ì–´ì§€ëŠ” ê²ƒ($\frac{\partial L}{\partial S_{i,j}} &lt; 0$) â†’ $S_{i,j}$ê°€ ì»¤ì§€ëŠ” ê²ƒ($S_{i,j} = S_{i,j} - \alpha \frac{\partial L}{\partial S_{i,j}}$)</li>
        </ul>
      </li>
      <li>ScoreëŠ” Weightê°€ fine tuning ë˜ë©´ì„œ í•¨ê»˜ í•™ìŠµ
        <ul>
          <li>ê¸°ì¡´ scoreë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì—ì„œëŠ” ì˜ëª»ëœ ì—ëŸ¬ì— ëŒ€í•´ì„œ ìˆ˜ì •í•  ê¸°íšŒê°€ ì—†ì—ˆë‹¤.</li>
          <li>ì—¬ê¸°ì—ì„œëŠ” í•™ìŠµë˜ëŠ” ê³¼ì •ì—ì„œ scoreë¥¼ ê³„ì‚°í•´ì„œ í•™ìŠµì´ ì§„í–‰ë˜ë©´ì„œ ì¼ì¢…ì˜ self-correctioní•˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="pruning-ì¶”ì²œ-ë…¼ë¬¸">Pruning ì¶”ì²œ ë…¼ë¬¸</h3>

<ul>
  <li>Encoderì˜ ê° ìœ„ì¹˜ë³„ë¡œ ì–´ë–¤ Knowledgeë¥¼ ê°€ì§€ê³  ìˆëŠ”ê°€?
    <ul>
      <li>On the Effect of Dropping Layers of Pre-trained Transformer Models</li>
      <li>Pretrained information(general linguistic knowledge)ëŠ” inputì— ê°€ê¹Œìš´ encoderë“¤ì— ì €ì¥ë˜ì–´ ìˆê³ , headì— ê°€ê¹Œìš´ ë¶€ë¶„ë“¤ì€ task specificí•œ ì •ë³´ë¥¼ ì €ì¥í•œë‹¤.</li>
      <li>pretraining ëª¨ë¸ì—ì„œ head ìª½ ë ˆì´ì–´ë¥¼ ì—†ì• ë„ fine-tuning ì‹œì˜ ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤.</li>
    </ul>
  </li>
  <li>Pretraining fine-tuning paradigmì´ ì™œ ì„±ëŠ¥, generalization capabilityê°€ ë” ì¢‹ì€ê°€?
    <ul>
      <li>Visualizing and Understanding the Effectiveness of BERT</li>
      <li>ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ fine-tuningí•˜ëŠ” ê³¼ì •ì—ì„œ loss surfaceê°€ í‰íƒ„í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì´ ë” ì˜ë˜ê³  generalization capabilityê°€ ë” ì¢‹ë‹¤.</li>
    </ul>
  </li>
</ul>

<h2 id="weight-factorizatino--weight-sharing">Weight Factorizatino &amp; Weight Sharing</h2>

<h3 id="albert-a-lite-bert-for-self-supervised-learning-of-language-representationsiclr-2020">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations(ICLR 2020)</h3>

<ul>
  <li>í° ëª¨ë¸ë“¤ì´ SOTA í¼í¬ë¨¼ìŠ¤ë¥¼ ì–»ê²Œ ë˜ëŠ”ë° ë©”ëª¨ë¦¬ í•œê³„ë•Œë¬¸ì— ì‚¬ì´ì¦ˆë¥¼ í¬ê²Œ í‚¤ìš°ëŠ” ê²ƒì— í•œê³„ê°€ ìˆë‹¤.</li>
  <li>ë˜í•œ, distributed trainingìœ¼ë¡œë¶€í„° Communication overheadê°€ ì¡´ì¬í•œë‹¤.</li>
  <li>ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” ì„¸ ê°€ì§€ ë°©ë²•
    <ul>
      <li>Cross-layer parameter sharing : íŒŒë¼ë¯¸í„° ìˆ˜ ê°ì†Œ</li>
      <li>Next Sentence Prediction â†’ Sentence Order Prediction : ì„±ëŠ¥ í–¥ìƒ</li>
      <li>Factored Embedding Parameterization : íŒŒë¼ë¯¸í„° ìˆ˜ ê°ì†Œ</li>
    </ul>
  </li>
  <li>ALBERTëŠ” ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ë” í° ëª¨ë¸ì„ ë§Œë“¤ìëŠ” ëª©ì </li>
</ul>

<h3 id="cross-layer-parameter-sharing">Cross-layer parameter sharing</h3>

<ul>
  <li>Weight sharingì€ input, output embeddingsì˜ L2 dist, Cosine similarityë¥¼ ê³„ì‚°í•´ë´¤ì„ ë•Œ network parametersë¥¼ stabilizingí•˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤.</li>
</ul>

<h3 id="sentence-ordering-objectives">Sentence Ordering Objectives</h3>

<ul>
  <li>Next Sentence Prediction(NSP)ê°€ ë„ˆë¬´ ì‰½ê¸° ë•Œë¬¸ì— Sentence Ordering Object(SOP)ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ í•¨</li>
  <li>NSP lossëŠ” SOP ìˆ˜í–‰ì— ë„ì›€ì„ ì£¼ì§€ ì•Šì§€ë§Œ SOP lossëŠ” NSP ìˆ˜í–‰ì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤.</li>
</ul>

<h3 id="factorized-embedding-parameterization">Factorized Embedding Parameterization</h3>

<ul>
  <li>WordPiece embeddings($E$)ëŠ” context-independent í‘œí˜„ì„ í•™ìŠµí•œë‹¤.</li>
  <li>Hidden layer embeddings($H$)ëŠ” context-dependent í‘œí˜„ì„ í•™ìŠµí•œë‹¤.</li>
  <li>BERTì—ì„œëŠ” $E$ì™€ $H$ì˜ dimensionì´ ê°™ë‹¤. â†’ BERTëŠ” context-dependent í‘œí˜„ì˜ í•™ìŠµì— íš¨ê³¼ì ì¸ êµ¬ì¡°.</li>
  <li>ê·¸ë ‡ë‹¤ë©´, ì™œ BERT ë ˆì´ì–´ê°€ context-independent representationì¸ WordPiece embeddingì— ë¬¶ì—¬ì•¼ í• ê¹Œ?</li>
  <li>WordPiece embedding ì‚¬ì´ì¦ˆ $E$ë¥¼ hidden layer ì‚¬ì´ì¦ˆ $H$ë¡œë¶€í„° í’€ì–´ë‚´ì.</li>
  <li>Untying dimensions by using decomposition
    <ul>
      <li>ì›ë˜ $O(V \times H)$ë¥¼ $O(V \times E + E \times H)$ë¡œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì„</li>
    </ul>
  </li>
</ul>

<h2 id="knowledge-distillation">Knowledge Distillation</h2>

<h3 id="ditilbert-a-distilled-version-of-bert-smaller-faster-cheaper-and-lighter">DitilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</h3>

<ul>
  <li>ì„¸ ê°€ì§€ lossë¥¼ ì œì•ˆ
    <ul>
      <li>ê¸°ì¡´ masked  language loss, distillation(Hinton) loss, cosine-similarity loss</li>
    </ul>
  </li>
  <li>Triple loss = MLM($L_{mlm}$)+Hinton($L_{ce}$)+Cosine embedding($L_{cos}$)
    <ul>
      <li>Masked Language Modeling Loss : CE-loss</li>
      <li>Distillation(Hinton) Loss : KL div of teacher, student softmax prob with temperature</li>
      <li>Cosine embedding loss : between teacher, student hidden state vectors</li>
    </ul>
  </li>
  <li>Student architecture &amp; initialization(paper)
    <ul>
      <li>token-type embedding, pooler ì œê±°</li>
      <li>ë ˆì´ì–´ ê°œìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ê³  hidden size dimensionì€ ê·¸ëŒ€ë¡œ ë‘ì—ˆìŒ(dimensionì„ ì¤„ì´ëŠ” ê²ƒì´ computationì—ì„œ í° íš¨ê³¼ê°€ ì—†ì—ˆë‹¤.)</li>
      <li>ì´ˆê¸°í™”ëŠ” studentì˜ ì²« ë²ˆì§¸ ë ˆì´ì–´ëŠ” teacherì˜ ë‘ ë²ˆì§¸, studentì˜ ë‘ ë²ˆì§¸ ë ˆì´ì–´ëŠ” teacherì˜ ë„¤ ë²ˆì§¸ ë ˆì´ì–´ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ í•œë‹¤.</li>
    </ul>
  </li>
</ul>

<h3 id="tinybert-distilling-bert-for-natural-language-understanding">TinyBERT: Distilling BERT for Natural Language Understanding</h3>

<ul>
  <li>Transformer distillation method: 3 types of loss
    <ul>
      <li>From the output embedding layer</li>
      <li>From the hidden states and attention matrices</li>
      <li>From the logits output by the prediction layer</li>
      <li>Teacher $N$ layers, student $M$ layers â†’ Nê°œì—ì„œ Mê°œë¥¼ ê³ ë¥¸ë‹¤.</li>
      <li>Teacherì™€ studentì˜ ë ˆì´ì–´ ë§µí•‘ì„ í•˜ëŠ” $n = g(m)$ì´ë¼ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤.(ë…¼ë¬¸ì—ì„œëŠ” g(m) = 3 *mìœ¼ë¡œ ì •ì˜í–ˆê³  1ë²ˆ ë ˆì´ì–´ë¥¼ 3ë²ˆ ë ˆì´ì–´ë¡œ ë§µí•‘í•œ ì˜ë¯¸ì´ë‹¤.)</li>
      <li>$L_{model} = \sum_{x \in \chi} \sum_{m=0}^{M+1} \lambda_m L_{layer}(f_m^S(x), f_{g(x)}^T(x))$
        <ul>
          <li>í•™ìŠµë°ì´í„° $x$ì— ëŒ€í•˜ì—¬ Student layer $m$ë§ˆë‹¤ ê° ë ˆì´ì–´ì˜ ë¡œìŠ¤ë¥¼ êµ¬í•˜ê³  ê° ë ˆì´ì–´ì˜ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ $\lambda$ë¥¼ ê³±í•œ ê²ƒì˜ í•©</li>
          <li>Layer lossëŠ” $m$ë²ˆì§¸ studentì˜ íŠ¹ì • output(Attn, hidden, logits), $g(m)$ë²ˆì§¸ teacherì˜ íŠ¹ì • output(Attn, hidden, logit)</li>
          <li>$m = 0$ ì¼ë•Œ, $L_{layer}$ëŠ” $L_{embd}$, $0&lt;mâ‰¤M$ ì¼ë•Œ, $L_{layer}$ëŠ” $L_{hidden}+L_{attn}$, $m = M+1$ ì¼ë•Œ, $L_{pred}$</li>
        </ul>
      </li>
      <li>Transformer-layer Distillation(Attention based)
        <ul>
          <li>$L_{attn} = \frac{1}{h} \sum_{i=1}^{h} MSE(A_i^S, A_i^T)$</li>
          <li>AëŠ” teacher ë˜ëŠ” studentì˜ attention matrixì¸ë° ì´ ë…¼ë¬¸ì—ì„œëŠ” unnormalized attention matrixë¡œ ì„¤ì •í•¨
            <ul>
              <li>unnormalizedê°€ ë¹ ë¥´ê³  ì„±ëŠ¥ì´ ë” ì¢‹ì•˜ìŒ</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Transformer-layer Distillation(Hidden state)
        <ul>
          <li>$L_{hidn} = MSE(H^SW_h,H^T)$</li>
          <li>$H^T$, $H^S$ : teacher hidden state, student hidden state</li>
          <li>$W_h$ : Learnable linear transformation(Studentì˜ hidden stateë¥¼ teacherì˜ dimensionë§Œí¼ í‚¤ì›Œì„œ MSEë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•¨)</li>
        </ul>
      </li>
      <li>Embedding-layer Distillation loss
        <ul>
          <li>$L_{embd} = MSE(E^SW_e,E^T)$</li>
          <li>$W_e$ëŠ” $W_h$ì™€ ê°™ì€ ì—­í• ì„ í•œë‹¤.</li>
        </ul>
      </li>
      <li>Prediction-layer Distillation loss
        <ul>
          <li>$L_{pred} = CE(z^T/t, z^S/t)$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Two stage learning framework
    <ul>
      <li>General Distillation : Large-scale Text Corpus â†’ General TinyBERT</li>
      <li>Task-specific Distillation : General TinyBERT â†’ Fine-tuned TinyBERT</li>
    </ul>
  </li>
</ul>

<h3 id="ê¸°íƒ€-ë…¼ë¬¸-ì¶”ì²œ">ê¸°íƒ€ ë…¼ë¬¸ ì¶”ì²œ</h3>

<ul>
  <li>MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices</li>
  <li>Exploring the Boundaries of Low-Resource BERT Distillation</li>
  <li>AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search</li>
</ul>

<h2 id="quantization">Quantization</h2>

<ul>
  <li>ì¥ì  : low memory footprint, inference speed(low precision operation ì¦ê°€ ì¶”ì„¸)</li>
  <li>ì£¼ë¡œ Quantization ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” accuracy dropì„ ì¤„ì´ëŠ” ë°©í–¥ì— ëŒ€í•œ ì—°êµ¬</li>
  <li>QAT(Quantizatino Aware Training), Quantization range ê³„ì‚° ë°©ë²• ì œì•ˆ ë“±</li>
</ul>

<h3 id="q-bert-hessian-based-ultra-low-precision-quantization-of-bert">Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT</h3>

<ul>
  <li>ë¯¼ê°ë„ê°€ ë†’ì€ layerëŠ” í° precision, ë‚®ì€ layerëŠ” ì‘ì€ precision</li>
  <li>group-wise quantizationì´ë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²• ì œì‹œ</li>
  <li>BERTì˜ bottleneck ì¡°ì‚¬</li>
  <li>Hessian spectrum(eigenvalues)
    <ul>
      <li>Higher Hessian spectrumì„ ê°€ì§€ëŠ” NN layerì—ì„œì˜ íŒŒë¼ë¯¸í„°ë“¤ì€ ë” ë¯¼ê°í•¨(Hessianì˜ top eigenvaluesì˜ í¬ê¸°ê°€ í•´ë‹¹ ë ˆì´ì–´ì˜ ë¯¼ê°ë„ì™€ ì—°ê´€ì´ ìˆë‹¤)</li>
      <li>Hessian spectrum ê³„ì‚°ì˜ ë³µì¡ì„± â†’ power iteration methodë¡œ í•´ê²°(Large sparse matrixì—ì„œì˜ ë¹ ë¥¸ ìˆ˜ë ´)</li>
      <li>ê°™ì€ ë°ì´í„°ì…‹ì´ë”ë¼ë„ Hessian spectrumì˜ varì´ ë§¤ìš° í¼ â†’ mean, stdë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ë¯¼ê°ë„ë¥¼ ì •ë ¬</li>
    </ul>
  </li>
  <li>Group-wise Quantization method
    <ul>
      <li>key, query, value, output ëª¨ë‘ ê°™ì€ Quantization rangeë¡œ ì •í–ˆì—ˆëŠ”ë° ì´ rangeê°€ ì»¤ë²„í•´ì•¼ í•˜ëŠ” matricesì˜ ë‹¨ìœ„ê°€ ë„ˆë¬´ í¬ë‹¤.</li>
      <li>query, key, valueì˜ ë¶„í¬ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆëŠ”ë° ì´ë•Œ ì—ëŸ¬ê°€ ì»¤ì§„ë‹¤.</li>
      <li>ë”°ë¼ì„œ, Multi-head ë³„ë¡œ ë”°ë¡œë”°ë¡œ quantizationì„ ì§„í–‰í•¨</li>
    </ul>
  </li>
</ul>

<h2 id="ì •ë¦¬">ì •ë¦¬</h2>

<h3 id="pruning">Pruning</h3>

<ul>
  <li>Structured : ëª¨ë¸ ì‚¬ì´ì¦ˆ ê°ì†Œ, ì†ë„ í–¥ìƒ, ì„±ëŠ¥ ë“œë</li>
  <li>Unstructured : ëª¨ë¸ ì‚¬ì´ì¦ˆ ê°ì†Œ, ì ì€ ì„±ëŠ¥ ë“œë, ì†ë„ í–¥ìƒ X(ë³„ë„ ì²˜ë¦¬ê°€ ì—†ëŠ” ê²½ìš°)</li>
</ul>

<h3 id="kd">KD</h3>

<ul>
  <li>íŒŒë¼ë¯¸í„° ìˆ˜ ê°ì†Œ, ë‹¤ì–‘í•œ range ëª¨ë¸, í° ì†ë„ í–¥ìƒ ì—¬ì§€(LSTM ë“± ë‹¤ë¥¸ êµ¬ì¡°ë¡œ distillation), ë¹„êµì  ë³µì¡í•œ í•™ìŠµ êµ¬ì„±, code maintain</li>
</ul>

<h3 id="weight">Weight</h3>

<ul>
  <li>Matrix decompostion(factorization) : ëª¨ë¸ ì‚¬ì´ì¦ˆ ê°ì†Œ, ì ì€ ì„±ëŠ¥ ê°ì†Œ, ì†ë„ í–¥ìƒ(ì£¼ë¡œ CPU), ì†ë„ ë³€í™” ë¯¸ë¯¸(GPU)</li>
  <li>Param(weight) sharing : ëª¨ë¸ ì‚¬ì´ì¦ˆ ê°ì†Œ, í•™ìŠµ ê´€ë ¨ ì´ì , ì ì€ ì†ë„ ê°œì„ </li>
</ul>

<h3 id="quantization-1">Quantization</h3>

<ul>
  <li>ëª¨ë¸ ì‚¬ì´ì¦ˆ ê°ì†Œ íƒì›”, ì ì€ ì„±ëŠ¥ í•˜ë½, ì†ë„ í–¥ìƒ ë¶ˆíˆ¬ëª…(ì§€ì› ì¦ê°€ ì¶”ì„¸)</li>
</ul>
:ET