I"<blockquote>
  <p>모기업 코딩테스트에 파이썬 기본 라이브러리로만 MLP를 구현하는 문제가 나왔던 적이 있습니다. 당시에 학습이 되지 않아 코딩테스트에서 떨어졌었고 구현하지 못했던 것이 계속 생각나서 구현해봤습니다.</p>
</blockquote>

<h2 id="계획">계획</h2>
<p>데이터셋을 MNIST로 잡고 MLP를 구현하고자 했습니다. 코딩테스트때도 입력으로 MNIST와 비슷한 값이 들어왔었기 때문입니다.</p>

<p>레이어는 총 3개로 input -&gt; (Linear -&gt; Activation) -&gt; (Linear -&gt; Activation) -&gt; (Linear -&gt; Softmax) -&gt; output 으로 생각하고 각 모듈 구현을 시작했습니다.</p>

<h2 id="forward--backward-propagation">Forward &amp; Backward Propagation</h2>
<p>모델이 학습을 하기 위해서는 역전파(backpropagation)가 진행되어야 합니다. 각 모듈들의 미분값을 출력하고 chain rule에 의해 값들을 곱해가면서 Linear 레이어의 가중치와 바이어스를 업데이트해야 합니다.</p>

<p>먼저, 아래처럼 구성된 모델이 있다고 가정하고 순전파(Feedforward)때 계산되는 과정을 살펴보겠습니다.</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model(
  (layer1): Sequential(
    (0): Linear(in_features=784, out_features=256, bias=True)
    (1): Sigmoid()
  )
  (layer2): Sequential(
    (0): Linear(in_features=256, out_features=10, bias=True)
    (1): Softmax()
  )
)
</code></pre></div></div>
<p>$y_1 = \sigma_{1}(z_1) = \sigma_1(w_1x + b_1), \sigma_{1} = \text{sigmoid}$</p>

<p>$\hat{y} = \sigma_{2}(z_2) = \sigma_{2}(w_2y_1+b_2), \sigma_{2} = \text{softmax}$</p>

<p>$L_{\text{MSE}} = \sum(\hat{y}-{y})^2$</p>

<p>이제, 위 식을 거꾸로 돌려가면서 역전파를 진행합니다.</p>

<p>$W_2$에 대해 편미분된 값을 먼저 구하면 다음과 같이 진행됩니다.</p>

<p>$\frac{\partial L}{ \partial w_2}=\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{dz_2} \cdot \frac{\partial z_2}{\partial w_2}$</p>

<p>${\partial \hat{y} \over \partial z_2} = \sigma_2(z_2)(\delta_{ij} - \sigma_2(z_2)), \ \delta_{ij}=
    \begin{cases}
        1, &amp; i=j \\ 0, &amp; i \ne j
    \end{cases}
$</p>

<p>$\frac{\partial L}{\partial w_2} = \frac{2}{m}(\hat{y}-y) \cdot \sigma_2(z_2)(\delta_{ij} - \sigma_2(z_2)) \cdot y_1$</p>

<p>$W_1$에 대해 편미분된 값을 구하면 다음과 같이 진행됩니다.</p>

<p>$\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z_2} \cdot \frac{\partial z_2}{\partial y_1} \cdot \frac{\partial y_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_1}$</p>

<p>${\partial y_1 \over \partial z_1} = \sigma_1(z_1)(1 - \sigma_1(z_1)) = y_1(1 - y_1)$</p>

<p>$\frac{\partial L}{\partial w_1}= \frac{2}{m}(\hat{y}-y) \cdot \sigma_2(z_2)(\delta_{ij} - \sigma_2(z_2)) \cdot w_2 \cdot y_1 \cdot (1 - y_1) \cdot x$</p>

<p>gradient의 계산에서 마지막 곱에는 <strong>입력값</strong>에 대해 dot product하고 입력 레이어 방향으로 <strong>이전 레이어의 weight</strong>를 dot product해야 합니다. 따라서 Linear 레이어는 입력값을 저장해야 backward 계산에서 사용할 수 있습니다.
<script src="https://gist.github.com/emeraldgoose/5bbdab6c658bc73da63bbc694bcf5f2a.js"></script></p>

<p>또한, sigmoid를 통과한 출력값들은 역전파때 <strong>element-wise product</strong>를 진행해야 합니다. 활성화함수는 입력값 각각에 대해 함수를 통과시키므로 역전파때도 똑같이 진행되어야 하기 때문입니다. softmax는 element-wise independent하지 않아 element-wise product를 수행해서는 안됩니다.</p>

<p>이 역전파를 구현하기 위해 레이어마다 backward()함수를 추가하여 gradient를 계산하고 Optimizer를 이용하여 weight와 bias를 학습할 수 있습니다.<br />
<script src="https://gist.github.com/emeraldgoose/f256205e7bed257c9b1c5ecbcfc409e5.js"></script></p>

<h2 id="결과">결과</h2>
<p>MNIST 5000장을 훈련데이터로 사용하고 1000장을 테스트데이터로 사용했습니다.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1k18xXPI4qMx31qgSTajBwQ6NwjycTpkr" alt="" width="400" />
<img src="https://drive.google.com/uc?export=view&amp;id=1Pzta5dtXVxduFsIgHGaSqsHOtKsh6jSh" alt="" width="400" /></p>

<p>10 에포크에도 loss가 잘 떨어지고 Accuracy도 잘 증가하는 것을 볼 수 있습니다.</p>

<p>벡터 계산이나 다른 수식 계산에 도움이 되는 numpy 없이 구현하려고 하니 코드에서 실수를 많이 했습니다. 계산 결과를 확인하기 위해 torch나 numpy에 있는 똑같은 함수를 불러오고 저의 코드를 불러와 계산결과가 맞는지 계속 확인했습니다.</p>

<p>torch로 모델을 학습하는 방법과 최대한 유사하게 작성할 수 있도록 구현하고자 했습니다. torch에서는 autograd 기능과 텐서를 사용할 수 있어 사용자가 쉽게 모델을 학습할 수 있었습니다.(라이브러리 개발자분들 존경합니다.) 하지만 직접 구현하려면 역전파를 위해 레이어마다 미분을 진행해줘야 하는 과정이 추가되어 생각보다 구현이 어려웠습니다.</p>

<h3 id="문제점">문제점</h3>
<p>가장 큰 문제점은 e^x 함수의 Overflow 현상입니다. 입력값이 음수이면서 큰 수일 때 softmax와 sigmoid 계산에서 overflow 현상이 일어났습니다.</p>

<h2 id="앞으로">앞으로</h2>
<p>MLP에 사용되는 레이어들만 구현되었지만 CNN이나 RNN을 사용할 수 있도록 레이어들을 추가할 생각입니다. 구현하기 위해서 수식이나 역전파 과정들을 찾아보니 난이도가 있어 보여서 언제 추가할 수 있을지는 잘 모르겠습니다…</p>

<h2 id="코드">코드</h2>
<blockquote>
  <p>구현된 코드는 깃허브에 있습니다.<br />
<a href="https://github.com/emeraldgoose/hcrot">https://github.com/emeraldgoose/hcrot</a></p>
</blockquote>

<h2 id="reference">Reference</h2>
<ul>
  <li><a href="http://taewan.kim/post/sigmoid_diff/">http://taewan.kim/post/sigmoid_diff/</a></li>
  <li><a href="https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/">https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/</a></li>
  <li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">https://pytorch.org/docs/stable/generated/torch.nn.Linear.html</a></li>
  <li><a href="https://velog.io/@gjtang/Softmax-with-Loss-%EA%B3%84%EC%B8%B5-%EA%B3%84%EC%82%B0%EA%B7%B8%EB%9E%98%ED%94%84">https://velog.io/@gjtang/Softmax-with-Loss-%EA%B3%84%EC%B8%B5-%EA%B3%84%EC%82%B0%EA%B7%B8%EB%9E%98%ED%94%84</a></li>
  <li>[https://aew61.github.io/blog/artificial_neural_networks/1_background/1.b_activation_functions_and_derivatives.html](https://aew61.github.io/blog/artificial_neural_networks/1_background/1.b_activation_functions_and_derivatives.html</li>
</ul>
:ET