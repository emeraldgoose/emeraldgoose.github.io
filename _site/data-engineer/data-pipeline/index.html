<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>데이터 파이프라인 구축해보기 - gooooooooooose</title>
<meta name="description" content="Motivation    빅데이터를 지탱하는 기술을 읽다가 데이터 엔지니어링에 사용되는 플랫폼들을 전체 파이프라인으로 구축해보고 싶어서 이 사이드 프로젝트를 진행하게 되었습니다.">


  <meta name="author" content="goooose">
  
  <meta property="article:author" content="goooose">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="gooooooooooose">
<meta property="og:title" content="데이터 파이프라인 구축해보기">
<meta property="og:url" content="http://localhost:4000/data-engineer/data-pipeline/">


  <meta property="og:description" content="Motivation    빅데이터를 지탱하는 기술을 읽다가 데이터 엔지니어링에 사용되는 플랫폼들을 전체 파이프라인으로 구축해보고 싶어서 이 사이드 프로젝트를 진행하게 되었습니다.">







  <meta property="article:published_time" content="2022-12-02T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/data-engineer/data-pipeline/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "emeraldgoose",
      "url": "http://localhost:4000/"
    
  }
</script>


  <meta name="google-site-verification" content="googleb34ce5276ba6573e" />





  <meta name="naver-site-verification" content="naver93cdc79a5629ab3736d7cf8ff7b51d80">


<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="gooooooooooose Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          gooooooooooose
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo.png" alt="goooose" itemprop="image" class="u-photo" width="110px" height="110px">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">goooose</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Dev blog.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:smk6221@naver.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/emeraldgoose" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="데이터 파이프라인 구축해보기">
    <meta itemprop="description" content="Motivation  빅데이터를 지탱하는 기술을 읽다가 데이터 엔지니어링에 사용되는 플랫폼들을 전체 파이프라인으로 구축해보고 싶어서이 사이드 프로젝트를 진행하게 되었습니다.">
    <meta itemprop="datePublished" content="2022-12-02T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">데이터 파이프라인 구축해보기
</h1>
          
<!--
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  0 minute read
</p>
            devinlife comments :
                싱글 페이지(포스트)에 제목 밑에 Updated 시간 표기
                기존에는 read_time이 표기. read_time -> date 변경
-->
            <p class="page__date"><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2022-12-02T00:00:00+09:00">December 02, 2022</time></p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h3 id="motivation">Motivation</h3>
<blockquote>
  <p><strong>빅데이터를 지탱하는 기술</strong>을 읽다가 데이터 엔지니어링에 사용되는 플랫폼들을 전체 파이프라인으로 구축해보고 싶어서
이 사이드 프로젝트를 진행하게 되었습니다.</p>
</blockquote>

<h3 id="data">Data</h3>
<p>먼저, 수집할 데이터는 nginx로부터 나오는 로그를 생각했습니다. 하지만 많은 양의 로그를 생산하려면 nginx로부터 나오게 하기는 어려웠습니다. 그래서 python 코드로 비슷한 nginx 로그를 생성하고 /var/log/httpd/access_log/*.log에 logging 모듈로 기록하는 방법으로 로그를 생산했습니다.</p>

<p>생산되는 로그는 다음과 같습니다.<br />
<code class="language-plaintext highlighter-rouge">206.176.215.237 - - [02/Dec/2022:18:57:34 +0900] "GET /api/items HTTP/1.1" 200 3456 477 "https://www.dummmmmy.com" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Mobile/15E148 Safari/604.1"</code></p>

<h3 id="producerfilebeat">Producer(FileBeat)</h3>
<p>서버 접속 기록을 로깅하는 서버에서 로그를 외부로 보내주는 무언가 필요했습니다. 로그 파일을 ELK 스택의 logstash로 읽는 방법이 있지만 저는 접속 서버에서 logstash를 사용하는 것은 자원 부담이 있다고 생각했습니다. 그래서 losgtash를 밖으로 빼내 수집 서버를 따로 두고 logstash와 잘 맞는 FileBeat를 서버에 동작시켰습니다. FileBeat는 /var/log/httpd/access_log/*.log 파일을 읽어 Logstash 서버로 추가된 로그를 전달하는 역할을 합니다.</p>

<p>FileBeat는 Logstash의 무겁다는 단점을 보완하여 개발된 로그 수집기입니다. 로그파일의 경로를 설정하면 offset을 기억해 추가되는 로그를 외부로 전달하는 역할을 합니다. 비슷한 수집기로 FluentBit가 있고 Fluentd의 무겁다는 단점을 보완한 수집기입니다.</p>

<h3 id="logstash">Logstash</h3>
<p>Logstash는 전달받은 로그를 Elasticsearch나 다른 곳으로 전달하는 역할을 합니다. Logstash를 사용한 이유는 람다 아키텍처같은 파이프라인을 생각하고 있기 때문입니다.</p>

<p>람다 아키텍처처럼 실시간으로 수집되어 보여주는 뷰와 배치 처리되어 보여주는 뷰를 제공하는 구조인데 logstash는 여러 경로의 Output을 지원하고 있기 때문에 적합하다고 생각했습니다. 저는 Logstash 서버에 Elasticsearch와 Redis를 연결했습니다.</p>

<p>logstash는 *.conf 파일을 사용하여 사용자가 원하는 데이터 가공이 가능합니다. 저는 각 항목과 ip의 위치주소, User Agent 정보를 파싱하는 필터를 넣어 파싱할 수 있었습니다. 로그를 파싱할때는 grok을 사용했고 다음과 같은 설정값을 사용했습니다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>filter {
  grok {
    match =&gt; {
      "message" =&gt; "%{IPORHOST:remote_addr} - %{USER:remote_user} \[%{HTTPDATE:time_local}\] \"%{WORD:method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})\" %{NUMBER:status} (?:%{NUMBER:body_bytes_sent}|-) (?:%{NUMBER:response_time}|-) \"%{GREEDYDATA:referrer}\" \"%{GREEDYDATA:UA}\""
    }
  }
  geoip {
    source =&gt; "remote_addr"
    target =&gt; "clientgeoip"
  }
  useragent {
    source =&gt; "UA"
  }
}
</code></pre></div></div>

<h3 id="elasticsearch-kibana">Elasticsearch, Kibana</h3>
<p>Elasticsearch는 logstash로부터 전달받은 데이터를 저장하는 DB역할을 합니다. Kibana는 Elasticsearch의 데이터를 보여주는 대시보드 역할을 합니다. Dockerfile을 따로 작성하지 않았는데 Elasticsearch와 Kibana까지 도커로 올리면 맥북이 감당하지 못할 것 같아서 서버를 빌려주는 플랫폼을 알아보게 되었습니다.</p>

<p>처음에는 GCP 프리티어를 생각했다가 <a href="https://ide.goorm.io">구름</a>이 생각나서 이곳에 설치했습니다. 구름ide가 빌려주는 서버 자원이 좋지는 않지만 항상 켜둘 수 있고 *.run.goorm.io라는 도메인도 제공되어 사용하게 되었습니다. 아래 주소로 Kibana에 접속할 수 있지만 동작이 느리므로 조금 기다려주세요.</p>
<ul>
  <li><a href="https://dashboard-kibana.run.goorm.io">dashboard-kibana.run.goorm.io</a></li>
  <li>만약, logstash로 구름에 있는 elasticsearch로 연결하려면 포트포워딩 세팅을 하고 port는 443으로 접근해야 합니다.</li>
</ul>

<h3 id="hdfs">HDFS</h3>
<p>로그를 수집하여 배치처리하려면 먼저 저장될 공간이 필요했습니다. 보통 AWS S3같은 오브젝트 스토리지를 사용하는 것 같은데 오픈소스로 Apache Ozone이 있지만 아직 잘 사용되지는 않은 것 같아서 하둡으로 결정했습니다. logstash로부터 온 데이터들은 먼저 hdfs에 저장되고 배치처리를 통해 RDB로 저장되는 과정을 생각했습니다.</p>

<p>하둡이 설치되는 도커를 더 늘리기는 어려워서 단일 노드를 사용하지만 설정은 분산 설정이 되어있는 모드인 Pseudo Distribute 모드로 사용했습니다.</p>

<p>Logstash는 webhdfs를 통해 데이터를 전달할 수 있는데 webhdfs를 사용하기 위해 /etc/hosts를 수정해야 하는 점이 있었습니다. Pseudo Distribute 모드를 사용하게 되면 Data Node의 주소가 컨테이너 id로 적용되어 /etc/hosts의 <code class="language-plaintext highlighter-rouge">컨테이너ip localhost</code>를 추가해야 합니다. 저는 hosts 파일을 수정하는 것이 싫어서 webhdfs를 사용하지 않는 방법으로 hdfs에 데이터를 적재했습니다.</p>

<h3 id="redis">Redis</h3>
<p>Logstash로부터 하루 간격의 데이터를 받아 hdfs로 한번에 적재하기 위해 logstash와 HDFS사이에 임시 데이터 저장소가 필요했습니다. kafka는 현업에서 자주 쓰이는 플랫폼이지만 zookeeper가 추가로 설치되어야 하므로 도커를 추가로 올리는데 부담되어 제외했습니다. 그래서 in-memory db로 사용되는 것 중 redis를 선택하게 되었습니다.</p>

<p>Logstash는 redis로 보낼때 key를 지정해야합니다. key는 그날 날짜로 지정하여 연속적으로 데이터를 redis로 전달하여 하루 간격의 배치 처리 스크립트가 실행될 때 어제 날짜로 key 접근하여 데이터를 모을 수 있었습니다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>output {
    redis {
        host =&gt; ["redis"]
        port =&gt; 6379
        data_type =&gt; "list"
        key =&gt; "%{+YYYYMMdd}"
    }
}
</code></pre></div></div>

<h3 id="spark">Spark</h3>
<p>Spark는 하둡이 설치된 도커에 같이 설치했습니다. 처음에는 하둡의 Yarn의 관리를 받게 하려고 설치했지만 단일 노드로 돌리느라 local과 yarn의 차이가 잘 안나서 pyspark를 그냥 local로 돌렸습니다. 따라서 spark를 모두 활용하지는 못했습니다.</p>

<p>redis에 저장된 데이터를 가져오고 hdfs에 적재하는 처리를 하는 pyspark 스크립트를 실행하는 역할을 합니다.</p>

<h3 id="airflow">Airflow</h3>
<p>배치 스크립트를 실행하도록 Airflow를 사용했습니다. 하둡 도커에서 spark-submit을 실행하는 커맨드를 사용할 수 있도록 SSHOperator가 포함된 태스크와 hdfs에서 DB로 적재하는 배치처리하는 태스크를 구성했습니다. 데이터 양도 적고 빠르게 확인하기 위해 모두 @daily로 사용하여 하루 간격으로 실행하도록 했습니다.</p>

<p>spark-submit을 사용하는 스크립트는 다음과 같습니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">pendulum</span>
<span class="kn">from</span> <span class="nn">airflow.decorators</span> <span class="kn">import</span> <span class="n">dag</span>
<span class="kn">from</span> <span class="nn">airflow.providers.ssh.operators.ssh</span> <span class="kn">import</span> <span class="n">SSHOperator</span>
<span class="kn">from</span> <span class="nn">airflow.providers.ssh.hooks.ssh</span> <span class="kn">import</span> <span class="n">SSHHook</span>

<span class="n">kst</span> <span class="o">=</span> <span class="n">pendulum</span><span class="p">.</span><span class="n">timezone</span><span class="p">(</span><span class="s">'Asia/Seoul'</span><span class="p">)</span>
<span class="n">now</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d'</span><span class="p">)</span>
<span class="n">one_day_ago</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">(</span><span class="n">tz</span><span class="o">=</span><span class="n">kst</span><span class="p">)</span> <span class="o">-</span> <span class="n">datetime</span><span class="p">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="o">@</span><span class="n">dag</span><span class="p">(</span><span class="n">dag_id</span><span class="o">=</span><span class="s">'logs_redis_to_hdfs'</span><span class="p">,</span> <span class="n">schedule_interval</span><span class="o">=</span><span class="s">'@daily'</span><span class="p">,</span> <span class="n">start_date</span><span class="o">=</span><span class="n">one_day_ago</span><span class="p">,</span> <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s">'batch'</span><span class="p">,</span><span class="s">'redis'</span><span class="p">,</span><span class="s">'hdfs'</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">parquet_to_hdfs_from_logstash</span><span class="p">():</span>
    <span class="n">hook</span> <span class="o">=</span> <span class="n">SSHHook</span><span class="p">(</span>
        <span class="n">remote_host</span><span class="o">=</span><span class="s">'hadoop-spark'</span><span class="p">,</span>
        <span class="n">username</span><span class="o">=</span><span class="s">'root'</span><span class="p">,</span>
        <span class="n">key_file</span><span class="o">=</span><span class="s">'/root/.ssh/id_rsa.pub'</span>
    <span class="p">)</span>

    <span class="n">run_script</span> <span class="o">=</span> <span class="n">SSHOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s">'run_script'</span><span class="p">,</span>
        <span class="n">ssh_hook</span><span class="o">=</span><span class="n">hook</span><span class="p">,</span>
        <span class="n">command</span><span class="o">=</span><span class="sa">f</span><span class="s">'/spark/bin/spark-submit /spark/logs_redis_to_hdfs.py --start_date </span><span class="si">{</span><span class="n">now</span><span class="si">}</span><span class="s">'</span><span class="p">,</span>
    <span class="p">)</span>
    
    <span class="n">run_script</span>
    
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">parquet_to_hdfs_from_logstash</span><span class="p">()</span>
</code></pre></div></div>
<p>ssh로 하둡이 설치된 도커로 접속하여 SSHOperator로 command를 실행하는 DAG입니다. ssh로 접속하기 위해 airflow 도커와 하둡 도커의 <code class="language-plaintext highlighter-rouge">~/.ssh/</code> 폴더를 공유시켜 하둡에서 생성된 key 파일을 airflow에서 사용할 수 있게 했습니다.</p>

<p>RDB로 적재하는 스크립트는 다음과 같습니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">pyspark</span>
<span class="kn">import</span> <span class="nn">sqlalchemy</span>
<span class="kn">import</span> <span class="nn">pendulum</span>
<span class="kn">from</span> <span class="nn">airflow.decorators</span> <span class="kn">import</span> <span class="n">dag</span><span class="p">,</span> <span class="n">task</span>

<span class="n">kst</span> <span class="o">=</span> <span class="n">pendulum</span><span class="p">.</span><span class="n">timezone</span><span class="p">(</span><span class="s">"Asia/Seoul"</span><span class="p">)</span>
<span class="n">yesterday</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">(</span><span class="n">tz</span><span class="o">=</span><span class="n">kst</span><span class="p">)</span> <span class="o">-</span> <span class="n">datetime</span><span class="p">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="o">@</span><span class="n">dag</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span><span class="s">'store_to_postgres'</span><span class="p">,</span> 
    <span class="n">schedule_interval</span><span class="o">=</span><span class="s">'@daily'</span><span class="p">,</span> 
    <span class="n">start_date</span><span class="o">=</span><span class="n">yesterday</span><span class="p">,</span> 
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s">'batch'</span><span class="p">,</span><span class="s">'hdfs'</span><span class="p">,</span><span class="s">'rdb'</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">batch_to_rdb</span><span class="p">():</span>
    <span class="o">@</span><span class="n">task</span>
    <span class="k">def</span> <span class="nf">get_logs_from_hdfs</span><span class="p">():</span>
        <span class="n">sc</span> <span class="o">=</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">SparkContext</span><span class="p">(</span><span class="n">master</span><span class="o">=</span><span class="s">'local'</span><span class="p">,</span> <span class="n">conf</span><span class="o">=</span><span class="n">pyspark</span><span class="p">.</span><span class="n">SparkConf</span><span class="p">())</span>
        <span class="n">sqlContext</span> <span class="o">=</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">sql</span><span class="p">.</span><span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">parquet</span><span class="p">(</span>
            <span class="sa">f</span><span class="s">'hdfs://hadoop-spark:9000/warehouse/</span><span class="si">{</span><span class="n">yesterday</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d"</span><span class="p">)</span><span class="si">}</span><span class="s">.parquet'</span>
            <span class="p">)</span>
        <span class="n">df</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">'data.parquet'</span><span class="p">)</span>
    
    <span class="o">@</span><span class="n">task</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">():</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s">'data.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span>
            <span class="n">columns</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s">"clientgeoip.geo.country_name"</span> <span class="p">:</span> <span class="s">"country_name"</span><span class="p">,</span>
                <span class="s">"clientgeoip.geo.region_name"</span> <span class="p">:</span> <span class="s">"region_name"</span><span class="p">,</span>
                <span class="s">"clientgeoip.geo.city_name"</span> <span class="p">:</span> <span class="s">"city_name"</span><span class="p">,</span>
                <span class="s">"user_agent.name"</span> <span class="p">:</span> <span class="s">"browser"</span><span class="p">,</span>
                <span class="s">"user_agent.device.name"</span> <span class="p">:</span> <span class="s">"device"</span><span class="p">,</span>
                <span class="s">"user_agent.os.name"</span> <span class="p">:</span> <span class="s">"os_name"</span><span class="p">,</span>
                <span class="s">"user_agent.os.version"</span> <span class="p">:</span> <span class="s">"os_version"</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="n">df</span><span class="p">[</span><span class="s">'timestamp'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'time_local'</span><span class="p">],</span> <span class="nb">format</span><span class="o">=</span><span class="s">'%d/%b/%Y:%H:%M:%S +0900'</span><span class="p">).</span><span class="n">dt</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%dT%H:%M:%S'</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span>
            <span class="p">[</span><span class="s">'timestamp'</span><span class="p">,</span><span class="s">'UA'</span><span class="p">,</span><span class="s">'body_bytes_sent'</span><span class="p">,</span><span class="s">'country_name'</span><span class="p">,</span><span class="s">'httpversion'</span><span class="p">,</span><span class="s">'message'</span><span class="p">,</span><span class="s">'method'</span><span class="p">,</span>
            <span class="s">'referrer'</span><span class="p">,</span><span class="s">'remote_addr'</span><span class="p">,</span><span class="s">'remote_user'</span><span class="p">,</span><span class="s">'request'</span><span class="p">,</span><span class="s">'response_time'</span><span class="p">,</span><span class="s">'status'</span><span class="p">,</span><span class="s">'device'</span><span class="p">,</span>
            <span class="s">'browser'</span><span class="p">,</span><span class="s">'os_name'</span><span class="p">,</span><span class="s">'os_version'</span><span class="p">,</span><span class="s">'city_name'</span><span class="p">,</span><span class="s">'region_name'</span><span class="p">]</span>
        <span class="p">]</span>

        <span class="n">df_yesterday</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span>
            <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'timestamp'</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">yesterday</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d'</span><span class="p">)</span><span class="o">+</span><span class="s">'T0:0:0'</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'timestamp'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">yesterday</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d'</span><span class="p">)</span><span class="o">+</span><span class="s">'T23:59:59'</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">df_today</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span>
            <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'timestamp'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">yesterday</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d'</span><span class="p">)</span><span class="o">+</span><span class="s">'T23:59:59'</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">df_yesterday</span><span class="p">.</span><span class="n">to_parquet</span><span class="p">(</span><span class="s">'yesterday.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">df_today</span><span class="p">.</span><span class="n">to_parquet</span><span class="p">(</span><span class="s">'today.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="o">@</span><span class="n">task</span>
    <span class="k">def</span> <span class="nf">store_to_postgres</span><span class="p">():</span>
        <span class="n">engine</span> <span class="o">=</span> <span class="n">sqlalchemy</span><span class="p">.</span><span class="n">create_engine</span><span class="p">(</span><span class="s">'postgresql://root:root@postgres/mart'</span><span class="p">)</span>
        <span class="n">df_yesterday</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s">'yesterday.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">)</span>
        <span class="n">df_today</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s">'today.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">)</span>

        <span class="n">df_yesterday</span><span class="p">.</span><span class="n">to_sql</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s">'mart_</span><span class="si">{</span><span class="n">yesterday</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d"</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">con</span><span class="o">=</span><span class="n">engine</span><span class="p">,</span> <span class="n">if_exists</span><span class="o">=</span><span class="s">'append'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">df_today</span><span class="p">.</span><span class="n">to_sql</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s">'mart_</span><span class="si">{</span><span class="p">(</span><span class="n">yesterday</span> <span class="o">+</span> <span class="n">datetime</span><span class="p">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">)).</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d"</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">con</span><span class="o">=</span><span class="n">engine</span><span class="p">,</span> <span class="n">if_exists</span><span class="o">=</span><span class="s">'append'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">engine</span><span class="p">.</span><span class="n">dispose</span><span class="p">()</span>

    <span class="n">get_logs_from_hdfs</span><span class="p">()</span> <span class="o">&gt;&gt;</span> <span class="n">transform</span><span class="p">()</span> <span class="o">&gt;&gt;</span> <span class="n">store_to_postgres</span><span class="p">()</span>
    
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">batch_to_rdb</span><span class="p">()</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">transform()</code> 과정에서 df_yesterday와 df_today 데이터프레임으로 나누는 코드가 존재합니다. 이것은 logstash가 UTC로 동작하기 때문입니다. 기록되는 날이 UTC 기준이라서 한국 시간과 9시간 차이가 나기 때문에 Redis에 같은 키에 다른 날짜의 로그가 들어오게 됩니다.</p>

<p>예를들면, 한국시각 2022-11-30의 데이터는 KST기준 2022-11-30 09:00:00부터 2022-12-01 08:59:59까지이므로 Redis에는 <code class="language-plaintext highlighter-rouge">20221130</code>키로 접근했을 때 2022-12-01 데이터가 들어와 있게 됩니다. 따라서 이를 나눠 DB에 적재하는 코드가 필요했습니다. DB에 나눠 적재하지 않으면 배치 처리할때 그 이전 날짜들의 테이블까지 모두 조회해야할 가능성이 있기 때문에 이를 방지하는 이유또한 있습니다.</p>

<p>airflow에 SparkSession으로 세션을 생성하면 30초의 timeout으로 DAG가 등록되지 않았습니다.(정확한 이유가 맞는지는 잘 모르겠습니다.) 그래서 SparkContext를 통해 hdfs에 접근하는 방법으로 배치 파일을 구성했습니다.</p>

<h3 id="postgresql-jupyter-notebook">PostgreSQL, Jupyter Notebook</h3>
<p>RDB로 PostgreSQL을 사용했습니다. PostgreSQL을 MySQL보다 자주 사용해서 익숙하기 때문에 선택했습니다. BI로 Jupyter Notebook을 사용했는데 똑같이 Apache Zeplin보다 익숙하기 때문에 선택했습니다.</p>

<p>Jupyter Notebook은 로그인 시 패스워드를 물어보지 않게 하면 토큰을 입력해야 하는데 Notebook 접속 시 Jupyter Notebook 도커의 로그에 있는 URL로 접속해야 하는 불편함이 있습니다.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#data-engineering" class="page__taxonomy-item p-category" rel="tag">data-engineering</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#data-engineer" class="page__taxonomy-item p-category" rel="tag">data-engineer</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2022-12-02T00:00:00+09:00">December 2, 2022</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/pytorch/cnn-implementation/" class="pagination--pager" title="python으로 CNN 구현하기
">Previous</a>
    
    
      <a href="/pytorch/rnn-impl/" class="pagination--pager" title="python으로 RNN 구현하기
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/rnn-impl/" rel="permalink">python으로 RNN 구현하기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> December 28 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">Related

  이전 포스트에서 CNN을 구현했고 이번에는 RNN을 구현하는 과정을 정리하려고 합니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/cnn-implementation/" rel="permalink">python으로 CNN 구현하기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> September 21 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">Related

  이전 포스트에서 MLP를 구현했고 이번에는 CNN을 구현하는 삽질을 진행했습니다.
여기서는 Conv2d의 구현에 대해서만 정리하려고 합니다. 밑바닥부터 구현하실때 도움이 되었으면 좋겠습니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/dl-implement/" rel="permalink">Python으로 딥러닝 구현하기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> July 04 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">동기

  모기업 코딩테스트에 파이썬 기본 라이브러리로만 MLP를 구현하는 문제가 나왔던 적이 있습니다. 당시에 학습이 되지 않아 코딩테스트에서 떨어졌었고 구현하지 못했던 것이 계속 생각나서 구현해봤습니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/paper/On-the-effect-of-corpora/" rel="permalink">On the Effect of Pretraining Corpora on In-context Learning by a LLM
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> June 06 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">
  모두의연구소에서 논문저자가 직접 논문을 리뷰해주는 세미나가 열렸습니다. 주제가 재밌어 보여 발표를 듣고 논문을 다시 읽어 리뷰해보려고 합니다.

</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 emeraldgoose. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'emeraldgoose/emeraldgoose.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [['$','$']],
  displayMath: [['$$','$$']],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

  </body>
</html>
