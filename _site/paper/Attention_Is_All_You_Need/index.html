<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Attention Is All You Need - gooooooooooose</title>
<meta name="description" content="Astract">


  <meta name="author" content="goooose">
  
  <meta property="article:author" content="goooose">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="gooooooooooose">
<meta property="og:title" content="Attention Is All You Need">
<meta property="og:url" content="http://localhost:4000/paper/Attention_Is_All_You_Need/">


  <meta property="og:description" content="Astract">







  <meta property="article:published_time" content="2021-08-17T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/paper/Attention_Is_All_You_Need/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "emeraldgoose",
      "url": "http://localhost:4000/"
    
  }
</script>


  <meta name="google-site-verification" content="googleb34ce5276ba6573e" />





  <meta name="naver-site-verification" content="naver93cdc79a5629ab3736d7cf8ff7b51d80">


<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="gooooooooooose Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          gooooooooooose
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo.png" alt="goooose" itemprop="image" class="u-photo" width="110px" height="110px">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">goooose</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Dev blog.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:smk6221@naver.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/emeraldgoose" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Attention Is All You Need">
    <meta itemprop="description" content="Astract">
    <meta itemprop="datePublished" content="2021-08-17T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Attention Is All You Need
</h1>
          
<!--
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  0 minute read
</p>
            devinlife comments :
                싱글 페이지(포스트)에 제목 밑에 Updated 시간 표기
                기존에는 read_time이 표기. read_time -> date 변경
-->
            <p class="page__date"><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2021-08-17T00:00:00+09:00">August 17, 2021</time></p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="astract">Astract</h2>

<hr />

<p>대표적인 문장번역모델들은 인코더와 디코더를 포함한 rnn 혹은 cnn에 기반하고 있습니다.</p>

<p>또한, 가장 성능이 좋은 모델들은 인코더와 디코더가 attention 매커니즘에 의해 연결되어있습니다.</p>

<p>우리는 recurrent와 컨볼루션한 것들을 모두 배제하고 트랜스포머라는 새로운 간단한 네트워크 구조를 제안합니다다.</p>

<p>두번의 기계 번역에서의 실험은 이러한 모델들이 더 병렬화가 가능하고 훈련하는데 시간이 적게 소요됨과 동시에 퀄리티가 좋다는 점을 보여줍니다다.</p>

<p>WMT 2014에서 영어 → 독어에서 28.4 BLEU를 얻으면서 최고기록을 경신하고 영어→불어에서 41.0 BLEU를 얻고 SOTA(State-Of-The-Art)에 올라섰습니다. (8 GPU와 3.5일간의 학습 - 최고 성능의 모델이 필요한 자원보다 매우 적다)</p>

<h2 id="introduction">Introduction</h2>

<hr />

<p>LSTM이나 GRU와 같이 RNN이 sequence 모델링과 언어 모델링과 기계 번역 문제에서 강자였습니다. RNN 모델은 일반적으로 input과 output sequence의 위치에 따라 계산합니다.</p>

<p>연산 step위치에 따라, 모델은 현재 위치 $t$에서의 입력과 이전 hidden state $h_{t-1}$을 사용하여 $h_t$인 hidden state의 sequence를 생성합니다.</p>

<p>이 sequential한 성질은 학습에서의 병렬화를 하지 못하게 하고 긴 문장에서는 메모리 제약으로 인해 일괄 처리가 되지 않는 치명적인 단점이됩니다.</p>

<p>최근 연구에서 factorization trick과 conditional computation을 통해 계산에서의 상당한 개선이 이루어졌습니다.</p>

<p>그러나 sequential 계산에서의 단점(fundamental constraint of sequential computation)이 아직 남아있습니다.</p>

<p>Attention 매커니즘은 compelling sequence modeling과 transduction model에서 빠지지 않는 부분이 되었습니다.</p>

<p>input과 output sequence에서 거리를 고려하지 않아도 종속성을 모델링할 수 있습니다.</p>

<p>하지만 몇가지 경우를 제외하고, Attention 매커니즘은 Recurrent network를 사용합니다.</p>

<p>input과 output 사이의 dependencies를 설계하기 위한 Recurrence는 Transformer에서는 사용하지 않는다. 대신 attention 매커니즘에서 사용합니다.</p>

<p>Transformer는 더 많은 병렬화가 가능하고 8대의 P100 GPU와 12시간의 학습으로 번역분야에서의 SOTA에 도달했습니다.</p>

<h2 id="model-architecture">Model Architecture</h2>

<hr />

<p>Transformer는 인코더와 디코더 모두 self-attention이 쌓여있고 fully connected layer를 사용하여 아키텍쳐를 구성하고 있습니다.</p>

<h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h3>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1APkQc2wNRZidhWZ6-wpHkJb2rayryWrB" alt="" /></p>

<ul>
  <li><strong>Encoder</strong>
    <ul>
      <li>인코더는 $N=6$의 identical 레이어의 스택으로 구성되어있습니다. 각 레이어는 두 개의 sub-layer들을 가지고 있습니다.</li>
      <li>sub-layer의 첫번째는 Multi-Head Attention이고 두번째는 position-wise fully connected feed-forward network입니다.</li>
      <li>sub-layer들은 residual connection을 가지고 있고 이후에 Layer normalization을 사용합니다.</li>
      <li>sub-layer에 들어가는 입력을 $x$라 할때, sub-layer의 출력은 LayerNorm($x$ + Sublayer($x$))로 표기할 수 있습니다.</li>
    </ul>
  </li>
  <li><strong>Decoder</strong>
    <ul>
      <li>디코더 또한 $N=6$의 identical 레이어의 스택으로 구성되어있습니다. 디코더는 3개의 sub-layer로 구성되어 있는데 인코더의 출력에 multi-head attention을 수행할 sub-layer가 추가되었습니다.</li>
      <li>인코더와 비슷하게 각각의 sub-layer에 residual connection을 채용하고 그 후에 layer normalization을 수행합니다.</li>
      <li>디코더에서는 subsequent position을 위해 self-attention을 변형했는데 masking이라는 것을 사용합니다.</li>
      <li>masking은 position $i$보다 이후에 있는 position에 attention을 주지 못하게 합니다. 이를 통해 position $i$에 대해 미래 정보를 활용하지 못하도록 하는 것입니다.</li>
    </ul>
  </li>
</ul>

<h3 id="attention">Attention</h3>

<p>attention은 query와 key-value pair들을 output에 맵핑해주는 함수입니다. 출력은 values들의 weighted sum으로 계산되는데 value에 할당된 weight는 query와 대응되는 key의 compatibility function에 의해 계산됩니다.</p>

<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1-tli8qOupke7v5-XB97E-COUc6XH42eI" alt="" /></p>

<p>여기서 사용하는 attention을 Scaled Dot-Product Attention(SDPA)라 부르는데, input은 dimension이 $d_k$인 query와 key, dimension이 $d_v$인 value들로 이루어집니다.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1xrZKQfNWlXCNM5738iO8B0TdLW_u7hB6" alt="" /></p>

<p>모든 query와 모든 key들에 대해 dot product로 계산되는데 각각의 결과에 $\sqrt{d_k}$로 나누어진다. 다음 value의 가중치를 얻기 위해 softmax 함수를 적용합니다.</p>

<p>attention 함수는 additive attention과 dot-product attention이 사용됩니다.</p>

<ul>
  <li>additive attention은 single hidden layer와 함께 feed-forward network에 compatibility function을 계산하는데 사용됩니다.</li>
  <li>dot-product attention이 좀 더 빠르고 실제로 space-efficient합니다. 왜냐하면 optimized matrix multiplication code를 사용해서 구현되었기 때문입니다.</li>
</ul>

<p>$d_k$가 작은 경우 additive attention이 dot product attention보다 성능이 좋습니다.</p>

<p>그러나 $d_k$가 큰 값인 경우 softmax 함수에서 기울기 변화가 거의 없는 region으로 이동하기 때문에 dot product를 사용하면서 $\sqrt{d_k}$으로 나누어 scaling을 적용했습니다.</p>

<h3 id="multi-head-attention">Multi-Head Attention</h3>

<p><img src="https://drive.google.com/uc?export=view&amp;id=18yEv3-etuUHyzdplbfG-yV_ruLBPOtzy" alt="" /></p>

<p>$d_{model}$ dimension의 query, key, value 들로 하나의 attention을 수행하는 대신, query, key, value들에 각각 학습된 linear projection을 $h$번 수행하는 것이 더 좋습니다.</p>

<ul>
  <li>즉, $Q, K, V$에 각각 다른 weight를 곱해주는 것입니다.</li>
  <li>parameter matrix : $W_i^Q \in R^{d_{model}\times d_k}, \space w_i^K \in R^{d_{model}\times d_k}, \space W_i^V \in R^{d_{model}\times d_v}, \space W^O \in R^{hd_v \times d_{model}}$</li>
</ul>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1GyI80NxaXxZDUruxYBGCYgLtjV6LF2UZ" alt="" /></p>

<p>이때, projection이라 하는 이유는 각각의 값들이 parameter matrix와 곱해졌을 때, $d_k, d_v, d_{model}$차원으로 project되기 때문입니다. query, key, value들을 병렬적으로 attention function을 거쳐 dimension이 $d_v$인 output 값으로 나오게 됩니다.</p>

<p>이후 여러개의 head를 concatenate하고 다시 $W^O$와 projection하여 dimension이 $d_{model}$인 output 값으로 나오게 됩니다.</p>

<h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h3>

<p>인코더와 디코더는 fully connected feed-forward network를 가지고 있습니다.</p>

<p>또한, 두 번의 linear transformations과 activation function ReLU로 구성되어집니다.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1aZhqVcl5FEpX-CZqG96r__WemGt_9JcH" alt="" /></p>

<p>각각의 position마다 같은 $W, b$를 사용하지만 layer가 달라지면 다른 parameter를 사용합니다.</p>

<h3 id="positional-encoding">Positional Encoding</h3>

<p>모델이 recurrence와 convolution을 사용하지 않기 때문에 문장안에 상대적인 혹은 절대적인 위치의 token들에 대한 정보를 주입해야만 했습니다.</p>

<p>이후 positional encoding이라는 것을 encoder와 decoder stack 밑 input embedding에 더해줬습니다.</p>

<p>positional encoding은 $d_{model}$인 dimension을 가지고 있기 때문에 둘을 더할 수 있습니다.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=11EUTF3UWb4pqpiEjTl4vjOODVWjRPVRN" alt="" /></p>

<ul>
  <li>$pos$는 position, $i$는 dimension</li>
</ul>

<p>$pos$는 sequence에서 단어의 위치이고 해당 단어는 $i$에 $0$부터 $\frac{d_{model}}{2}$까지 대입해 dimension이 $d_{model}$인 positional encoding vector를 얻을 수 있습니다.</p>

<h2 id="why-self-attention">Why Self-Attention</h2>

<hr />

<p>Self-Attention을 사용하는 첫 번째 이유는 layer마다 total computational complexity가 작기 때문입니다.</p>

<p>두 번째 이유는 computation의 양이 parallelized하기때문에 sequential operation의 minimum으로 측정되기 때문입니다.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1zSom0Z9VTnveWpCw-_LQeFEafPhk5r9t" alt="" /></p>

<p>세 번째 이유로는 네트워크에서의 long-range dependencies사이의 path length때문입니다. long-range dependencies를 학습하는 것은 많은 문장 번역 분야에서의 key challenge가 됩니다.</p>

<p>input sequence와 output sequence의 길이가 길어지면 두 position간의 거리가 멀어져 long-range dependencies를 학습하는데 어려워집니다.</p>

<p>테이블을 보면 Recurrent layer의 경우 Sequential operation에서 $O(n)$이 필요하지만 Self-Attention의 경우 상수시간에 실행될 수 있습니다.</p>

<p>또한 Self-Attention은 interpretable(설명가능한) model인 것이 이점입니다.</p>

<h2 id="traning">Traning</h2>

<hr />

<h3 id="optimizer">Optimizer</h3>

<p>Adam optimizer와 $\beta_1=0.9$, $\beta_2=0.98$, $\epsilon=10^{-9}$를 사용했습니다.</p>

<p>학습동안 아래의 공식을 통해 learning rate를 변화시켰습니다.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1aQqVdboUkvGkAqrxOeuirWmtQerdXRCo" alt="" /></p>

<p>이는 warmup_step에 따라 linear하게 증가시키고 step number에 따라 square root한 값을 통해 점진적으로 줄여갔습니다. 그리고 warmup_step = 4000을 사용했습니다.</p>

<h3 id="residual-dropout">Residual Dropout</h3>

<p>각 sub-layer에서 input을 더하는 것과 normalization을 하기전에 output에 dropout을 설정했습니다. 또한 encoder와 decoder stacks에 embedding의 합계와 positional encoding에도 dropout을 설정했습니다.</p>

<p>그리고 rate $P_{drop}=0.1$을 사용했습니다.</p>

<h3 id="label-smoothing">Label Smoothing</h3>

<p>학습하는 동안 label smoothing value $\epsilon_{ls}=0.1$를 적용했습니다.</p>

<h2 id="result">Result</h2>

<hr />

<h3 id="machine-translation">Machine Translation</h3>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1GlZ6NFZ3XS1s63Xz7zP8f2aJY7NV21_d" alt="" /></p>

<p>영어→독일어 번역에서는 기존 모델들보다 높은 점수가 나왔고 영어→프랑스어 번역에서는 single 모델보다 좋고 ensemble 모델들과 비슷한 성능을 내주는 것을 볼 수 있습니다.</p>

<p>여기서 중요한 점은 Training Cost인데 기존 모델들보다 훨씬 적은 Cost가 들어가는 것을 볼 수 있습니다.</p>

<h3 id="model-variations">Model Variations</h3>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1W4prVn3P9rgL_SBWPH3M9dXhH8g7Q25v" alt="" /></p>

<ul>
  <li>테이블에 row (A)를 보면 single-head attention은 head=16일때보다 0.9 BLEU 낮고 head=32로 늘렸을 때도 head=16일때보다 BLEU가 낮습니다.</li>
  <li>row (B)를 보면 $d_k$를 낮추는 것이 model quality를 낮추게 합니다.</li>
  <li>row (C), (D)를 보면 더 큰 모델일수록 좋고, dropout이 overfitting을 피하는데 도움이 되는 것을 볼 수 있습니다.</li>
  <li>row (E)를 보면 sinusoidal position대신 learned positional embeddings를 넣었을 때의 결과가 base model과 동일한 결과인 것을 볼 수 있습니다.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<hr />

<p>우리는 완전히 attention에 기반한 첫번째 문장번역 모델인 Transformer를 소개했다. encoder-decorder 아키텍쳐에서 자주 사용되는 recurrent 레이어들을 multi-headed self-attention으로 대체합니다.</p>

<p>번역 분야에서, Transformer는 recurrent 또는 convolutional 레이어 기반 아키텍쳐들보다 상당히 빠르게 학습할 수 있습니다.</p>

<p>WMT 2014 영어→독어, 영어→불어 번역 분야에서 우리는 새로운 SOTA를 얻을 수 있었다. Transformer는 기존 모든 앙상블 모델들을 능가하는 성능을 낼 수 있습니다.</p>

<p>우리는 attention-based models의 미래에 기대하고 있고 다른 분야에 적용할 계획이 있습니다.</p>

<p>Transformer를 텍스트 이외에 입력 및 출력 형식과 관련된 문제로 확장하고 이미지, 오디오, 비디오와 같은 대규모 입력과 출력을 효율적으로 다루기 위한 local, restricted attention 매커니즘을 조사할 계획입니다.</p>

<p>generation을 덜 순차적으로 만드는 것도 또 다른 연구 목표입니다.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#transformer" class="page__taxonomy-item p-category" rel="tag">transformer</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#paper" class="page__taxonomy-item p-category" rel="tag">paper</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-08-17T00:00:00+09:00">August 17, 2021</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/boostcamp/sequential-model/" class="pagination--pager" title="Sequential Model 정리
">Previous</a>
    
    
      <a href="/atcoder/abc215/" class="pagination--pager" title="AtCoder Beginner Contest 215
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/rnn-impl/" rel="permalink">python으로 RNN 구현하기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> December 28 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">Related

  이전 포스트에서 CNN을 구현했고 이번에는 RNN을 구현하는 과정을 정리하려고 합니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/data-engineer/data-pipeline/" rel="permalink">데이터 파이프라인 구축해보기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> December 02 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">Motivation

  빅데이터를 지탱하는 기술을 읽다가 데이터 엔지니어링에 사용되는 플랫폼들을 전체 파이프라인으로 구축해보고 싶어서
이 사이드 프로젝트를 진행하게 되었습니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/cnn-implementation/" rel="permalink">python으로 CNN 구현하기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> September 21 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">Related

  이전 포스트에서 MLP를 구현했고 이번에는 CNN을 구현하는 삽질을 진행했습니다.
여기서는 Conv2d의 구현에 대해서만 정리하려고 합니다. 밑바닥부터 구현하실때 도움이 되었으면 좋겠습니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/dl-implement/" rel="permalink">Python으로 딥러닝 구현하기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> July 04 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">동기

  모기업 코딩테스트에 파이썬 기본 라이브러리로만 MLP를 구현하는 문제가 나왔던 적이 있습니다. 당시에 학습이 되지 않아 코딩테스트에서 떨어졌었고 구현하지 못했던 것이 계속 생각났었습니다.
그리고 numpy로 구현한 코드는 많았지만 numpy도 사용하지 않고 구현한 코드는...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 emeraldgoose. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'emeraldgoose/emeraldgoose.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [['$','$']],
  displayMath: [['$$','$$']],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

  </body>
</html>
