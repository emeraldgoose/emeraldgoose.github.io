<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Finetuning Large Language Models 정리 - gooooooooooose</title>
<meta name="title" content="Finetuning Large Language Models 정리">
<meta name="description" content="Finetuning Large Language Models - Deeplearning.ai">


  <meta name="author" content="goooose">
  
  <meta property="article:author" content="goooose">
  


<!-- open-graph tags -->
<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="gooooooooooose">
<meta property="og:title" content="Finetuning Large Language Models 정리">
<meta property="og:url" content="http://localhost:4000/coursera/finetuning-LLMs/">


  <meta property="og:description" content="Finetuning Large Language Models - Deeplearning.ai">







  <meta property="article:published_time" content="2023-12-07T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/coursera/finetuning-LLMs/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "emeraldgoose",
      "url": "http://localhost:4000/"
    
  }
</script>


  <meta name="google-site-verification" content="googleb34ce5276ba6573e" />





  <meta name="naver-site-verification" content="naver93cdc79a5629ab3736d7cf8ff7b51d80">


<!-- end _includes/seo.html -->




<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>

<!-- Lightbox2 -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/css/lightbox.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/js/lightbox-plus-jquery.js" integrity="sha512-oaWLach/xXzklmJDBjHkXngTCAkPch9YFqOSphnw590sy86CVEnAbcpw17QjkUGppGmVJojwqHmGO/7Xxx6HCw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/js/lightbox-plus-jquery.min.js" integrity="sha512-U9dKDqsXAE11UA9kZ0XKFyZ2gQCj+3AwZdBMni7yXSvWqLFEj8C1s7wRmWl9iyij8d5zb4wm56j4z/JVEwS77g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/" itemprop="item"><span itemprop="name">Home</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#coursera" itemprop="item"><span itemprop="name">Coursera</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Finetuning Large Language Models 정리</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo.png" alt="goooose" itemprop="image" class="u-photo" width="110px" height="110px">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">goooose</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in ML/DL, Data Engineering.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:smk6221@naver.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://github.com/emeraldgoose" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
          
            <li><a href="https://www.linkedin.com/in/minseong-kim-84428b231/" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://gooooooooooose.tistory.com/" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Problem Solving OJ (KOR)</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Finetuning Large Language Models 정리">
    <meta itemprop="description" content="  Finetuning Large Language Models - Deeplearning.ai">
    <meta itemprop="datePublished" content="2023-12-07T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Finetuning Large Language Models 정리
</h1>
          
<!--
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  0 minute read
</p>
            devinlife comments :
                싱글 페이지(포스트)에 제목 밑에 Updated 시간 표기
                기존에는 read_time이 표기. read_time -> date 변경
-->
            <p class="page__date"><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2023-12-07T00:00:00+09:00">December 07, 2023</time></p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <blockquote>
  <p><a href="https://www.coursera.org/projects/finetuning-large-language-models-project">Finetuning Large Language Models - Deeplearning.ai</a></p>
</blockquote>

<h1 id="why-finetune">Why Finetune?</h1>
<hr />
<p>Finetuning은 GPT-3와 같은 범용 모델을 사용하여 채팅을 잘 할 수 있는 ChatGPT 혹은 자동으로 코드를 완성하는 co-pilot과 같은 모델로 전문화하는 것을 말합니다. finetuning은 모델을 보다 일관된 동작으로 조정하는 것 외에도, 환각(hallucination)을 줄이는 데 도움이 될 수 있습니다.</p>

<p>finetuning은 prompt engineering과 차이점이 존재합니다. prompt engineering은 시작하는데 데이터가 필요하지 않습니다. 초기비용이 적게 든다는 장점이 있고 시작하기 위한 기술적 지식이 필요하지 않지만 환각에 대한 문제가 존재합니다. 모델이 이미 학습한 잘못된 정보를 수정하는 것이 어렵기 때문에 모델이 잘못된 정보를 출력하는 경우가 많습니다.</p>

<p>finetuning은 많은 수의 데이터를 사용할 수 있고 모델이 새로운 정보를 학습하는데 좋습니다. 따라서 이전에 학습했을 잘못된 정보를 수정하거나 이전에 학습되지 않은 최근 정보를 입력할 수도 있습니다. 그러나 좋은 품질의 더 많은 데이터와 컴퓨팅 리소스가 필요합니다.</p>

<p>따라서, prompt engineering은 다양한 사이드 프로젝트나 프로토타입에 적합하고 finetuning은 엔터프라이즈와 프로덕션에 적합합니다.</p>

<p>finetuning을 할 수 있는 라이브러리는 3가지가 존재합니다.</p>

<ul>
  <li>Pytorch(Meta) : Low-level interface</li>
  <li>Huggingface : Pytorch 보다 고수준의 인터페이스</li>
  <li>Llama(Llamini) : 세 라이브러리 중 가장 높은 수준의 인터페이스
    <ul>
      <li>여기서는 주로 Llamini 라이브러리를 사용하고 있습니다.</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Llamini Library
</span><span class="kn">from</span> <span class="nn">llama</span> <span class="kn">import</span> <span class="n">BasicModelRunner</span>
<span class="n">non_finetuned</span> <span class="o">=</span> <span class="n">BasicModelRunner</span><span class="p">(</span><span class="s">"meta-llama/Llama-2-7b-hf"</span><span class="p">)</span>
<span class="n">non_finetuned_output</span> <span class="o">=</span> <span class="n">non_finetuned</span><span class="p">(</span><span class="s">"Tell me how to train my dog to sit"</span><span class="p">)</span>

<span class="c1"># Output
# .
# Tell me how to train my dog to sit. 
# I have a 10 month old puppy and I want to train him to sit. 
# I have tried the treat method and he just sits there and looks at me like I am crazy. 
# I have tried the "sit" command and he just looks at me like I am crazy. 
# I have tried the "sit" command and he just looks at me like I am crazy. 
</span><span class="p">...</span>
<span class="c1"># I have tried the "sit" command and he just looks at me like I am crazy. 
# I have tried the "sit" command and he just looks
</span>
<span class="n">finetuned_model</span> <span class="o">=</span> <span class="n">BasicModelRunner</span><span class="p">(</span><span class="s">"meta-llama/Llama-2-7b-chat-hf"</span><span class="p">)</span>
<span class="n">finetuned_otuput</span> <span class="o">=</span> <span class="n">finetuned_model</span><span class="p">(</span><span class="s">"Tell me how to train my dog to sit"</span><span class="p">)</span>

<span class="c1"># Output
# on command.
# Training a dog to sit on command is a basic obedience command that can be achieved with patience, consistency, and positive reinforcement. Here's a step-by-step guide on how to train your dog to sit on command:
</span>
<span class="c1"># 1. Choose a quiet and distraction-free area: Find a quiet area with minimal distractions where your dog can focus on you.
# 2. Have treats ready: Choose your dog's favorite treats and have them ready to use as rewards.
# 3. Stand in front of your dog: Stand in front of your dog and hold a treat close to their nose.
# 4. Move the treat up and back: Slowly move the treat up and back, towards your dog's tail, while saying "sit" in a calm and clear voice.
# 5. Dog will sit: As you move the treat, your dog will naturally sit down to follow the treat. The moment their bottom touches the ground, say "good sit" and give them the treat.
# 6. Repeat the process: Repeat steps 3-5 several times, so your dog starts to associate the command "sit" with
</span></code></pre></div></div>

<h1 id="where-finetuning-fits-in">Where finetuning fits in</h1>
<hr />
<p>Finetuning은 사전훈련(Pretrain) 단계 뒤에 진행합니다. 사전훈련은 전혀 세팅되지 않은 모델을 사용하며 다음 토큰을 예측하는 목표를 가지고 훈련합니다. 예를들어 ‘wants’ 뒤에 ‘upon’이라는 단어를 예측하도록 훈련하며 라벨링되지 않은 거대한 양의 데이터를 학습합니다. 이러한 학습 방법을 Self-supervised Learning이라 합니다.</p>

<p>Finetuning은 모델의 동작을 변화시킵니다. 모델 응답을 일관성있게 하거나 질문에 좀 더 집중할 수 있게 합니다. 또한, 모델의 능력을 발현(?)시킬 수 있습니다. 대화에 더 능숙해져 다양한 주제에 대해 이야기 할 수 있게 됩니다. 이전에는 이러한 정보를 얻기 위해 많은 Prompt engineering을 해야 했지만 finetuning은 쉽게 가능합니다.</p>

<p>Finetuning을 처음 하게 된다면 추천하는 몇가지 단계에 대해 소개합니다.</p>

<ol>
  <li>Identify task(s) by prompt-engineering a large LLM</li>
  <li>Find tasks that you see an LLM doing ~OK at</li>
  <li>Pick one task</li>
  <li>Get ~1000 inputs and outputs for the task
    <ul>
      <li>Better than the ~OK from the LLM</li>
    </ul>
  </li>
  <li>Finetune a small LLM on this data</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">jsonlines</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="n">filename</span> <span class="o">=</span> <span class="s">"lamini_docs.jsonl"</span>
<span class="n">instruction_dataset_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_json</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">examples</span> <span class="o">=</span> <span class="n">instruction_dataset_df</span><span class="p">.</span><span class="n">to_dict</span><span class="p">()</span>

<span class="n">prompt_template_qa</span> <span class="o">=</span> <span class="s">"""### Question:
{question}

### Answer:
{answer}"""</span>

<span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s">"question"</span><span class="p">])</span>
<span class="n">finetuning_dataset_text_only</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">finetuning_dataset_question_answer</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_examples</span><span class="p">):</span>
  <span class="n">question</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s">"question"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
  <span class="n">answer</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s">"answer"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>

  <span class="n">text_with_prompt_template_qa</span> <span class="o">=</span> <span class="n">prompt_template_qa</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
    <span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">,</span>
    <span class="n">answer</span><span class="o">=</span><span class="n">answer</span>
  <span class="p">)</span>
  <span class="n">finetuning_dataset_text_only</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
    <span class="p">{</span>
      <span class="s">"text"</span><span class="p">:</span> <span class="n">text_with_prompt_template_qa</span>
    <span class="p">}</span>
  <span class="p">)</span>

  <span class="n">text_with_prompt_template_q</span> <span class="o">=</span> <span class="n">prompt_template_q</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
    <span class="n">question</span><span class="o">=</span><span class="n">question</span>
  <span class="p">)</span>
  <span class="n">finetuning_dataset_question_answer</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
    <span class="p">{</span>
      <span class="s">"question"</span><span class="p">:</span> <span class="n">text_with_prompt_template_q</span><span class="p">,</span>
      <span class="s">"answer"</span><span class="p">:</span> <span class="n">answer</span>
    <span class="p">}</span>
  <span class="p">)</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">finetuning_dataset_text_only</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Output
# {'text': '### Question:\n'
#          'What are the different types of documents available in the '
#          "repository (e.g., installation guide, API documentation, developer's "
#          'guide)?\n'
#          '\n'
#          '### Answer:\n'
#          'Lamini has documentation on Getting Started, Authentication, '
#          'Question Answer Model, Python Library, Batching, Error Handling, '
#          'Advanced topics, and class documentation on LLM Engine available at '
#          'https://lamini-ai.github.io/.'}
</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">finetuning_dataset_question_answer</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Output
# {'answer': 'Lamini has documentation on Getting Started, Authentication, '
#            'Question Answer Model, Python Library, Batching, Error Handling, '
#            'Advanced topics, and class documentation on LLM Engine available '
#            'at https://lamini-ai.github.io/.',
#  'question': '### Question:\n'
#              'What are the different types of documents available in the '
#              'repository (e.g., installation guide, API documentation, '
#              "developer's guide)?\n"
#              '\n'
#              '### Answer:'}
</span></code></pre></div></div>

<h1 id="instruction-finetuning">Instruction finetuning</h1>
<hr />
<p>Finetuning 중 Instruction Finetuning(instruction-tuned, instruction-following)이라는 finetuning이 있습니다. 이 방식은 모델을 챗봇과 같이 행동하도록 조정하는 방법입니다. Instruction-following 데이터셋의 예로는 FAQs, Customer support conversation, Slak 메시지 등이 있습니다. 만약 데이터가 없다면 LLM을 프롬프트 템플릿을 사용하여 Non-QnA 데이터를 QnA 데이터로 변환하는 방법도 존재합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Instruction-tuning
</span><span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">jsonlines</span>

<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="n">instruction_tuned_dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span>
	<span class="s">"tatsu-lab/alpaca"</span><span class="p">,</span> 
	<span class="n">split</span><span class="o">=</span><span class="s">"train"</span><span class="p">,</span> 
	<span class="n">streaming</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="c1"># Two prompt templates
</span><span class="n">prompt_template_with_input</span> <span class="o">=</span> <span class="s">"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:"""</span>

<span class="n">prompt_template_without_input</span> <span class="o">=</span> <span class="s">"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:"""</span>

<span class="c1"># Hydrate prompts (add data to prompts)
</span><span class="n">processed_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">top_m</span><span class="p">:</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">j</span><span class="p">[</span><span class="s">"input"</span><span class="p">]:</span>
    <span class="n">processed_prompt</span> <span class="o">=</span> <span class="n">prompt_template_without_input</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">instruction</span><span class="o">=</span><span class="n">j</span><span class="p">[</span><span class="s">"instruction"</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">processed_prompt</span> <span class="o">=</span> <span class="n">prompt_template_with_input</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">instruction</span><span class="o">=</span><span class="n">j</span><span class="p">[</span><span class="s">"instruction"</span><span class="p">],</span> <span class="nb">input</span><span class="o">=</span><span class="n">j</span><span class="p">[</span><span class="s">"input"</span><span class="p">])</span>

  <span class="n">processed_data</span><span class="p">.</span><span class="n">append</span><span class="p">({</span><span class="s">"input"</span><span class="p">:</span> <span class="n">processed_prompt</span><span class="p">,</span> <span class="s">"output"</span><span class="p">:</span> <span class="n">j</span><span class="p">[</span><span class="s">"output"</span><span class="p">]})</span>

<span class="c1"># Save data to jsonl
</span><span class="k">with</span> <span class="n">jsonlines</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s">'alpaca_processed.jsonl'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
    <span class="n">writer</span><span class="p">.</span><span class="n">write_all</span><span class="p">(</span><span class="n">processed_data</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">llama</span> <span class="kn">import</span> <span class="n">BasicModelRunner</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_input_tokens</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">max_output_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="c1"># Tokenize
</span>  <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">,</span>
    <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_input_tokens</span>
  <span class="p">)</span>

  <span class="c1"># Generate
</span>  <span class="n">device</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">device</span>
  <span class="n">generated_tokens_with_prompt</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_output_tokens</span>
  <span class="p">)</span>

  <span class="c1"># Decode
</span>  <span class="n">generated_text_with_prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_tokens_with_prompt</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="c1"># Strip the prompt
</span>  <span class="n">generated_text_answer</span> <span class="o">=</span> <span class="n">generated_text_with_prompt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):]</span>

  <span class="k">return</span> <span class="n">generated_text_answer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"EleutherAI/pythia-70m"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"EleutherAI/pythia-70m"</span><span class="p">)</span>

<span class="n">finetuning_dataset_path</span> <span class="o">=</span> <span class="s">"lamini/lamini_docs"</span>
<span class="n">finetuning_dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">finetuning_dataset_path</span><span class="p">)</span>

<span class="n">test_sample</span> <span class="o">=</span> <span class="n">finetuning_dataset</span><span class="p">[</span><span class="s">"test"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">inference</span><span class="p">(</span><span class="n">test_sample</span><span class="p">[</span><span class="s">"question"</span><span class="p">],</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>

<span class="c1"># Output
# I have a question about the following:
# 
# How do I get the correct documentation to work?
# 
# A:
# 
# I think you need to use the following code:
# 
# A:
# 
# You can use the following code to get the correct documentation.
# 
# A:
# 
# You can use the following code to get the correct documentation.
# 
# A:
# 
# You can use the following
</span></code></pre></div></div>

<h1 id="data-preparation">Data preparation</h1>
<hr />
<p>데이터 준비에 필요한 몇가지가 있습니다.</p>

<ol>
  <li>고품질의 데이터</li>
  <li>데이터의 다양성</li>
  <li>데이터의 양
    <ul>
      <li>데이터의 양보다 품질이 더 중요함</li>
    </ul>
  </li>
</ol>

<p>데이터를 준비하는 스텝은 다음과 같습니다.</p>

<ol>
  <li>instruction-response 쌍을 수집한다.</li>
  <li>이러한 쌍을 연결(concatenate)하거나 Prompt 템플릿을 추가한다.</li>
  <li>데이터를 토크나이징하고 패딩을 추가하거나 데이터를 잘라 알맞은 크기로 모델에 입력하도록 한다.</li>
  <li>해당 데이터를 train과 test로 분리한다.</li>
</ol>

<p>토크나이징은 텍스트 데이터를 가져와 각각의 텍스트 조각을 숫자로 변환하는 작업입니다. 다양한 토크나이징 도구가 있으며 모델들은 훈련된 특정 토크나이저와 연관되어 있습니다. 잘못된 토크나이저를 모델에 사용하면 모델과 토크나이저가 서로 다른 문자 및 단어를 나타내어 모델이 혼란스러워질 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">datasets</span>

<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"EleutherAI/pythia-70m"</span><span class="p">)</span>

<span class="n">filename</span> <span class="o">=</span> <span class="s">"lamini_docs.jsonl"</span>
<span class="n">instruction_dataset_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_json</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">examples</span> <span class="o">=</span> <span class="n">instruction_dataset_df</span><span class="p">.</span><span class="n">to_dict</span><span class="p">()</span>

<span class="n">prompt_template</span> <span class="o">=</span> <span class="s">"""### Question:
{question}

### Answer:"""</span>

<span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s">"question"</span><span class="p">])</span>
<span class="n">finetuning_dataset</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_examples</span><span class="p">):</span>
  <span class="n">question</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s">"question"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
  <span class="n">answer</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s">"answer"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
  <span class="n">text_with_prompt_template</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">)</span>
  <span class="n">finetuning_dataset</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
    <span class="p">{</span>
      <span class="s">"question"</span><span class="p">:</span> <span class="n">text_with_prompt_template</span><span class="p">,</span> 
      <span class="s">"answer"</span><span class="p">:</span> <span class="n">answer</span>
    <span class="p">}</span>
  <span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
  <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token</span>
  <span class="n">tokenized_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s">"np"</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="p">)</span>

  <span class="n">max_length</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
    <span class="n">tokenized_inputs</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="mi">2048</span>
  <span class="p">)</span>
  <span class="n">tokenizer</span><span class="p">.</span><span class="n">truncation_side</span> <span class="o">=</span> <span class="s">"left"</span>
  <span class="n">tokenized_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s">"np"</span><span class="p">,</span>
    <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span>
  <span class="p">)</span>

  <span class="k">return</span> <span class="n">tokenized_inputs</span>

<span class="n">finetuning_dataset_loaded</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span>
  <span class="s">"json"</span><span class="p">,</span> 
  <span class="n">data_files</span><span class="o">=</span><span class="n">filename</span><span class="p">,</span> 
  <span class="n">split</span><span class="o">=</span><span class="s">"train"</span>
<span class="p">)</span>

<span class="n">tokenized_dataset</span> <span class="o">=</span> <span class="n">finetuning_dataset_loaded</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span>
  <span class="n">tokenize_function</span><span class="p">,</span>
  <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">drop_last_batch</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="n">tokenized_dataset</span> <span class="o">=</span> <span class="n">tokenized_dataset</span><span class="p">.</span><span class="n">add_column</span><span class="p">(</span><span class="s">"labels"</span><span class="p">,</span> <span class="n">tokenized_dataset</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">])</span>
<span class="n">split_dataset</span> <span class="o">=</span> <span class="n">tokenized_dataset</span><span class="p">.</span><span class="n">train_test_split</span><span class="p">(</span>
  <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
  <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
  <span class="n">seed</span><span class="o">=</span><span class="mi">123</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">split_dataset</span><span class="p">)</span>

<span class="c1"># Output
# DatasetDict({
#     train: Dataset({
#         features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],
#         num_rows: 1260
#     })
#     test: Dataset({
#         features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],
#         num_rows: 140
#     })
# })
</span></code></pre></div></div>

<h1 id="training-process">Training process</h1>
<hr />
<p>기존 신경망에서의 학습은 학습 데이터를 추가하고 loss를 계산하여 역전파과정을 거쳐 파라미터를 업데이트합니다. 하이퍼파라미터는 learning rate, learning rate scheduler, optimizer hyperparameters가 된다. 그리고 Pytorch와 llamini를 사용하면 학습 프로세스가 아래 코드와 같은 형태로 작성됩니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Llamini
</span><span class="kn">from</span> <span class="nn">llama</span> <span class="kn">import</span> <span class="n">BasicModelRunner</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BasicModelRunner</span><span class="p">(</span><span class="s">"EleutherAI/pythia-410m"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">load_data_from_jsonlines</span><span class="p">(</span><span class="s">"lamini_docs.jsonl"</span><span class="p">,</span> <span class="n">input_key</span><span class="o">=</span><span class="s">"question"</span><span class="p">,</span> <span class="n">output_key</span><span class="o">=</span><span class="s">"answer"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">is_public</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">config</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">jsonlines</span>

<span class="kn">from</span> <span class="nn">utilities</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">llama</span> <span class="kn">import</span> <span class="n">BasicModelRunner</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="p">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>
<span class="n">global_config</span> <span class="o">=</span> <span class="bp">None</span>

<span class="c1"># Load the Lamini docs dataset
</span><span class="n">dataset_name</span> <span class="o">=</span> <span class="s">"lamini_docs.jsonl"</span>
<span class="n">dataset_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"/content/</span><span class="si">{</span><span class="n">dataset_name</span><span class="si">}</span><span class="s">"</span>
<span class="n">use_hf</span> <span class="o">=</span> <span class="bp">False</span>

<span class="n">dataset_path</span> <span class="o">=</span> <span class="s">"lamini/lamini_docs"</span>
<span class="n">use_hf</span> <span class="o">=</span> <span class="bp">True</span>

<span class="c1"># Set up model, training config, and tokenizer
</span><span class="n">model_name</span> <span class="o">=</span> <span class="s">"EleutherAI/pythia-70m"</span>
<span class="n">training_config</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">"model"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"pretrained_name"</span><span class="p">:</span> <span class="n">model_name</span><span class="p">,</span>
    <span class="s">"max_length"</span> <span class="p">:</span> <span class="mi">2048</span>
  <span class="p">},</span>
  <span class="s">"datasets"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"use_hf"</span><span class="p">:</span> <span class="n">use_hf</span><span class="p">,</span>
    <span class="s">"path"</span><span class="p">:</span> <span class="n">dataset_path</span>
  <span class="p">},</span>
  <span class="s">"verbose"</span><span class="p">:</span> <span class="bp">True</span>
<span class="p">}</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">tokenize_and_split_data</span><span class="p">(</span><span class="n">training_config</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

<span class="c1"># Load the base model
</span><span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">device_count</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span>
<span class="k">if</span> <span class="n">device_count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
  <span class="n">logger</span><span class="p">.</span><span class="n">debug</span><span class="p">(</span><span class="s">"Select GPU device"</span><span class="p">)</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">logger</span><span class="p">.</span><span class="n">debug</span><span class="p">(</span><span class="s">"Select CPU device"</span><span class="p">)</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">)</span>

<span class="n">base_model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Define function to carry out inference
</span><span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_input_tokens</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">max_output_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="c1"># Tokenize
</span>  <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">,</span>
    <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_input_tokens</span>
  <span class="p">)</span>

  <span class="c1"># Generate
</span>  <span class="n">device</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">device</span>
  <span class="n">generated_tokens_with_prompt</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_output_tokens</span>
  <span class="p">)</span>

  <span class="c1"># Decode
</span>  <span class="n">generated_text_with_prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_tokens_with_prompt</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="c1"># Strip the prompt
</span>  <span class="n">generated_text_answer</span> <span class="o">=</span> <span class="n">generated_text_with_prompt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):]</span>

  <span class="k">return</span> <span class="n">generated_text_answer</span>

<span class="c1"># Setup trainig
</span><span class="n">max_steps</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">trained_model_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"lamini_docs_</span><span class="si">{</span><span class="n">max_steps</span><span class="si">}</span><span class="s">_steps"</span>
<span class="n">output_dir</span> <span class="o">=</span> <span class="n">trained_model_name</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>

  <span class="c1"># Learning rate
</span>  <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0e-5</span><span class="p">,</span>

  <span class="c1"># Number of training epochs
</span>  <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

  <span class="c1"># Max steps to train for (each step is a batch of data)
</span>  <span class="c1"># Overrides num_train_epochs, if not -1
</span>  <span class="n">max_steps</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span>

  <span class="c1"># Batch size for training
</span>  <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

  <span class="c1"># Directory to save model checkpoints
</span>  <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">,</span>

  <span class="c1"># Other arguments
</span>  <span class="n">overwrite_output_dir</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># Overwrite the content of the output directory
</span>  <span class="n">disable_tqdm</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># Disable progress bars
</span>  <span class="n">eval_steps</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="c1"># Number of update steps between two evaluations
</span>  <span class="n">save_steps</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="c1"># After # steps model is saved
</span>  <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># Number of warmup steps for learning rate scheduler
</span>  <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># Batch size for evaluation
</span>  <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s">"steps"</span><span class="p">,</span>
  <span class="n">logging_strategy</span><span class="o">=</span><span class="s">"steps"</span><span class="p">,</span>
  <span class="n">logging_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">optim</span><span class="o">=</span><span class="s">"adafactor"</span><span class="p">,</span>
  <span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
  <span class="n">gradient_checkpointing</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>

  <span class="c1"># Parameters for early stopping
</span>  <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">metric_for_best_model</span><span class="o">=</span><span class="s">"eval_loss"</span><span class="p">,</span>
  <span class="n">greater_is_better</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>

<span class="n">model_flops</span> <span class="o">=</span> <span class="p">(</span>
  <span class="n">base_model</span><span class="p">.</span><span class="n">floating_point_ops</span><span class="p">(</span>
    <span class="p">{</span>
      <span class="s">"input_ids"</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">training_config</span><span class="p">[</span><span class="s">"model"</span><span class="p">][</span><span class="s">"max_length"</span><span class="p">])</span>
      <span class="p">)</span>
    <span class="p">}</span>
  <span class="p">)</span>
  <span class="o">*</span> <span class="n">training_args</span><span class="p">.</span><span class="n">gradient_accumulation_steps</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
  <span class="n">model</span><span class="o">=</span><span class="n">base_model</span><span class="p">,</span>
  <span class="n">model_flops</span><span class="o">=</span><span class="n">model_flops</span><span class="p">,</span>
  <span class="n">total_steps</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span>
  <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
  <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
  <span class="n">eval_dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Train a few steps
</span><span class="n">training_output</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># Save model locally
</span><span class="n">save_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">output_dir</span><span class="si">}</span><span class="s">/final'</span>

<span class="n">trainer</span><span class="p">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">save_dir</span><span class="p">)</span>

<span class="n">finetuned_slightly_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">local_files_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">finetuned_slightly_model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Finetune a model in 3 lines of code using Lamini
</span><span class="n">model</span> <span class="o">=</span> <span class="n">BasicModelRunner</span><span class="p">(</span><span class="s">"EleutherAI/pythia-410m"</span><span class="p">)</span> 
<span class="n">model</span><span class="p">.</span><span class="n">load_data_from_jsonlines</span><span class="p">(</span><span class="s">"lamini_docs.jsonl"</span><span class="p">,</span> <span class="n">input_key</span><span class="o">=</span><span class="s">"question"</span><span class="p">,</span> <span class="n">output_key</span><span class="o">=</span><span class="s">"answer"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">is_public</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="n">lofd</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">out</span><span class="p">[</span><span class="s">'eval_results'</span><span class="p">]:</span>
  <span class="n">q</span>  <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">e</span><span class="p">[</span><span class="s">'input'</span><span class="p">]</span><span class="si">}</span><span class="s">"</span>
  <span class="n">at</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">e</span><span class="p">[</span><span class="s">'outputs'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s">'output'</span><span class="p">]</span><span class="si">}</span><span class="s">"</span>
  <span class="n">ab</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">e</span><span class="p">[</span><span class="s">'outputs'</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="s">'output'</span><span class="p">]</span><span class="si">}</span><span class="s">"</span>
  <span class="n">di</span> <span class="o">=</span> <span class="p">{</span><span class="s">'question'</span><span class="p">:</span> <span class="n">q</span><span class="p">,</span> <span class="s">'trained model'</span><span class="p">:</span> <span class="n">at</span><span class="p">,</span> <span class="s">'Base Model'</span> <span class="p">:</span> <span class="n">ab</span><span class="p">}</span>
  <span class="n">lofd</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">di</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">lofd</span><span class="p">)</span>
<span class="n">style_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">set_properties</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="s">'text-align'</span><span class="p">:</span> <span class="s">'left'</span><span class="p">})</span>
<span class="n">style_df</span> <span class="o">=</span> <span class="n">style_df</span><span class="p">.</span><span class="n">set_properties</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="s">"vertical-align"</span><span class="p">:</span> <span class="s">"text-top"</span><span class="p">})</span>

<span class="c1"># Output
# question : Does Lamini have the ability to understand and generate code for audio processing tasks?
# trained model : Yes, Lamini has the ability to understand and generate code.
# Base Model : A: Lamini is a very good language for audio processing.\nA: I think...
</span></code></pre></div></div>

<h1 id="evaluation-and-iteration">Evaluation and iteration</h1>
<hr />
<p>모델을 훈련한 후에는 다음 단계로 모델을 평가하고 성능을 확인해야 합니다. 이 과정은 시간이 지남에 따라 모델을 개선하는데 도움을 줄 수 있는 반복적인 과정입니다.</p>

<p>생성 모델은 명확한 측정 지표가 없어 평가하는 것이 어렵습니다. 그래서 Human expert evaluation이 종종 가장 신뢰성있는 방법이며 해당 도메인을 이해하는 전문가들이 평가하는 것입니다.</p>

<p>LLM 벤치마크는 여러 평가 방법의 모음으로 구성됩니다.</p>

<ul>
  <li>ARC : 초등학교 질문의 모음</li>
  <li>HellaSwag : 상식 테스트</li>
  <li>MMLU : 다양한 초등학교 과목</li>
  <li>TruthfulQA : 모델이 온라인에서 흔히 찾을 수 있는 거짓말을 재현하는 능력을 측정</li>
</ul>

<p>모델을 분석하고 평가하는데 사용되는 방법 중 “Error Analysis”가 있습니다. Error Analysis는 에러를 범주화하여 매우 일반적인 에러 유형을 이해하고 가장 흔한 에러와 매우 치명적인 에러를 우선적으로 해결하는 것입니다.</p>

<p>모델을 finetuning하기 전에 기본 모델의 에러를 분석한 후 finetuning을 했을 때 가장 큰 효과를 줄 수 있는 데이터 종류를 파악할 수 있습니다.</p>

<p>에러의 범주는 다음과 같습니다.</p>

<ul>
  <li>Misspelling(맞춤법 오류)</li>
  <li>Too long(길이) : 데이터셋이 덜 장황하도록 하여 모델이 질문에 명확하게 답변할 수 있도록 하는 것이 중요합니다.</li>
  <li>Repetitive(반복) : 모델이 반복적인 응답을 할 수 있는데 이를 해결하는 한 가지 방법은 stop token들을 사용하거나(eos 토큰을 가리키는 것 같습니다) Prompt 템플릿을 이용하는 것입니다. 데이터셋에 반복이 적고 다양성이 있는 예제를 포함하는 것도 중요합니다.</li>
</ul>

<p>이 강의에서는 벤치마크에 너무 집착하지 말라고 합니다. 모델을 순위를 매기는 방식이긴 하지만, 실제 사용사례와 다를 수 있기 때문입니다. 따라서 finetuning된 모델은 다양한 task에 맞게 조정될 수 있으며, 이는 다양한 평가 방법이 필요합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">config</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">difflib</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">utilities</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="p">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>
<span class="n">global_config</span> <span class="o">=</span> <span class="bp">None</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s">"lamini/lamini_docs"</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s">"test"</span><span class="p">]</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s">"lamini/lamini_docs_finetuned"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Setup a really basic evaluation function
</span><span class="k">def</span> <span class="nf">is_exact_match</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">a</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">==</span> <span class="n">b</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span>

<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span> <span class="c1"># dropout과 같은 기능이 비활성화되었는지 확인해야 한다
</span>
<span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_input_tokens</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">max_output_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="c1"># Tokenize
</span>  <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token</span>
  <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span>
      <span class="n">text</span><span class="p">,</span>
      <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">,</span>
      <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
      <span class="n">max_length</span><span class="o">=</span><span class="n">max_input_tokens</span>
  <span class="p">)</span>

  <span class="c1"># Generate
</span>  <span class="n">device</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">device</span>
  <span class="n">generated_tokens_with_prompt</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">max_output_tokens</span>
  <span class="p">)</span>

  <span class="c1"># Decode
</span>  <span class="n">generated_text_with_prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_tokens_with_prompt</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="c1"># Strip the prompt
</span>  <span class="n">generated_text_answer</span> <span class="o">=</span> <span class="n">generated_text_with_prompt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):]</span>

  <span class="k">return</span> <span class="n">generated_text_answer</span>

<span class="c1"># Run model and compare to expected answer
</span><span class="n">generated_answer</span> <span class="o">=</span> <span class="n">inference</span><span class="p">(</span><span class="n">test_question</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

<span class="n">answer</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s">"answer"</span><span class="p">]</span>

<span class="n">exact_match</span> <span class="o">=</span> <span class="n">is_exact_match</span><span class="p">(</span><span class="n">generated_answer</span><span class="p">,</span> <span class="n">answer</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">exact_match</span><span class="p">)</span>

<span class="c1"># Output
# False
# generated_answer와 answer가 같은지 단순히 확인하는 간단한 방법입니다
# 다른 방법으로는 generated_answer와 answer를 LLM에 입력하고 같은 답변인지 점수로 매겨 얼마나 가까운지 확인할 수 있다
</span>
<span class="c1"># Run over entire dataset
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s">'exact_matches'</span><span class="p">:</span> <span class="p">[]}</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"i Evaluating: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">item</span><span class="p">))</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s">'question'</span><span class="p">]</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s">'answer'</span><span class="p">]</span>

    <span class="k">try</span><span class="p">:</span>
      <span class="n">predicted_answer</span> <span class="o">=</span> <span class="n">inference</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
      <span class="k">continue</span>
    <span class="n">predictions</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">predicted_answer</span><span class="p">,</span> <span class="n">answer</span><span class="p">])</span>

    <span class="c1">#fixed: exact_match = is_exact_match(generated_answer, answer)
</span>    <span class="n">exact_match</span> <span class="o">=</span> <span class="n">is_exact_match</span><span class="p">(</span><span class="n">predicted_answer</span><span class="p">,</span> <span class="n">answer</span><span class="p">)</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s">'exact_matches'</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">exact_match</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">n</span> <span class="ow">and</span> <span class="n">n</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
      <span class="k">break</span>
</code></pre></div></div>

<h1 id="consideration-on-getting-started-now">Consideration on getting started now</h1>
<hr />
<p>Finetuning과정에 대한 실용적인 접근을 소개합니다.</p>

<ol>
  <li>Task를 먼저 이해해야 합니다.</li>
  <li>Task의 입력과 출력에 관련된 데이터를 수집합니다.</li>
  <li>데이터가 충분하지 않다면 데이터를 생성합니다.
    <ul>
      <li>Prompt Template</li>
    </ul>
  </li>
  <li>먼저 작은 모델을 finetuning하는 것이 좋습니다.
    <ul>
      <li>4억(400M)에서 10억(1B) 파라미터 모델을 추천합니다.</li>
    </ul>
  </li>
  <li>데이터의 양을 변경하면서 모델의 성능을 측정합니다.</li>
  <li>모델을 평가하면서 잘 되고 있는지 확인합니다.</li>
  <li>모델을 개선하기 위해 더 많은 데이터를 수집합니다.</li>
  <li>Task를 좀 더 복잡하게 해봅니다.
    <ul>
      <li>글쓰기 작업은 읽기 작업보다 더 많은 토큰을 생성해야 하므로 어렵습니다.</li>
      <li>여러 가지 작업을 묶어 모델이 여러 작업을 수행하도록 할 수도 있습니다.</li>
    </ul>
  </li>
  <li>성능을 위해 모델의 사이즈를 늘려봅니다.</li>
</ol>

<p>작업 복잡성에 따른 모델 크기를 감안할 때 하드웨어에 대한 고려가 필요합니다. v100 1장은 16GB 메모리를 가지고 있어 추론시 최대 7B의 모델을 사용할 수 있지만 훈련시 최대 1B 모델만 사용가능합니다. 더 큰 모델을 사용하려면 다른 옵션을 고려해야 합니다.</p>

<p>더 큰 모델을 사용하려는 경우 PEFT(Parameter-efficient Fine-tuning)나 LoRa(Low-Rank Adaptation)와 같은 다양한 방법들을 고려해볼 수 있습니다. 이 중 LoRa는 main pretrained weight들을 freezing하고 일부 레이어안에 새로운 가중치를 훈련시키는 방법입니다. 새롭게 훈련된 가중치를 메인 가중치로 다시 병합하여 fine-tuning된 모델을 더 효율적으로 얻을 수 있습니다.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#llm" class="page__taxonomy-item p-category" rel="tag">LLM</a><span class="sep">, </span>
    
      <a href="/tags/#nlp" class="page__taxonomy-item p-category" rel="tag">nlp</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#coursera" class="page__taxonomy-item p-category" rel="tag">coursera</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2023-12-07T00:00:00+09:00">December 7, 2023</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/contest/ai-ctf/" class="pagination--pager" title="AI Village Capture the Flag @ DEFCON31 후기
">Previous</a>
    
    
      <a href="/airflow/airflow-task-design/" class="pagination--pager" title="Airflow task 디자인
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/data-engineer/opensearch-ml/" rel="permalink">Reranking - Opensearch
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> February 17 2025</p>
    
    <p class="archive__item-excerpt" itemprop="description">Reranker
문서 검색 과정에서 문서를 임베딩 벡터로 변환하는 과정과 검색 시간 단축을 위해 Approximate Nearest Neighbor search(ANNs)와 같이 근사시키는 기술로 인해 정보 손실이 발생합니다. 이로 인해 필요한 문서가 누락되는 경우가 발생할 수 있...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/github/code-review-action/" rel="permalink">Github Action 코드리뷰 봇 만들기(Gemini-1.5-flash)
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> October 16 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Github Action 설정
Github Action은 사용자가 원하는 트리거에 따라 워크플로우를 실행할 수 있는 CI(Continuous Integration) 도구입니다. 구글의 Gemini-1.5-flash 모델을 사용하여 Pull Request시 코드 변경사항에 대해 LL...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/data-engineer/kafka-pipeline/" rel="permalink">ksqlDB: 실시간 데이터 처리 후 시각화까지
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> September 03 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">ksqlDB
ksqlDB는 Kafka Streams에 기반하는 SQL 엔진입니다. ksqlDB는 Kafka topic에 이벤트 스트리밍 애플리케이션을 구축할 수 있는 쿼리 계층을 제공합니다. Kafka Streams와 달리 ksqlDB는 SQL로 새로운 스트림을 생성하거나 Mate...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/transformer-scratch-implementation-2/" rel="permalink">python으로 Transformer 바닥부터 구현하기[2] (Transformer)
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> August 01 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Objective
앞에서 구현한 LayerNorm, MultiHeadAttention, GELU를 사용하고 이전에 구현해둔 Linear, Dropout, Softmax 클래스를 사용하여 Transformer 클래스를 구현하여 테스트해봅니다.
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 emeraldgoose. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'emeraldgoose/emeraldgoose.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [['$','$']],
  displayMath: [['$$','$$']],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

  </body>
</html>
