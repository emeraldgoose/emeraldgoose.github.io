<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Transformer 정리 - gooooooooooose</title>
<meta name="description" content="Attention is all you need    No more RNN or CNN modules   Attention만으로 sequence 입력, 출력이 가능하다.">


  <meta name="author" content="goooose">
  
  <meta property="article:author" content="goooose">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="gooooooooooose">
<meta property="og:title" content="Transformer 정리">
<meta property="og:url" content="http://localhost:4000/boostcamp/transformer/">


  <meta property="og:description" content="Attention is all you need    No more RNN or CNN modules   Attention만으로 sequence 입력, 출력이 가능하다.">







  <meta property="article:published_time" content="2021-09-16T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/boostcamp/transformer/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "emeraldgoose",
      "url": "http://localhost:4000/"
    
  }
</script>


  <meta name="google-site-verification" content="googleb34ce5276ba6573e" />





  <meta name="naver-site-verification" content="naver93cdc79a5629ab3736d7cf8ff7b51d80">


<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="gooooooooooose Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          gooooooooooose
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo.png" alt="goooose" itemprop="image" class="u-photo" width="110px" height="110px">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">goooose</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Dev blog.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:smk6221@naver.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/emeraldgoose" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Transformer 정리">
    <meta itemprop="description" content="Attention is all you need  No more RNN or CNN modules  Attention만으로 sequence 입력, 출력이 가능하다.">
    <meta itemprop="datePublished" content="2021-09-16T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Transformer 정리
</h1>
          
<!--
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  0 minute read
</p>
            devinlife comments :
                싱글 페이지(포스트)에 제목 밑에 Updated 시간 표기
                기존에는 read_time이 표기. read_time -> date 변경
-->
            <p class="page__date"><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2021-09-16T00:00:00+09:00">September 16, 2021</time></p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="attention-is-all-you-need">Attention is all you need</h2>
<ul>
  <li>No more RNN or CNN modules</li>
  <li>Attention만으로 sequence 입력, 출력이 가능하다.</li>
</ul>

<h2 id="bi-directional-rnns">Bi-Directional RNNs</h2>

<p><img src="https://drive.google.com/uc?export=view&amp;id=17FoHi1TYtgulYYIF_8wxQ0dlBJdJvhXM" alt="" /></p>

<ul>
  <li>왼쪽에서 오른쪽으로 흐르는 방식을 Forward라 하면 오른쪽에서 왼쪽으로 흐르는 것을 Backward라 할 수 있다.</li>
  <li>Forward는 “go”와 왼쪽에 있던 정보까지를 $h_2^f$, Backward는 “go”와 오른쪽에 있던 정보까지를 $h_2^b$라 하자.</li>
  <li>인코딩 벡터는 왼쪽에 있던 정보 뿐만 아니라 오른쪽에 있는 정보도 같이 포함할 수 있도록 Forward RNN과 Backward RNN을 병렬적으로 만든다.</li>
  <li>예를들어 “go”라는 동일단어에 대해서 Forward RNN, Backward RNN의 히든 스테이스트 벡터를 concat하여 히든 스테이트 벡터의 2배에 해당하는 dimension을 구성하도록 한다.</li>
</ul>

<h2 id="transformer-long-term-dependency">Transformer: Long-Term Dependency</h2>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1KwLLispNtOYBguahypHzheqHWxxB2s5q" alt="" /></p>

<ul>
  <li>
    <p>그림 출처 : <a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a></p>
  </li>
  <li>각각의 입력 벡터들은 자기자신을 포함하여 나머지 벡터들과의 내적을 통해 유사도를 구하게 된다. 이 유사도를 softmax에 통과시켜 가중치를 얻게 되고 가중 평균을 구하여 해당 입력벡터의 인코딩 벡터로 사용한다.</li>
  <li>유사도를 구하게 되는 히든 스테이트 벡터들 간에 구별이 없이 동일한 세트의 벡터들 내에서 적용이 될 수 있다는 측면에서 이를 self-attention module이라 부른다.</li>
  <li>입력 벡터들은 각각의 벡터들과의 유사도를 구해서 디코더 히든 스테이트처럼 벡터들 중에 어떤 벡터를 선별적으로 가져올지에 대한 기준이 되는 벡터로 사용이 되는데 이것을 Query라고 표현한다.</li>
  <li>쿼리벡터와 내적을 위한 각각의 재료가 되는 벡터들은 Key 벡터라고 표현한다.</li>
  <li>각각의 벡터들과의 유사도를 구한 후 softmax를 취하게 되어 나온 가중치를 다시 입력벡터에 적용한 벡터를 Value 벡터라고 표현한다.
    <ul>
      <li>또한, query, key, value로 사용될 때에 적용되는 선형변환 매트릭스가 각각 따로 정의되어 있다. 주어진 같은 벡터를 사용할 때 query, key, value로 쓰일 때 다른 벡터로 쓰일 수 있다.</li>
    </ul>
  </li>
  <li>쿼리와 키에 대해 서로 다른 변환이 존재하는 유연한 유사도를 얻어낼 수 있게 된다.</li>
  <li>시퀀스 길이가 길다 하더라도 self-attention 모듈을 적용해서 시퀀스 내 각각의 word들을 인코딩벡터로 만들게 되어 멀리있는 정보도 손쉽게 가져올 수 있다. → RNN에서의 한계점(long-term dependency)을 극복</li>
  <li>자기 자신과 내적을 했을 때 유사도는 큰 값이 나오면서 큰 가중치를 얻게 된다. 이러한 문제를 개선한 모델이 등장한다.</li>
</ul>

<h2 id="transformer-scaled-dot-product-attention">Transformer: Scaled Dot-Product Attention</h2>
<ul>
  <li>inputs : 하나의 쿼리벡터 $q$, key-value의 쌍 $(k, v)$</li>
  <li>output : value 벡터들의 가중 평균</li>
  <li>각각의 가중치는 value벡터와 대응하는 key와의 내적값을 통해서 구한다.</li>
  <li>query와 key는 내적연산이 가능해야 하기 때문에 같은 dimension $d_k$을 가져야 하고 value 벡터의 경우 꼭 같은 dimension을 가지지 않아도 된다.
    <ul>
      <li>왜냐하면 value 벡터는 상수배를 적용하여 가중평균을 내기 때문에 꼭  같지 않아도 실행되는데 문제 없다. 그래서 value의 dimension은 $d_v$를 가지게 된다.</li>
      <li>$A(q,K,V) = \sum_i{\frac{exp(q\cdot k_i)}{\sum_j exp(q \cdot k_j)}}v_i$</li>
      <li>Becomes:
        <ul>
          <li>$A(Q,K,V) = softmax(QK^T)V$</li>
          <li>$(|Q| \times d_k) \times (d_k \times |K|) \times (|V| \times d_v)= (|Q| \times d_v)$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Example from illustrated transformer</li>
</ul>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1MukQ2h5yHD_MmSqUaRwRAjY6Kx7LvdNM" alt="" /></p>

<ul>
  <li>
    <p>그림 출처 : <a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a></p>
  </li>
  <li>입력 벡터가 2개 → Q, K, V 모두 2개의 벡터로 이루어짐</li>
  <li>Problem
    <ul>
      <li>$d_k$가 매우 커서 $q^Tk$의 분산이 매우 커진다.</li>
      <li>벡터의 특정 원소의 값이 매우 커서 softmax를 취하면 그 값 또한 매우 커진다. 이런 문제는 gradient를 작게 한다. → gradient vanishing problem</li>
      <li>예를들어 2차원 벡터의 query와 key가 있다고 가정하자. 그리고 query와 key의 element는 평균 0과 분산 1 을 가지고 있다고 가정하자.</li>
      <li>이때 query와 key의 내적값의 평균이 0이고 내적에 참여하는 곱의 분산은 1로 같게 된다. 따라서 내적의 평균은 0, 분산은 $d_k$가 된다.</li>
      <li>이처럼 내적한 값이 분산에 휘둘리는 형태가 되므로 내적 값의 분산을 일정하게 유지시키기 위해 표준편차 $\sqrt{d_k}$로 나누어준다.</li>
      <li>softmax의 output을 적절한 정도의 범위로 조절하는 것이 학습에 도움이 된다.
        <ul>
          <li>쿼리, 키, 밸류 행렬 가중치 초기화 : Gaussian, Xavier, Kaimin도 가능</li>
          <li><a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a">https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="transformer-multi-head-attention">Transformer: Multi-Head Attention</h2>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1VKZaWHgo1IeXjxBnTS6AwbSQ2C0qWqwH" alt="" /></p>

<ul>
  <li>Head $h$ : number of low-dimensional spaces via $W$ matrices</li>
  <li>h개의 Attention들은 모두 병렬로 작동하며 각각 attention을 적용하고 나온 output들을 concat하고 linear layer로 보낸다.
    <ul>
      <li>MultiHead($Q,K,V$) = Concat($head_1, …., head_h$)$W^O$</li>
      <li>where $head_i$ = Attention($QW_i^Q, KW_i^K, VW_i^V$)</li>
    </ul>
  </li>
  <li>각각의 input은 head에 들어가 Query, Key, Value를 생성하고 거기서 계산된 output들을 Concat한다.</li>
</ul>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1x3H-HQdjVRs9MYRm-9W9Ue-7T4vzXjIq" alt="" /></p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1FdKVa1slEva7VLkk7KEZ9J8YttCob2pV" alt="" /></p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1ElxAzhjX_qcQuYSh1kyjKdGRQLjuTItV" alt="" /></p>

<ul>
  <li>마지막으로 linear trasform을 수행하여 최종 output을 얻어낸다.</li>
  <li>한눈에 보기 쉽게 설명된 이미지이다.</li>
</ul>

<p><img src="https://drive.google.com/uc?export=view&amp;id=18ylIob97ZBF6LHv_2QWWS-5J8PWBWCMe" alt="" /></p>

<ul>
  <li>
    <p>그림 출처 : <a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a></p>
  </li>
  <li>다른 모델과의 complexity를 계산하여 비교한 표이다.
    <ul>
      <li>$n$ : sequence lengh</li>
      <li>$d$ : dimension of representation → hyper parameter</li>
      <li>$k$ : kernel size of convolutions</li>
      <li>$r$ : size of the neighborhood in restriced self-attention</li>
    </ul>

    <p><img src="https://drive.google.com/uc?export=view&amp;id=1fX7st_3Ff9Zfsg7izHfRtmlNxLQK_5UN" alt="" /></p>
  </li>
  <li>Self-Attention
    <ul>
      <li>$softmax(\frac{QK^T}{\sqrt{d}})$이므로 $(n \times d) \times (d \times n)$이다. $n \times n$을 $d$만큼 수행해야 하므로 $O(n^2\cdot d)$의 복잡도를 가지게 된다.</li>
      <li>또한, 자원이 무한하다는 가정하에 모든 값을 메모리에 올리면 한번에 연산이 가능하므로 Sequencial Operations(병렬연산)가 $O(1)$이 된다.</li>
    </ul>
  </li>
  <li>Recurrent
    <ul>
      <li>이전 히든 스테이트 벡터가 다음 타임스텝의 히든 스테이트 벡터 계산에 참여한다. 이전 히든 스테이트 벡터의 dimension이 $d$라 할때,  $h_{t-1}$과 곱해지는 $W_{hh}$는 $(d, d)$의 크기를 가지고 있다. 이 과정을 $n$번만큼 계산하므로 $O(n\cdot d^2)$의 복잡도를 가지게 된다.</li>
      <li>RNN이 Self-Attention보다 적은 메모리를 요구한다. 왜냐하면 $n$이 입력의 길이이므로 Self-Attention은 $n$의 제곱에 해당하는 메모리를 모두 저장하고 있어야 하기 때문이다.</li>
      <li>하지만 RNN은 이전 히든 스테이트를 기다려야 하므로 병렬연산이 불가능하다. 따라서 $O(n)$이다.</li>
    </ul>
  </li>
  <li>Maximum Path Length는 Long-term dependency와 직접적으로 관련이 된다.
    <ul>
      <li>RNN은 첫 번째 단어가 마지막까지 영향을 주려면 $n$ 길이만큼 정보가 전달되어야 한다.</li>
      <li>Self-Attention은 모든 단어를 $Q$, $K$, $V$로 이용해서 정보를 전달하므로 $O(1)$이 된다.</li>
    </ul>
  </li>
</ul>

<h2 id="transformer-block-based-model">Transformer: Block-Based Model</h2>

<p><img src="https://drive.google.com/uc?export=view&amp;id=13Nr0Z-PfLZTQIBwxhCc1Cele3YZYau3a" alt="" /></p>

<ul>
  <li>각 블록은 두 개의 sub-layer로 이루어져 있다.
    <ul>
      <li>Multi-head attention</li>
      <li>TWo-layer feed-forward NN (with ReLU)</li>
    </ul>
  </li>
  <li>sub-layer들은 residual connection을 가지고 있다.
    <ul>
      <li>residual connection은 입력 벡터와 인코딩 벡터의 차이값만을 이용하는 방법인데 gradient vanishing 문제를 해결할 수 있고 학습도 좀 더 안정화 시킬 수 있다.</li>
      <li>residual connection을 사용하려면 입력 벡터의 차원과 아웃풋 벡터의 차원이 일치해야 한다.</li>
    </ul>
  </li>
  <li>Transformer: Layer Normalization
    <ul>
      <li>Layer normalization은 들어온 입력들의 정보를 버리고 평균 0, 분산 1인 정보로 정규화해주는 것을 말한다. 다음 각 레이어 마다 Affine transformation을 수행한다.
  <img src="https://drive.google.com/uc?export=view&amp;id=1D7E5K8rKonPpJhaXs3_Ps8-1Xoxszgbn" alt="" /></li>
    </ul>
  </li>
</ul>

<h2 id="transformer-positional-encoding">Transformer: Positional Encoding</h2>
<ul>
  <li>RNN은 순서대로 입력이 되기 때문에 순서 정보를 넣을 필요가 없다. 그러나 Transformer는 순서 정보를 넣을 수 없기 때문에 Positional Encoding을 넣어준다.</li>
  <li>Positional Encoding은 입력들의 순서정보를 기록해주는 것이다. 이때 주기함수를 사용하여 위치를 표현한다.
    <ul>
      <li>$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$</li>
      <li>$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})$</li>
    </ul>
  </li>
</ul>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1XjqcwpPTRMCzVkHzckMfjcYY81BXTstM" alt="" /></p>

<ul>
  <li>
    <p>그림 출처 : <a href="http://nlp.seas.harvard.edu/2018/04/03/attention">http://nlp.seas.harvard.edu/2018/04/03/attention</a></p>
  </li>
  <li>각 dimension별로 sin 혹은 cos 함수를 사용하여 dimension 개수만큼 그래프가 생성된다. 위 그림에서는 4번째 dimension에서는 sin, 5번째 dimension에서는 cos함수를 사용한다.</li>
  <li>첫 번째 위치(0번)를 zero base의 인덱스로 삼아서 0번에서의 position encoding vector 즉, 입력 벡터에 더해주는 첫 번째 위치를 나타내는 4번째 dimension에서는 파란색 그래프, 5번째 그래프에서는 주황색 그래프로 표현한다.</li>
</ul>

<h2 id="transformer-warm-up-learning-rate-scheduler">Transformer: Warm-up Learning Rate Scheduler</h2>
<ul>
  <li>하이퍼 파라미터인 learning rate를 학습중에 스케줄러를 통해 적절히 변경하여 학습이 잘되도록 한다.
    <ul>
      <li>학습이 되지 않으면 learning rate를 줄이는 방법</li>
    </ul>
  </li>
  <li>learning rate = $ d_{model}^{-0.5} \cdot min(step^{-0.5}, step \cdot \text{wramup_steps}^{-1.5}) $</li>
</ul>

<h2 id="transformer-decoder">Transformer: Decoder</h2>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1GvTrlAHNvlouDiFI79DoJiFMGI3Y8U7g" alt="" /></p>

<ul>
  <li>디코더에 [SOS] 토큰을 넣어주면 Masked Multi-Head Attention을 거쳐 Query를 생성하고 인코더에서 생성된 output을 key와 value로 해서 다시 Multi-Head Attention에 넣어준다.</li>
  <li>이후 나온 값을 Feedforward layer에 넣어 softmax를 취한 값이 출력이 되며 이 출력을 다시 디코더에 삽입하여 다음 단어를 예측하게 한다.</li>
  <li>디코더 안 sub-layer들도 모두 residual connection을 가지고 있다.</li>
</ul>

<h2 id="transformer-masked-self-attention">Transformer: Masked Self-Attention</h2>
<ul>
  <li>Query, Key를 내적하고 softmax를 취한 가중치값은 한 단어와 다른 단어에 대해 어느정도의 유사도를 지니는지를 나타내게 된다.</li>
  <li>하지만 한 단어 뒤의 단어를 예측하는 것은 미래 단어 정보가 필요하지 않다. 그래서 현재 단어에 대해 미래 단어들에 대한 유사도 값을 0으로 만들어주는 후처리과정이 필요하다. 이후 row별로 합이 1이 되도록 normalizing을 진행한다.</li>
  <li>masking이라는 것은 attention을 모두가 볼 수 있도록 허용한 후 보지 말아야 되는 단어들에 대한 가중치를 0으로 만들어 다시 normalization 하는 후처리 방식을 말한다.</li>
</ul>

<h2 id="transformer-experimental-results">Transformer: Experimental Results</h2>

<p><img src="https://drive.google.com/uc?export=view&amp;id=19ZXnUe9j9Kux2N49T1nck57Hkqgu-Upz" alt="" /></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#transformer" class="page__taxonomy-item p-category" rel="tag">transformer</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#boostcamp" class="page__taxonomy-item p-category" rel="tag">BoostCamp</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-09-16T00:00:00+09:00">September 16, 2021</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/boostcamp/attention-mechanism/" class="pagination--pager" title="Encoder-Decoder &amp; Attention mechanism 정리
">Previous</a>
    
    
      <a href="/boostcamp/self-supervised-pretraining-model/" class="pagination--pager" title="Self-supervised Pre-training Models 정리
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/byte-pair-encoding/" rel="permalink">Byte Pair Encoding
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> February 19 2023</p>
    
    <p class="archive__item-excerpt" itemprop="description">Reference
https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/rnn-impl/" rel="permalink">python으로 RNN 구현하기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> December 28 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">Related

  이전 포스트에서 CNN을 구현했고 이번에는 RNN을 구현하는 과정을 정리하려고 합니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/data-engineer/data-pipeline/" rel="permalink">데이터 파이프라인 구축해보기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> December 02 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">Motivation

  빅데이터를 지탱하는 기술을 읽다가 데이터 엔지니어링에 사용되는 플랫폼들을 전체 파이프라인으로 구축해보고 싶어서
이 사이드 프로젝트를 진행하게 되었습니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/cnn-implementation/" rel="permalink">python으로 CNN 구현하기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> September 21 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">Related

  이전 포스트에서 MLP를 구현했고 이번에는 CNN을 구현하는 삽질을 진행했습니다.
여기서는 Conv2d의 구현에 대해서만 정리하려고 합니다. 밑바닥부터 구현하실때 도움이 되었으면 좋겠습니다.

</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 emeraldgoose. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'emeraldgoose/emeraldgoose.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [['$','$']],
  displayMath: [['$$','$$']],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

  </body>
</html>
