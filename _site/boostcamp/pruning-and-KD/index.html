<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Pruning and Knowledge Distillation - gooooooooooose</title>
<meta name="description" content="Pruning">


  <meta name="author" content="goooose">
  
  <meta property="article:author" content="goooose">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="gooooooooooose">
<meta property="og:title" content="Pruning and Knowledge Distillation">
<meta property="og:url" content="http://localhost:4000/boostcamp/pruning-and-KD/">


  <meta property="og:description" content="Pruning">







  <meta property="article:published_time" content="2021-11-29T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/boostcamp/pruning-and-KD/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "emeraldgoose",
      "url": "http://localhost:4000/"
    
  }
</script>


  <meta name="google-site-verification" content="googleb34ce5276ba6573e" />





  <meta name="naver-site-verification" content="naver93cdc79a5629ab3736d7cf8ff7b51d80">


<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="gooooooooooose Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          gooooooooooose
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo.png" alt="goooose" itemprop="image" class="u-photo" width="110px" height="110px">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">goooose</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Dev blog.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:smk6221@naver.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/emeraldgoose" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Pruning and Knowledge Distillation">
    <meta itemprop="description" content="Pruning">
    <meta itemprop="datePublished" content="2021-11-29T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Pruning and Knowledge Distillation
</h1>
          
<!--
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  0 minute read
</p>
            devinlife comments :
                싱글 페이지(포스트)에 제목 밑에 Updated 시간 표기
                기존에는 read_time이 표기. read_time -> date 변경
-->
            <p class="page__date"><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2021-11-29T00:00:00+09:00">November 29, 2021</time></p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              
                <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
                <ul class="toc__menu"><li><a href="#pruning">Pruning</a><ul><li><a href="#structured-pruning">Structured Pruning</a></li><li><a href="#unstructured-pruning">Unstructured Pruning</a></li></ul></li><li><a href="#knowledge-distillation">Knowledge Distillation</a><ul><li><a href="#kd의-핵심">KD의 핵심</a></li><li><a href="#response-based-knowledge-distillation">Response-Based Knowledge Distillation</a></li><li><a href="#feature-based-knowledge-distillation">Feature-Based Knowledge Distillation</a></li><li><a href="#relation-based-knowledge-distillation">Relation-Based Knowledge Distillation</a></li><li><a href="#주의">주의</a></li><li><a href="#추가-논문">추가 논문</a></li></ul></li></ul>

              
            </nav>
            <!-- devinlife comment : right-sidebar ads -->
            <nav class="toc-custom">
              
            </nav>
          </aside>
        
        <h2 id="pruning">Pruning</h2>

<ul>
  <li>Pruning은 중요도가 낮은 파라미터를 제거하는 것</li>
  <li>어떤 단위로 Pruning? → Structured(group) / Unstructured(fine grained)</li>
  <li>어떤 기준으로 Pruning? → 중요도 정하기(Magnitude(L2, L1), BN scaling facgtor, Energy-based, Feature map, …)</li>
  <li>기준은 어떻게 적용? → Network 전체로 줄 세워서(global), Layer 마다 동일 비율로 기준(local)
    <ul>
      <li>global : 전체 n%, 어떤 layer는 많이, 어떤 layer는 적게</li>
      <li>local : 모든 layer를 균일하게 n%</li>
    </ul>
  </li>
  <li>어떤 phase에? → 학습된 모델에(trained model) / initialize 시점에(pruning at initialization)</li>
</ul>

<h3 id="structured-pruning">Structured Pruning</h3>

<ul>
  <li>파라미터를 그룹 단위로 pruning(그룹 단위는 channel/filter/layer level 등이 가능)</li>
  <li>Masked (0으로 pruning된) filter 제거 시 실질적 연산 횟수 감소로 직접적인 속도 향상</li>
  <li>BN
    <ul>
      <li>Learning Efficient Convolutional Networks through Network Slimming(ICCV 2017)</li>
      <li>Scaling factor $\gamma$
        <ul>
          <li>BN의 scaling factor $\gamma$는 Conv의 Out Channel에 곱해지는 형태</li>
          <li>$\gamma$가 크면, 해당 Filter의 weight크기는 일반적으로 클 것</li>
          <li>$\gamma$의 크기를 기준으로 작은 p%의 채널을 pruning</li>
          <li>$\gamma$에 L1-norm을 Regularizer로 사용, Insignificant Channels은 자연스럽게 Prune되도록 유도</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Rank
    <ul>
      <li>Rank를 기준으로 한 논문 : HRank: Filter Pruning using High-Rank Feature Map (CVPR 2020)</li>
      <li>Weight가 아닌 실제 Feature map output을 보자!</li>
      <li>Feature map output의 각 Filter에 SVD를 적용, Rank를 계산</li>
      <li>이미지에 따라 Feature map output은 당연히 달라지므로, 그때마다 SVD rank 개수는 달라지는 것 아닌가?
        <ul>
          <li>각 다른 Batch 이미지들로 Feature map output을 계산, Rank를 구했을 때, 차이가 없음을 실험적으로 보임</li>
        </ul>
      </li>
      <li>Rank 계산을 구현할 때는 <code class="language-plaintext highlighter-rouge">torch.matrix_rank()</code> 함수를 사용하면 된다.</li>
    </ul>
  </li>
</ul>

<h3 id="unstructured-pruning">Unstructured Pruning</h3>

<ul>
  <li>파라미터 각각을 독립적으로 Pruning</li>
  <li>Pruning을 수행할 수록 네트워크 내부의 행렬이 점점 희소(Sparse)해짐</li>
  <li>Structured Pruning과 달리 Sparse Computation에 최적화된 소프트웨어 또는 하드웨어에 적절한 기법
    <ul>
      <li>Sparse computation을 지원하는 소프트웨어 혹은 하드웨어에서만 속도 향상이 있고 그 외에는 없음</li>
      <li>왜나하면, 각 레이어의 일부 가중치만 0이 될 뿐이고 실질적은 레이어 개수는 변함이 없음</li>
    </ul>
  </li>
  <li>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks(ICLR 2019)
    <ul>
      <li>Prune된 sparse한 모델들은 일반적으로 초기치로부터 학습이 어려움</li>
      <li>Lottery Ticket Hypothesis
        <ul>
          <li>Dense, randomly-initialized, feed-forward net은 기존의 original network와 필적하는 성능을 갖는 sub networks(winning tickets)를 갖는다.</li>
        </ul>
      </li>
      <li>이 Lottery Ticket(sub network)을 찾는 한 방법을 제안함</li>
      <li>Identifying winning tickets
        <ol>
          <li>네트워크 임의로 초기화 $f(x;\theta_0)$</li>
          <li>네트쿼으 j번 학습하여 파라미터 $\theta_j$를 도출</li>
          <li>파라미터 $\theta_j$를 $p$%만큼 Pruning하여 mask $m$을 생성($p$는 보통 20%)</li>
          <li>Mask되지 않은 파라미터를 $\theta_0$로 되돌리고(초기화하고), 이를 winning ticket이라 지칭 $f(x;m \odot\theta_0)$</li>
          <li>Target sparsity에 도달할 때까지 2-4 반복</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>Stabilizing the Lottery Ticket Hypothesis(arXiv 2019): Weight Rewinding
    <ul>
      <li>LTH의 경우 데이터셋 또는 네트워크의 크기가 커졌을 때 불안정한 모습을 보임</li>
      <li>k번째 epoch에서 학습한 파라미터로 네트워크를 초기화 하면 학습이 안정화 됨을 보임</li>
      <li>논문에서는 총 iteration의 0.1% ~ 7% 정도의 지점을 rewinding point로 언급</li>
    </ul>
  </li>
  <li>Comparing Rewinding And Fine-tuning In Neural Network Pruning(ICLR 2020): Learning Rate Rewinding
    <ul>
      <li>Weight rewinding 대신, weight는 그대로 유지하고, 학습했던 Learning rate scheduling을 특정 시점(k)로 rewinding하는 전략을 제안</li>
      <li>어느 시점의 weight를 rewind할지에 대한 파라미터 고민 없이, Learning rate를 0으로 설정한 후 다시 학습하면 대체로 좋은 성능을 보임</li>
    </ul>
  </li>
  <li>LTH 관련 기타 추천 논문
    <ul>
      <li>왜 잘되는지 아직 명쾌히 설명되지 않음(but RL, NLP 등등 다양한 분야에서 적용이 가능함)</li>
      <li>특정 initialization에 대해서, 학습 후에 도달하는 공간이 유사하다? → “네트워크 학습 및 수렴” 관련 연구와 접목되는 트렌드</li>
    </ul>
  </li>
  <li>Linear Mode Connectivity and the Lottery Ticket Hypothesis(ICML 2020)
    <ul>
      <li>네트워크의 학습 및 수렴 관련된 실험</li>
      <li>특정 학습 시점(epoch at 0, k)에서 seed를 변경하여 두개의 Net을 학습 → SGD를 통한 계산 결과가 다르므로, 다른 곳으로 수렴</li>
      <li>둘 간의 weight를 Linear interpolation하여, 성능을 비교</li>
      <li>두 weight 공간 사이의 interpolated net들의 성능을 확인</li>
      <li>특정 시점부터 네트워크가 수렴하는 공간은 한 plateau 위에 있는것이 아닌가?라는 해석을 제시</li>
    </ul>
  </li>
  <li>Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask(NeurIPS 2019)
    <ul>
      <li>LTH는 $w_f$의 L1 norm으로 줄을 세워서 masking하는데 $w_i$를 고려하면?</li>
      <li>$w_i, w_f$를 함께 고려하면?(NLP 쪽의 movement pruning처럼)</li>
    </ul>
  </li>
  <li>Pruning at Initialization(unstructured)
    <ul>
      <li>Train이전에 “중요도”를 보고 Pruning을 수행하자. 그 후 학습하면 시간이 훨씬 절약된다.</li>
      <li>중요도는 어떻게 계산?
        <ul>
          <li>SNIP(ICLR 2018): Training 데이터 샘플, Forward해서 Gradient와 Weight의 곱의 절댓값으로!</li>
          <li>GraSP(ICLR 2019): Training 데이터 샘플, Forward해서 Hessian-gradient product와 Weight의 곱으로!</li>
          <li>SynFlow(NeurIPS 2020): 전부 1로 된 가상 데이터를 Forward해서 Gradient와 Weight의 곱으로!</li>
        </ul>
      </li>
      <li>네트워크의 가능성을 본다 → Training Free NAS/AutoML Zero-const proxies for Lightweight NAS(ICLR 2021)
        <ul>
          <li>Pruning at initialization의 기법들을 일종의 score로 network를 정렬</li>
          <li>각 기법의 score와 실제 학습 결과의 상관관계(Spearman 상관계수)를 확인</li>
          <li>생각보다 score와 실 학습 결과의 상관 계수가 높고, voting을 하면 더욱 높다</li>
          <li>Synflow가 CV외 여러 task(NLP, ASR)에도 높은 상관계수를 보여줌</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>아쉬운점
    <ul>
      <li>Structured/Unstructured 모두 간단한 Architecture로 벤치마크를 수행(Modern architecture는 잘 다루지 않음)</li>
      <li>다양한 Architecture에 대해서 여러 예외처리, 추가 검증이 필요함</li>
      <li>파라미터 수, FLOPs로 비교를 많이 하는 편이나 실질적 latency 향상 정도와는 차이가 있음</li>
      <li>Unstructured는 특히 아직 가속되는 HW가 많지 않음</li>
    </ul>
  </li>
</ul>

<h2 id="knowledge-distillation">Knowledge Distillation</h2>

<h3 id="kd의-핵심">KD의 핵심</h3>

<blockquote>
  <p>Teacher의 정보를 어떻게 빼내는가?</p>

</blockquote>

<h3 id="response-based-knowledge-distillation">Response-Based Knowledge Distillation</h3>

<ul>
  <li>Teacher model의 last output layer를 활용하는 기법, 즉 직접적인 final prediction을 활용</li>
  <li>대표적으로 hinton loss가 있음</li>
</ul>

<h3 id="feature-based-knowledge-distillation">Feature-Based Knowledge Distillation</h3>

<ul>
  <li>Teacher의 layer의 중간 중간의 intermediate representations(feature)를 student가 학습하도록 유도</li>
  <li>$F_t(x)$,  $F_s(t)$를 각각 teacher, student의 feature map이라 하고 $T_t$, $T_s$를 각각 techer, student의 transformation function이라 하자.</li>
  <li>Distance $d$가 있을 때, feature based KD의 loss는 아래처럼 정의된다.
    <ul>
      <li>$L_{distill} = d(T_t(F_t), T_s(F_s))$</li>
    </ul>
  </li>
  <li>가장 간단하게는 $d$은 L2 norm, $T_t$는 identity, $T_s$는 learnable한 linear transformation matrix로 정의한다.
    <ul>
      <li>feature channel 수가 보통은 teacher가 많으므로, 이를 맞춰주기 위해 도입</li>
    </ul>
  </li>
  <li>논문들의 주된 방향은, Feature distillation 과정에서 유용한 정보는 가져오고, 중요하지 않은 정보는 가져오지 않도록 Transformation function과 distance, 위치 등을 조정하는 것
    <ul>
      <li>중간 결과를 가져오므로, network 구조에 크게 의존함</li>
    </ul>
  </li>
</ul>

<h3 id="relation-based-knowledge-distillation">Relation-Based Knowledge Distillation</h3>

<ul>
  <li>다른 레이어나 Sample들 간의 관계를 정의하여 Knowledge distllation을 수행</li>
  <li>Relation among examples represented by teacher to student</li>
</ul>

<h3 id="주의">주의</h3>

<ul>
  <li>재현이 잘 되지 않는다는 주장을 하는 논문이 다수 존재</li>
  <li>Contrastive representation distillation, ICLR 2019</li>
  <li>NLP의 경우 Transformer, Transfer Learning이 main stream이어서 KD 연구가 활발</li>
</ul>

<h3 id="추가-논문">추가 논문</h3>

<ul>
  <li>크기가 문제가 아닌 성능이 문제라면?</li>
  <li>Self-training with noisy student improves imagenet classification(CVPR 2019)
    <ul>
      <li>아래와 같은 방식을 사용
        <ul>
          <li>Train a teacher model on labeled images</li>
          <li>Use the teacher to generate pseudo labels on unlabeled images</li>
          <li>Train a student model on the combination of labeled images and pseudo labeled images</li>
        </ul>
      </li>
      <li>Student를 다음 iteration에서의 teacher로, student는 유지하거나 점점 키움</li>
      <li>Unlabeled 데이터를 사용하지만, 당시 ImageNet SOTA 기록</li>
      <li>KD와의 차이점?
        <ul>
          <li>Noise를 추가(input noise: RandAug, Model noise: dropout, stochastic depth function)</li>
          <li>Student가 점점 커지는 framework</li>
          <li>Soft target을 사용한다는 점에서 KD와의 유사성 존재</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#knowledge-distillation" class="page__taxonomy-item p-category" rel="tag">knowledge Distillation</a><span class="sep">, </span>
    
      <a href="/tags/#lightweight" class="page__taxonomy-item p-category" rel="tag">lightweight</a><span class="sep">, </span>
    
      <a href="/tags/#pruning" class="page__taxonomy-item p-category" rel="tag">pruning</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#boostcamp" class="page__taxonomy-item p-category" rel="tag">BoostCamp</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-11-29T00:00:00+09:00">November 29, 2021</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/boostcamp/automl-augmentation/" class="pagination--pager" title="좋은 모델 찾기
">Previous</a>
    
    
      <a href="/boostcamp/lightweight-bert/" class="pagination--pager" title="BERT 경량화
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/dl-implement/" rel="permalink">최대한 기본 라이브러리로만 딥러닝 구현하기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> July 04 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">Motivation

  모기업 코딩테스트에 파이썬 기본 라이브러리로만 MLP를 구현하는 문제가 나왔던 적이 있습니다. 당시에 학습이 되지 않아 코딩테스트에서 떨어졌었고 구현하지 못했던 것이 계속 생각났었습니다.
numpy로 구현한 코드는 많았지만 numpy도 사용하지 않고 구현한...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/paper/On-the-effect-of-corpora/" rel="permalink">On the Effect of Pretraining Corpora on In-context Learning by a LLM
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> June 06 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">
  모두의연구소에서 논문저자가 직접 논문을 리뷰해주는 세미나가 열렸습니다. 주제가 재밌어 보여 발표를 듣고 논문을 다시 읽어 리뷰해보려고 합니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ops/kubenetes-overview/" rel="permalink">Kubernetes
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> May 15 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">컨테이너 기반 배포
애플리케이션 배포 방식은 물리적인 컴퓨터에 OS와 APP을 설치하여 서비스하던 방식에서 가상화 배포 방식으로 변화했습니다. 가상화 방식은 가상머신 성능을 각각 관리하면서 자원 상황에 따라 조절할 수 있습니다. 그러나 가상머신마다 OS를 새로 설치해야 하고 용량 ...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/atcoder/abc250/" rel="permalink">AtCoder Beginner Contest 250
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> May 10 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">A. Adjacent Squares
(H, W) 크기의 행렬이 주어졌을 때 위치 (R, C)에서 인접한 원소의 수를 출력하는 문제이다.
H와 W가 1인 경우를 예외처리해주면 쉽게 풀 수 있다.
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 emeraldgoose. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'emeraldgoose/emeraldgoose.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [['$','$']],
  displayMath: [['$$','$$']],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

  </body>
</html>
