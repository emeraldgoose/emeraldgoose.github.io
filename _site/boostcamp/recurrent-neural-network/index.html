<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Basics of Recurrent Nerual Networks (RNN) - gooooooooooose</title>
<meta name="title" content="Basics of Recurrent Nerual Networks (RNN)">
<meta name="description" content="Types fo RNNs    각 타임스텝에서 들어오는 입력벡터 $X_t$와 이전 state에서 계산된 $h_{t-1}$을 입력으로 받아서 현재 타임스텝에서의 $h_t$를 출력으로 내어주는 구조를 가지고 있다.   모듈 A가 재귀적으로 호출되는 구조를 가지고 있다. 왼쪽의 그림을 rolled 버전, 오른쪽 그림을 unrolled 버전이라 한다.">


  <meta name="author" content="goooose">
  
  <meta property="article:author" content="goooose">
  


<!-- open-graph tags -->
<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="gooooooooooose">
<meta property="og:title" content="Basics of Recurrent Nerual Networks (RNN)">
<meta property="og:url" content="http://localhost:4000/boostcamp/recurrent-neural-network/">


  <meta property="og:description" content="Types fo RNNs    각 타임스텝에서 들어오는 입력벡터 $X_t$와 이전 state에서 계산된 $h_{t-1}$을 입력으로 받아서 현재 타임스텝에서의 $h_t$를 출력으로 내어주는 구조를 가지고 있다.   모듈 A가 재귀적으로 호출되는 구조를 가지고 있다. 왼쪽의 그림을 rolled 버전, 오른쪽 그림을 unrolled 버전이라 한다.">







  <meta property="article:published_time" content="2021-09-09T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/boostcamp/recurrent-neural-network/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "emeraldgoose",
      "url": "http://localhost:4000/"
    
  }
</script>


  <meta name="google-site-verification" content="googleb34ce5276ba6573e" />





  <meta name="naver-site-verification" content="naver93cdc79a5629ab3736d7cf8ff7b51d80">


<!-- end _includes/seo.html -->




<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/" itemprop="item"><span itemprop="name">Home</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#boostcamp" itemprop="item"><span itemprop="name">Boostcamp</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Basics of Recurrent Nerual Networks (RNN)</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo.png" alt="goooose" itemprop="image" class="u-photo" width="110px" height="110px">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">goooose</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in ML/DL, Data Engineering.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:smk6221@naver.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://github.com/emeraldgoose" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
          
            <li><a href="https://www.linkedin.com/in/minseong-kim-84428b231/" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://gooooooooooose.tistory.com/" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Problem Solving OJ (KOR)</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Basics of Recurrent Nerual Networks (RNN)">
    <meta itemprop="description" content="Types fo RNNs  각 타임스텝에서 들어오는 입력벡터 $X_t$와 이전 state에서 계산된 $h_{t-1}$을 입력으로 받아서 현재 타임스텝에서의 $h_t$를 출력으로 내어주는 구조를 가지고 있다.  모듈 A가 재귀적으로 호출되는 구조를 가지고 있다. 왼쪽의 그림을 rolled 버전, 오른쪽 그림을 unrolled 버전이라 한다.">
    <meta itemprop="datePublished" content="2021-09-09T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Basics of Recurrent Nerual Networks (RNN)
</h1>
          
<!--
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  0 minute read
</p>
            devinlife comments :
                싱글 페이지(포스트)에 제목 밑에 Updated 시간 표기
                기존에는 read_time이 표기. read_time -> date 변경
-->
            <p class="page__date"><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2021-09-09T00:00:00+09:00">September 09, 2021</time></p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="types-fo-rnns">Types fo RNNs</h2>
<ul>
  <li>각 타임스텝에서 들어오는 입력벡터 $X_t$와 이전 state에서 계산된 $h_{t-1}$을 입력으로 받아서 현재 타임스텝에서의 $h_t$를 출력으로 내어주는 구조를 가지고 있다.</li>
  <li>모듈 A가 재귀적으로 호출되는 구조를 가지고 있다. 왼쪽의 그림을 rolled 버전, 오른쪽 그림을 unrolled 버전이라 한다.</li>
</ul>

<h2 id="recurrent-neural-network">Recurrent Neural Network</h2>

<ul>
  <li>$h_{t-1}$ : old hidden-state vector</li>
  <li>$x_t$ : input vector at some time step</li>
  <li>$h_t$ : new hidden-state vector</li>
  <li>$f_W$ : RNN function with parameters $W$
    <ul>
      <li>파라미터 $W$는 모든 타임 스텝에서 공유되는 것이 RNN의 특징</li>
    </ul>
  </li>
  <li>$y_t$ : output vector at time step $t$
    <ul>
      <li>만약 출력해야 하는 y가 품사인 경우 매 타임스텝마다 단어의 품사를 출력해야 하고 만약 문장의 sentiment를 출력해야 하는 경우(positive, negative) 맨 마지막 state에서 출력해야 한다.</li>
    </ul>
  </li>
  <li>hidden state $h_t = f_W(h_{t-1},x_t)$
    <ul>
      <li>$h_t=f_W(h_{t-1},x_t)=tanh(W_{hh}h_{t-1}+W_{xh}x_t)$</li>
      <li>$y_t=W_{hy}h_t$</li>
    </ul>
  </li>
  <li>예를들면, $x_t = (x,1)$와 $h_t = (h,1)$이라고 가정하면
    <ul>
      <li>$W = (h, x+h)$인 크기를 갖게 되고 $W$의 $(h,x)$는 $X_t$와 내적하고 $W$의 나머지 부분인 $(h,h)$는 $h_t$와 내적하게 된다.</li>
      <li>$W_{xh}$는 $X_t$를 $h_t$로 변환해주는 역할을 하고 $W_{hh}$는 $h_{t-1}$을 $h_t$로 변환해주는 역할을 한다. 이렇게 계산된 값을 비선형 변환 중 하나인 $tanh$을 통과시켜준다.</li>
      <li>$W_{hy}$는 $h_t$를 $y_t$로 변환해주는 역할을 하는 것으로 이해할 수 있다.</li>
      <li>여기에 sigmoid를 씌우게 되면 binary classification을 수행할 수 있고 multi class classification에서는 $y_t$가 class개수만큼의 dimension을 가지는 벡터로 나와서 softmax 레이어를 통과해서 분류하고자 하는 클래스와 동일한 개수만큼의 확률분포를 얻을 수 있게 된다.</li>
    </ul>
  </li>
  <li>Types of RNNs
    <ul>
      <li>one to one : Standard Nerual Networks</li>
      <li>one to many : Image Captioning
        <ul>
          <li>one to many의 경우 입력이 들어가지 않게 하기 위해 입력과 동일한 크기의 0으로 채워진 입력을 넣어준다.</li>
        </ul>
      </li>
      <li>many to one: Sentiment Classification</li>
      <li>many to many(left) : Machine Translation</li>
      <li>many to many(right) : Video classification on frame level, POS</li>
    </ul>
  </li>
</ul>

<h2 id="character-level-language-model">Character-level Language Model</h2>

<ul>
  <li>문자열이나 단어들의 순서를 바탕으로 다음 단어가 무엇인지 맞추는 모델</li>
  <li>sequence : “hello”
    <ul>
      <li>Vocabulary : [h, e, l, o] → one-hot vector : [1, 0, 0, 0], [0, 1, 0, 0], …</li>
      <li>$h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t + b)$</li>
      <li>Logit = $W_{hy}h_t + b$
        <ul>
          <li>선형변환을 통과하여 사전의 크기와 동일한 output vector가 나오고 multi class classification을 수행하기 위해 softmax layer를 통과하게 된다.</li>
          <li>“h”의 예측값은 “o”이지만 ground truth는 ‘e’이므로 softmax loss를 통해 ‘e’에 가까워지도록 학습하게 된다.</li>
        </ul>
      </li>
      <li>3번째와 4번째는 같은 입력이 들어오지만 다음으로 올 문자는 달라야 한다. 이것이 가능한 이유는 이전 타임스텝에서 넘어오는 정보가 있기 때문이다.</li>
    </ul>
  </li>
  <li>문단에 대해서도 학습을 할 수 있는데 이때 문장안에 빈칸이나 특수문자도 Vocab의 dimension을 차지하게 된다.</li>
</ul>

<h3 id="backpropagation-through-time-bptt">Backpropagation through time (BPTT)</h3>

<ul>
  <li>임의의 타임스텝 $t$에서 $W_{hh}$, $W_{xh}$, $W_{hy}$ 모두 backpropagtion을 수행하게 되는데 이것이 많아질수록 메모리 한계가 올 수 있다. 이를 예방하기 위해 일부를 잘르는 truncation작업을 통해 제한된 길이의 sequence를 학습시키는 방법을 사용하고 있다.</li>
</ul>

<h3 id="searching-for-interpretable-cells">Searching for Interpretable Cells</h3>

<ul>
  <li>RNN에서 필요로하는 정보를 저장하는 공간은 매 타임스텝마다 업데이트를 수행하는 hidden state vector $h_t$이다.</li>
  <li>필요한 정보가 hidden state vector의 어디에 저장되었는지 확인하려면 vector의 각각의 차원 하나를 고정?하여 타임스텝가 진행됨에 따라 어떻게 변하는지를 분석함으로써 RNN의 특성을 확인할 수 있다.</li>
</ul>

<h3 id="character-level-language-model-1">Character-level Language Model</h3>

<p><img src="https://lh3.googleusercontent.com/fife/ALs6j_FebZol2fq2T7B8tkPNf6hNih7D2NPA2PdQxiAlyb2mWMx7VrIGXXCzXVmh2yiGs64Ly2nBMp3TRALQ3QNZtPQjqEBCbsZ0neosdw-rvDvXagpeoSkhhFyWyCOnp2Ia0rxH3-FKZ3GyhYsy9u1aa5JcCowtp5WLt-S253dUV4M0Aea5AHPPtCiUvkNmEJXdrXrkTsG2aVCbUYvXFivK1-bGEQA_XBvX7B6B_8_aNcXBZmggKMfr-lMWHo5Xotljyfh8VElsbk7OzDppaBE2c6Pa4hkcc2tY94gfpStkZa0Cw5ygdsq8LaqIhDaI0laFGYUkzFfsKzRZg_MpOYPS39FicNOCn3hLt0AlrfNc_ZP6DPCL4BvugEfbxMD235B1XFeP6yWwH-sqVFGRnfHPmu09lmoNHo_s64YnYLTuFTImFX2ncyRQOsn68sbn8R8D-A-l5M6Ediygt-1ODexfxHgCFWK-t6_g3Rl8fOP17Jr-Hpe0HsSCuzCNJ4DTkCzxNXA4YrwuIFjfRMyofu6O12mDIXU4ikedD-evr_y85f3RJ60j7J5yKXuPHz4HRCZ0IrlMqv2-w_25yj4KcWlvFytcs5v5R6UiK9WoCYYkITDyaiR58z5bhYBXw8CsAEKThaLjFWhdNTXlOsY15aRwybfhj7nJFEgOrA16700RpWk0OIg7Fys09Hgbq1Yui0ntlAo1weZDDt-pmorRORcooxhYQoO70muVNHdUwoNCPkbzfi9ArLrlGDxfdPvj6nOEiscf9sQSbrEWLJfcCcEAERts3fEejk1fcWzhxJjVia07sh4p78hed7arA8ubdduou3n803MSG5KY7vlOiQHjfsl0GzJKYkaTX8ukI_xsjj4FdiTC0QC8CfXcZgM0ph3dI2SGoZHqO02NObxu5Sbf5ZFBylS2wURuxv8SxGbG0u8rPBNwFHQjlLtoKceThgkuF2MCe_BGCWnrbcJNCKnwDUXrLA26B3AFM0QBixVMEpn0UhbvzU5QY7EgwF9YM7JnjRQq-HgUHRPHE96C1EZo2KPeNAHatQ9eu0q1cccFwL5aK54PE-kveM7xt9OSHjvfWBfLgGUizZaGe2Vn6GL6lJcAioFapS0qItKAlIWWy5EYI8hi-bTudS5zC7izKIN5HXRiUhe_Gg7kzErXel0k4Zo1Y2gpbqBO1IgSaV9XOwKnMXbgpZT8RFKc5Bs-tP0BOG_cAWQJo4bKgpLQ6lKdmZomAADc8hDmBZLyPJo7vLmb2wwfcl6POG2l4b7Cttuzjcb9zATNRfg55E_jWnBOfN8Qoyvs81xRdD_-0JR4ViCVR1iaWN2yQjDdy0KHFRIeyIJiui7CUTuK4jjogzbsdFURHgKYB_5w2Ba7RMSFlcAAc_Wkh_Io6thNuNn7cDX_8fdx0ooGmTVkJFJW8eUOEu1jO-qyxOlrt-etxusyI3Smy-kRJIFeXlc_0fA03b6NDgm8_L-aA9jEmdjMl17Va74I7Qef8xXpjahTOmT7-0nuz1opp-ATuvk8gAUH7HZgNAEN4DIa1nkU8RpM9RAVoHHOiItUWQmeOJJ1OX6QlxpGRg1WdiZveh0Q9ZzYb-1nXBrqXPeRfEB-hjB3wjhkHeIXC7-NsC0bOQ6N2XEno_LtqPdY" alt="" /></p>

<ul>
  <li>특정한 dimension의 노드 state가 어떻게 변하는지 시각화한 모습이다.</li>
</ul>

<p><img src="https://lh3.googleusercontent.com/fife/ALs6j_GKx6ijvX19ueovyIE6TDIqkcGnequ8eajSB6p7FPQjxTS9KGWonkqooDl3wKnvvBYjgdrJT_ep3RwBzwtg5pQ7_sGONQcFEROv4qHW-FjX0xz7aS-4u5CIPnYMzmJSvy8IZFVXH3cpoSyDgVxEKe4Is8izhXRtf-m4aFcepWk0Wqg59ipbTYeb30i1VUEZt-3XWQZ84NeVuMqxhzHGW8qEz54v8NEIKWxN73dKVBbBVLNopSuy-NuzsVz8nOXNuAKkUipse3S-rY9YfulOKdgcqEuZ0Fq2iYddwKiZJcl7GzxC9rYvxV9u5Vnp5eAI7hi8vLHNzjKZPbpa0zowjQyy68W4NSqsmH4HZY_LZc6Qs-zmzk24BvDJN3algYlYXIuBscshKgB_fTXNMsBw9bheoaMOLarrNPh1n0vH_OH4tECjjYQkvedumLT5w9M0OzVptyF9_M8mDoBCowEFFe3Josdj4QcQ_GouQDOxqVQ2rK-UoUAFJbF_KT_xRbkPyY50ErGkzc1wx5DbvJVT7wHb57uZYhDblS0hMoMDFtFRjpzYm15YU1NO43uC3Xa1Lpna_2HEbGUP3rNP5SxzUOwMt5GlKXO0hjLljdKgUGaKSngLKY9o91I1Nij_3Lqtj7sCxEIn8HtKKIcC0olvBKEv7k7_zE4MspYtmFn-07enzE2jmYUjPXr47xPJWmq9JqFmwaPfEHfPJ-Oa8FtO7ivb4cFw-xYDA6qHr6vI47M5X5iDSoN5ip8IrQv5g9DegLPb7mKtZwV0vKdemZrLoZmjwHYdy4tw3hdiQpNoix2cf8T9w2PNGppBUe0BtC9xjIIuKTBpDSdghDFFmc6nTIqBks29QzU6pQmw3Mh5FER9aKTcRYnyUrS63LNTU_l1rxUVQfDGiYBw30NfN3tmk-wCY-Cf9_Qnq47geIWzJyD3kpRfdPN37x27CJSJXAxxUVHBTN1FK9zkKWATIgspMVdYgB-DBCaDNwl0FJEeqMRQf5NqqijYvLnJ6dIs-_dTyy6F8Mf3UCgxxxDowWvL8WRB6CAkLCGoo28bFw6u3iAO0GV9r7iHpov9A-Ry82H_Q9N6dXq3UKrUN8QbqFEJOlLMpNXrAgitS_bsL1bLe_bEHZ2z36IbRS2e61QxR_-2_4zI3T2M4JjMpSlsXJPUDWuCJ3JMUP7MYFLadwonRqDNVhAArZADapX5MKoZkr80ywYQb9V03B093Nck73SQj4Ef7nqcK54a2OKIWSp8uNN3WhOg_ke_NXFXHUO9P09zsWSuLoMjFy7pO2IUY0uz7j8O3xEgU0Hioc4O0oQyqn1fPcnS8vGHb7_pUIuAX3HGcN6CMw2myFuiG14RSiEGYyModAEjz2LjMelVgXHmrb3KuxYFhdeid6knMuJKau7Jt-qgnlKA3Z7h23FgvCMAzfyoqFo9jSwukuA2RS6PVIeGnVDdq2Df0FIjgctMs7v_eOj9VjbUQkrtUtnPRLWPCeNKujPGEqJTSRjH79PMGqCri3krmFpTExbinvjYDnrAOLbRWKKgTAW-JSU4gDuqYSZEu942GXjuIg36CT2f5Hby81R9svw7f2QDeAEPpLE1SwBpqYpmH09ZAT23LW92fOoHTTO0G8olQDCrAfK1SBI1VP6U" alt="" /></p>

<ul>
  <li>
    <p>“가 열릴때 음수였다가 “가 닫히면서 양수, 다시 열리면 음수가 되는 것을 보면 이 모델은 “의 정보를 계속 기억하고 있음을 볼 수 있다.</p>
  </li>
  <li>
    <p>그림 출처 : <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p>
  </li>
</ul>

<h3 id="vanishingexploding-gradient-problem-in-rnn">Vanishing/Exploding Gradient Problem in RNN</h3>

<ul>
  <li>RNN은 동일한 matrix를 매번 곱하게 backpropagation의 결과가 매우 작아지거나 매우 커지는 경우를 볼 수 있다.</li>
  <li>간단한 예시:
    <ul>
      <li>$h_t = tahn(W_{xh}x_t + W_{hh}tahn(W_{xh}x_{t-1}+W_{hh}tahn(W_{xh}x_{t-2}+W_{hh}h_{t-2} + b) + b) + b)$</li>
      <li>$h_t$에서 backpropagation을 위해 편미분을 실행하게 되는데 타임스텝를 거슬러 올라갈수록 점점 증폭된다. 이때 이전 타임스텝의 유의미한 값을 전달해줄 수 없게 된다.</li>
    </ul>
  </li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#nlp" class="page__taxonomy-item p-category" rel="tag">nlp</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#boostcamp" class="page__taxonomy-item p-category" rel="tag">BoostCamp</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-09-09T00:00:00+09:00">September 9, 2021</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/boostcamp/long-short-term-memory/" class="pagination--pager" title="LSTM &amp; GRU 정리
">Previous</a>
    
    
      <a href="/boostcamp/attention-mechanism/" class="pagination--pager" title="Encoder-Decoder &amp; Attention mechanism 정리
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/github/code-review-action/" rel="permalink">Github Action 코드리뷰 봇 만들기(Gemini-1.5-flash)
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> October 16 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Github Action 설정
Github Action은 사용자가 원하는 트리거에 따라 워크플로우를 실행할 수 있는 CI(Continuous Integration) 도구입니다. 구글의 Gemini-1.5-flash 모델을 사용하여 Pull Request시 코드 변경사항에 대해 LL...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/data-engineer/kafka-pipeline/" rel="permalink">ksqlDB: 실시간 데이터 처리 후 시각화까지
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> September 03 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">ksqlDB
ksqlDB는 Kafka Streams에 기반하는 SQL 엔진입니다. ksqlDB는 Kafka topic에 이벤트 스트리밍 애플리케이션을 구축할 수 있는 쿼리 계층을 제공합니다. Kafka Streams와 달리 ksqlDB는 SQL로 새로운 스트림을 생성하거나 Mate...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/transformer-scratch-implementation-2/" rel="permalink">python으로 Transformer 바닥부터 구현하기[2] (Transformer)
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> August 01 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Objective
앞에서 구현한 LayerNorm, MultiHeadAttention, GELU를 사용하고 이전에 구현해둔 Linear, Dropout, Softmax 클래스를 사용하여 Transformer 클래스를 구현하여 테스트해봅니다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/transformer-scratch-implementation-1/" rel="permalink">python으로 Transformer 바닥부터 구현하기[1] (MultiHead-Attention, LayerNorm, GELU)
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> July 31 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Transformer
트랜스포머(Transformer)는 2017년에 등장한 모델입니다. Attention is all you need 논문은 트랜스포머 모델에 대해 설명하고 있으며 이후 BERT, GPT라는 새로운 모델을 탄생시키는 배경이 됩니다.
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 emeraldgoose. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'emeraldgoose/emeraldgoose.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [['$','$']],
  displayMath: [['$$','$$']],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

  </body>
</html>
