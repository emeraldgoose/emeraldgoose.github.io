<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Self-supervised Pre-training Models 정리 - gooooooooooose</title>
<meta name="title" content="Self-supervised Pre-training Models 정리">
<meta name="description" content="Recent Trends">


  <meta name="author" content="goooose">
  
  <meta property="article:author" content="goooose">
  


<!-- open-graph tags -->
<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="gooooooooooose">
<meta property="og:title" content="Self-supervised Pre-training Models 정리">
<meta property="og:url" content="http://localhost:4000/boostcamp/self-supervised-pretraining-model/">


  <meta property="og:description" content="Recent Trends">







  <meta property="article:published_time" content="2021-09-17T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/boostcamp/self-supervised-pretraining-model/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "emeraldgoose",
      "url": "http://localhost:4000/"
    
  }
</script>


  <meta name="google-site-verification" content="googleb34ce5276ba6573e" />





  <meta name="naver-site-verification" content="naver93cdc79a5629ab3736d7cf8ff7b51d80">


<!-- end _includes/seo.html -->




<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>

<!-- Lightbox2 -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/css/lightbox.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/js/lightbox-plus-jquery.js" integrity="sha512-oaWLach/xXzklmJDBjHkXngTCAkPch9YFqOSphnw590sy86CVEnAbcpw17QjkUGppGmVJojwqHmGO/7Xxx6HCw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/js/lightbox-plus-jquery.min.js" integrity="sha512-U9dKDqsXAE11UA9kZ0XKFyZ2gQCj+3AwZdBMni7yXSvWqLFEj8C1s7wRmWl9iyij8d5zb4wm56j4z/JVEwS77g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/" itemprop="item"><span itemprop="name">Home</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#boostcamp" itemprop="item"><span itemprop="name">Boostcamp</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Self-supervised Pre-training Models 정리</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo.png" alt="goooose" itemprop="image" class="u-photo" width="110px" height="110px">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">goooose</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in ML/DL, Data Engineering.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:smk6221@naver.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://github.com/emeraldgoose" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
          
            <li><a href="https://www.linkedin.com/in/minseong-kim-84428b231/" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://gooooooooooose.tistory.com/" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Problem Solving OJ (KOR)</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Self-supervised Pre-training Models 정리">
    <meta itemprop="description" content="Recent Trends">
    <meta itemprop="datePublished" content="2021-09-17T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Self-supervised Pre-training Models 정리
</h1>
          
<!--
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  0 minute read
</p>
            devinlife comments :
                싱글 페이지(포스트)에 제목 밑에 Updated 시간 표기
                기존에는 read_time이 표기. read_time -> date 변경
-->
            <p class="page__date"><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2021-09-17T00:00:00+09:00">September 17, 2021</time></p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="recent-trends">Recent Trends</h2>

<ul>
  <li>Transformer 모델과 self-attention block의 범용적인 sequence encoder와 decoder가 다양한 자연어 처리 분야에 좋은 성능을 내고 있다.</li>
  <li>Transformer의 구조적인 변경없이 인코더와 디코더의 스택을 12개 혹은 24개로 사용하고 이 상태에서 Fine-Tuning을 하는 방향으로 사용된다.</li>
  <li>추천 시스템, 신약 개발, 영상처리 등으로 확장된다.</li>
  <li>NLG에서 self-attention 모델이 아직도 greedy decoding에서 벗어나지 못하는 단점도 존재한다.</li>
</ul>

<h2 id="gpt-1">GPT-1</h2>

<h3 id="improving-language-understanding-by-generative-pre-training">Improving Language Understanding by Generative Pre-training</h3>

<ul>
  <li>
    <p>GPT-1</p>

    <p><img src="https://lh3.google.com/u/0/d/1Dt6MzObB1bLToXuUl4s113c_2y2kvuiH" alt="" /></p>

    <ul>
      <li>
        <p>출처 : <a href="https://openai.com/blog/language-unsupervised">openai.com/blog/language-unsupervised</a></p>
      </li>
      <li>다양한 special token들을 제안해서 간단한 task 뿐만 아니라 다양한 자연어 처리 task에서 모두 사용할 수 있는 통합된 모델을 제안했다.</li>
      <li>만약, Classification을 위해 학습된 모델이 있고 다른 Task를 수행하고 싶을 때, 마지막 layer를 떼어내고 random initialized된 layer를 붙여 해당 Task를 위해 다시 학습을 한다.</li>
      <li>학습을 할 때는 마지막 layer는 학습을 해야 하지만 이미 학습된 나머지는 상대적으로 learning_rate을 작게 줌으로써 큰 변화가 일어나지 않도록 한다.</li>
      <li>이전 학습 내용이 잘 담겨있도록 하여 그 정보를 원하는 Task에 활용할 수 있는 형태로 Pre-training 및 메인 Task를 위한 Fine-Tuning과정이 일어나게 된다.</li>
      <li>Self-supervised learning이라는 것은 대규모 데이터로부터 얻은 지식을 소량의 데이터를 학습하는데 지식을 전이하여 성능을 향상하는 방법이다.</li>
    </ul>
  </li>
</ul>

<h2 id="bert">BERT</h2>

<h3 id="bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h3>

<ul>
  <li>BERT도 GPT와 마찬가지로 Language Modeling이란 Task로써 문장에 있는 일부 단어를 맞추도록 Pre-trained된 모델이다.</li>
  <li>Transformer 이전에 LSTM을 이용하여 접근한 ELMO 방법도 존재한다. ELMO는 인코더와 디코더를 모두 LSTM으로 대체한 모델이다.</li>
</ul>

<h3 id="masked-language-model">Masked Language Model</h3>

<ul>
  <li>Motivation
    <ul>
      <li>기존 한계점 : 이전 예측된 단어를 보고 다음 단어를 예측해야 하는 단점. 즉, 한쪽 방향에서의 정보만으로 예측한다.</li>
      <li>앞뒤의 문맥을 보고 예측하는 것이 좀 더 좋은 성능을 낼 것이다.</li>
    </ul>
  </li>
</ul>

<h3 id="pre-training-tasks-in-bert">Pre-training Tasks in BERT</h3>

<p><img src="https://lh3.google.com/u/0/d/1DCM7-K5IQcaIR1x1GUK-zj1iig_S11Mm" alt="" /></p>

<ul>
  <li>
    <p>출처 : <a href="nlp.stanford.edu/seminar/details/jdevlin.pdf">nlp.stanford.edu/seminar/details/jdevlin.pdf</a></p>
  </li>
  <li>중간에 단어를 [MASK]로 치환해서 단어가 무엇인지 맞추는 형태로 학습을 진행한다.</li>
  <li>예측할 단어의 비율을 하이퍼 파라미터로 둔다.
    <ul>
      <li>너무 적게 masking 한 경우 : 학습시 cost가 비싸다.(ex. 100단어를 읽어서 1단어를 예측함)</li>
      <li>너무 많이 masking 한 경우 : context를 잡아내는 것이 충분하지 않다.</li>
    </ul>
  </li>
  <li>여기서는 k=15%로 두고 학습을 진행했지만 [MASK]를 100% 모두 예측할 수는 없다.
    <ul>
      <li>[MASK]라는 것에 익숙해진 모델이 나올 수 있다. 그래서 [MASK]를 치환할 때도 아래와 같이 다르게 치환한다.</li>
      <li>[MASK]로 치환할 단어들 중 80%는 실제로 [MASK]로 치환한다.</li>
      <li>남은 20% 중 10%는 random word로 치환한다.</li>
      <li>남은 10%는 그대로 둔다. 다른 단어로 바뀌어야 한다고 할 때 원래 있는 단어와 동일해야 한다고 알려주기 위함이다.</li>
    </ul>
  </li>
</ul>

<h3 id="pre-training-tasks-in-bert-next-sentence-prediction">Pre-training Tasks in BERT: Next Sentence Prediction</h3>

<p><img src="https://lh3.google.com/u/0/d/1wj90_COt5PwbCmbAeUstBvYW29Cj4Jj9" alt="" /></p>

<ul>
  <li>문장 레벨에서 Pre-training하기 위한 기법이다.</li>
  <li>[MASK]위치의 단어를 예측하고 [CLS]토큰을 위해 Binary Classification layer를 하나 추가하여 입려된 두 문장이 연결되었는지 혹은 연결되지 않았는지 판단하도록 한다.</li>
</ul>

<h3 id="bert-summary">BERT Summary</h3>

<ul>
  <li>모델의 구조는 Trasformer 구조를 그대로 사용한다.</li>
  <li>Input Representation
    <ul>
      <li>WordPiece embeddings : 각각의 sequence의 단위를 subword라고 부르는 단위로 임베딩하여 입력벡터로 넣어준다.</li>
      <li>Learned positional embeddings : positional embedding 벡터 또한 학습시킨다.</li>
      <li>[CLS] - Classification embeddings</li>
      <li>[SEP] - Packed sentence embedding</li>
      <li>Segment Embedding : 또다른 Positional Embedding 벡터인데 문장레벨에서의 position을 반영한 벡터이다.</li>
    </ul>
  </li>
</ul>

<h3 id="bert-vs-gpt">BERT vs. GPT</h3>

<ul>
  <li>GPT는 특정 타임스텝 이후의 단어의 접근을 허용하지 않는다. BERT는 특정 타임스텝에서도 모든 단어의 접근을 허용한다.([MASK]와 같은 토큰도 포함)</li>
  <li>Traning-data size
    <ul>
      <li>GPT는 BookCorpus(800M)인 반면, BERT는 BookCorpus(2500M)을 학습했다.</li>
    </ul>
  </li>
  <li>Training special toekns during training
    <ul>
      <li>BERT는 [SEP], [CLS], sentence A/B를 구분할 수 있는 Segment embedding을 넣어 학습한다.</li>
    </ul>
  </li>
  <li>Batch size
    <ul>
      <li>BERT - 128,000 words; GPT - 32,000 words</li>
      <li>BERT는 큰 batch size를 사용했을 때 안정된 결과를 얻을 수 있는데 이는 gradient descent 알고리즘을 수행할 때 다수의 데이터로 도출된 gradient로 학습하기 때문이다. 하지만 batch size를 키우기 위해 한번에 load해야 하는 메모리 + backpropagation을 위한 메모리가 너무 크다.</li>
    </ul>
  </li>
  <li>Task-specific fine-tuning
    <ul>
      <li>GPT는 모든 fine-tuning에서 동일한 learning rate을 사용했지만 BERT는 task마다 learning rate를 fine-tuning했다.</li>
      <li>BERT도 GPT와 동일하게 마지막 layer를 제거하고 새로운 task를 수행하기 위한 layer를 붙이고 남은 구조의 learning rate을 작게 해서 학습을 진행했다.</li>
    </ul>
  </li>
</ul>

<h3 id="machine-reading-comprehension-mrc-question-answering">Machine Reading Comprehension (MRC), Question Answering</h3>

<ul>
  <li>기계독해 기반</li>
  <li>문서와 질문을 이해하고 질문에 답을 찾는 Task</li>
  <li>SQuAD Dataset
    <ul>
      <li>v1.1 : 질문과 문서에 답이 존재</li>
      <li>v2.0 : 질문이 있지만 문서에 답이 없는 경우도 존재</li>
    </ul>
  </li>
  <li>SWAG Dataset</li>
</ul>

<h3 id="bert-ablation-study">BERT: Ablation Study</h3>

<ul>
  <li>layer와 paramter를 점진적으로 늘리면서 학습하면 성능이 지속적으로 증가한다.</li>
</ul>

<h2 id="gpt-2">GPT-2</h2>

<h3 id="gpt-2-language-models-are-unsupervised-multi-task-learners">GPT-2: Language Models are Unsupervised Multi-task Learners</h3>

<ul>
  <li>transformer의 레벨을 더 쌓아서 크기를 키운 모델이다. 또한, pre-trained 모델은 Language Model Task를 수행한다.</li>
  <li>40GB의 text로 학습을 진행했고 데이터의 퀄리티를 증가시켰다.</li>
  <li>여러 downstream task가 zero-shot setting으로써 다루어질 수 있다.
    <ul>
      <li>zero-shot setting : 어떠한 파라미터나 아키텍쳐의 변경이 없는 세팅</li>
    </ul>
  </li>
</ul>

<h3 id="gpt-2-motivation-decanlp">GPT-2: Motivation (decaNLP)</h3>

<ul>
  <li>“The Natual Language Decathlon: Multitask Learning as Question Answering” 논문에 영향을 받았다.</li>
  <li>모든 종류의 자연어 처리 task가 모두 Quesition Answering task로 바꿀 수 있다는 논문으로 자연어 처리 task들의 통합에 대해 다루고 있다.</li>
</ul>

<h3 id="gpt-2-datasets">GPT-2: Datasets</h3>

<ul>
  <li>Reddit의 글에 있는 문서와 외부링크가 첨부되어 있는 경우 외부링크를 타서 내부 문서까지 모두 크롤링한다.</li>
  <li>Byte pair encoding (BPE)를 사용한다.</li>
</ul>

<h3 id="gpt-2-model">GPT-2: Model</h3>

<ul>
  <li>Layer normalization이 각 sub-block의 입력으로 이동했다.</li>
  <li>각 레이어를 random initialization할 때, 레이어의 인덱스에 비례해서 더 작은 값으로 초기화한다.
    <ul>
      <li>레이어가 위로 올라갈 수록 거기에 쓰이는 선형변환의 해당하는 값들이 점점 더 0에 가까워지도록 하여 위쪽 레이어가 하는 역할이 점점 더 줄어들게 의도했다.</li>
    </ul>
  </li>
</ul>

<h3 id="gpt-2-question-answering">GPT-2: Question Answering</h3>

<ul>
  <li>대화형의 질의응답 데이터(Conversion question answering dataset(CoQA))가 있을 때, 학습데이터를 쓰지않고 질문을 읽고 답을 하게 했을 경우 55의 F1 score를 얻었다고 한다.
    <ul>
      <li>Fine-Tuned BERT는 89의 F1 score을 얻었다.</li>
    </ul>
  </li>
</ul>

<h3 id="gpt-2-summarization">GPT-2: Summarization</h3>

<ul>
  <li>CNN and Daily Mail Dataset</li>
  <li>Article 뒤에 TL;DR(Too long, didn’t read)이라는 것을 삽입하여 TL;DR이 나올경우 앞쪽 모든 문단에 대해 요약하는 Task를 수행한다.</li>
</ul>

<h3 id="gpt-2-translation">GPT-2: Translation</h3>

<ul>
  <li>WMT14 en-fr dataset</li>
</ul>

<h2 id="gpt-3">GPT-3</h2>

<h3 id="gpt-3-language-models-are-few-shot-learners">GPT-3: Language Models are Few-Shot Learners</h3>

<ul>
  <li>
    <p>GPT-3는 GPT-2를 개선한 모델인데 개선방향은 GPT-2의 파라미터보다 훨씬 많은 파라미터와 훨씬 더 많은 레이어를 쌓고 더 많은 데이터, 더 큰 배치 사이즈로 학습한 모델이다.</p>

    <p><img src="https://lh3.googleusercontent.com/fife/ALs6j_ELhzKjWr6qRHJrLCeXYXxCUF7gzkbxOhJGiQ5E2j3LIiVb_eSHyE8qI2cJyN9fsFnBbfEHW6UWRynrCCVN59sMUCXd2NH6O6vjgk--Th8UaYeApLuGQAIfhwj5E-LrnCgn77Yad3HYDx4Qnzfq8QfRc1jXYShxudgwLj0n-y0EqTYysNNMcaAf6n1XhCP34kuX0tGTm-kYPY8gQ9aVmreWcJhKL2YospWXRKHHcnDSioQ4l1ErA50K-8CTZUenCJhsLHrYqZj32ca7p5M0sSPCi_rgQK95zHtGbN9iysckk60AU4xXjS956vONcwaEFqMAcaBq1HeQ9or4BBnOklVxrECz-GSFTj1K_-iooGHLjeA_HypjaCE7PtlcPD-EztcMkGxjDrxbrmAtYG5bIIvzXnnrRU1Smcmm3sguRW3anCxWQj3XJpbJhlulx1H6vh11uEMRe6-mIW0ZY1d9nbO0i1egZAtPcapLqvU15sdXy4YuGaTxkAkRgSYzA2U-Fq2AKBuC0hv4L_LSxICX7XbN8jy0zTQHaE5H7tI9GP2qMZp08-cCYaYiDlp3nQ-ugfM2p5J3zvfj9-2GQwWroV-w42TQX0KHH_6wRWccJ5HVViXilHgcXkBkrmeCjNEKY4k9bDW0wfJNgTaGfrGTK23yfUQqjisC6wfq-x1shicOVg7K_xL1oKpK6gQBh4hNyED0mFEyh_RXmDxH2Hr1fHglj4bi94m8h-FvJmIkpq6COsUH3HfuNmtj-K96Yviqj68j_q07eglUcafTydxCymP9AvZVn665hl1FogjfCLOAQi2yanl0FUAO0hqSHXz3uaEfIAvKBhq_KUEpYyOux2NQNfjZ3uZWEvoghl8Jk60pM7VMzo6_tZRW33skz25RcvmJNRkJJ2vVEiXpZ7q_PtB4Xiq5Z0B6L2zaeQRLRCGuaCWEnBTUFYtmgK-RL44vD06kz05BEyy6vUsca5Ttd6QRiMSQWwGoK0Lk0Pal3qGGYfn7Suo8QFwnUqUZdu1qJ7NsgeXqS6uagM5FhVHOHt67i-O5puzD__UicmEt-blUTYTJ_Xfl1lc1hIVJXU5nfT2FmhCuzwFeQipf_H7sZgqHK9S_inQyCKCPduOKzejFz1wJWfttxbCcHg5dQfKweLlq0UBFNeCCjAJ0UXLyUDudS1DglQ4mxiX69Yd_BjuUlB3qRxzvqrhVuWo0DKL2x_YaOtNy3p016U2liYBCtCcTgI4r9cFCT7GlzMdxgCHOdqiGpaDwjJ-ldpYpFUQrP2uxVz109Hs8cIICFVFadcSVS2IYudW0Sa9I2BdVJPZoPGA9nqKQpNNmgbTs4C_AkKAcNfEbR4fqHPe7Nfnz7CRzqdmEJI8qAyCbwREUq-AXkzCaava565PPkXm0Nj5hsKiUGLink4Y20LLWuaUWpf7vhlYopBtWfhbaRRYvibigmCCdiMz6KAABjTdccuLlAVTTt3tDFght6oJSctcHKWrSL4J_YmqcX3fOMHToYD_jqOzRpU9tAZJHrJizGFGk9QctlRcr-aBb91vuAknvnp7W2MwgB4-YPupql8PVr3PfTllfHlgvT0sOykQwREELNDHFxkWNjrSYZqxwy-0ZTX_COoHDw755qgFwS8KDuBNVn7AKOg" alt="" /></p>
  </li>
  <li>
    <p>Few-shot Learners</p>

    <p><img src="https://lh3.googleusercontent.com/fife/ALs6j_Ftx6qPqivotnbse8W5QT4sIZIX7CEQImeAJtKHN-jcGaBeJzxcUhJI7-wdC2QCzl98_wGwT65hGww_oYNtWXeJ0bL79vbSwLjHKJZlSyZ7hCwKVd63T5Uj54Tp4SGGqOX04VDdBZTJhLD4cYWmwhdf7FadSZlHpMusy8D-IR2--GcL5d6Q1KuVp4oOCJlp466vbRlX5vES0ohHmUXC5F3V7exZTnnPJagsVBn08jQFZoG_2LbX4EA49PYxJkQ2NoO5Z5zST7Zc4yhV9PeK1VrByXzxFirDukJj5vOa0b3maYh4cdzvvDtlesCtqYv55tbdmD145IWEHS-5bWNqYJVxBFTRyEwSdmiXqyRCZcScGYQEwW2o22SIjcAwoZxgswuq3dmkNy_DYi7-19Mt5Cq1lMmz0hAubpwOLdMOih25elhnSffDEvI41P2mDH2QW1MiH9dbF6lEBuY8zIkTIlfgMg-DEH9lgVL_4VW4e4V9Cmo_O4Lwu73MFCXNnO32S2yOdqSwEl36cUfHP8D2CBENcus51eqA5LkI-l4zUYk_o1rcqu4eOiOks585CdwelZJLwIGSrd3y4UFl_2prsKM-4mM2mLJn2FGMyo5nyQHeTvljnyVuuaP9_M0qafvQh5h_KBMOn4qtpfT4Qc3nx7b8esw5m9XdCVSDGY15lTCew-CCcth3CeZVWJJGG4vjpJLU6UPbZPOfVpjKntfYteZ4TBafEb3T-F7MZXhWNcjnx_1-5Qxc07UXnhTK54vZeo0tMUSZHiS-afAQG5kyYz-0Xm0raQU-HkHrd83lfFy8WJZuX6cOkb2qbGG-jci1eGKqoOLsojN5T68qmlJfH6ctbwksSOQQUgcrAspnUjvyzxtzsOLcqN0GoBbRkzrkyZinxPF2swZ0NVvUk4AYvM2jSSPOZsHcxONqQ799yTHUdFEvx-L8N_OtJrJO2_c9mfmLx6b0zUBxdzrThLojbzHOQz_Y2ns6LuLekvW1H-gswrkwUhQL4eFtkwqo4S6xrKLmU-JqrAYFPAhMlmXno0r2dmK042dNkmm7EQQRJU385MOwyZSuD87YVT3vwUxKEsz2lDze5QAIT8A5QUWLzxj4n18NlchYqn6-ZUyysx5HcSVnL8aTcDTJMsC0z6jroXUHihAPtpq1iijHq0Qfm8eD2tnIcfUccbB22zId9vL1G4nxhv2D7iEJjMGN_f69gKlq-zSwzumxVQufczXBkOVwhIt_MMcUuLgKT01lZemuRBUzWc9GqZtEOUURTkF9D1VXCJ4ydJB_1Hvd9MIUrcA_reXnCd6Xwwc9uHZjcNBwRVzO6DlbqFSwd7gx2o2IRgTndupT3sJSh46k6eAAVnkmeQbB0DFYDiXK1C0BU0LEXR7ybcMDaC37dNMBpEIbOwvkSQEZJG7ubaxDJoXe8dS4qT2P9Ya8clJGvwVRLC-LfZRdvEzdzkJPzz0oIfRpBEhlRtyWkwhlByVYpt72O8swQjX0ALteKFC4mOIPfey9FnB2PysmLRnNVuLVf9VEkEdHc6sbFYEyJd3CJG6Y6Df2xFQJtOUC4Dca_yBw08Qri-yB8M0RS0kTEMuF3g8rI-3IDBsHTn9_Kq7-vgeLJY-wuoV_leZ2oaE60bMp0bTDDjvlBw" alt="" /></p>

    <ul>
      <li>Zero-shot: task-description을 가지고 바로 answer를 답하도록 하는 task</li>
      <li>One-shot: 한 가지 예를 주고 answer를 답하도록 하는 task</li>
      <li>Few-shot: 여러가지 예를 주고 answer를 답하도록 하는 task</li>
      <li>이전에는 downstream을 위해 output layer를 추가하여 학습시키는 과정을 거쳤지만 지금은 변경하지 않고 inference과정 중에 예시를 input text에 example를 포함시키는 방법을 사용한다. → zero-shot보다 성능이 좋아졌음</li>
    </ul>
  </li>
  <li>
    <p>모델 사이즈를 키울수록 zero-shot, few-shot 성능이 빠르게 올라간다.</p>

    <p><img src="https://lh3.googleusercontent.com/fife/ALs6j_GmawF4_jROAh77px5_s5S-XLFO2ZTI0p8LTVKx9Irg0YB8Y9uDFmCD7S-uezCiStoGMQzGgz0kvzixM4qCf1Pz1ga9a_7sqeEKhobNc-yITs-uqTS8EN7iNE5zEdi7zdFkKzXA43GpJXMC8ifmg6hU83xLPZAwGwAniW8C3cIUnbE9qA_eeEUSP6kqmIS9BoH12YV4UQyP0MCWuFgRTU606BApGc5av-qIdfdh7Hj3D26WnKe-kZ_KqkpBOiC7NTimPNnJ_yzdwtWRCgcyuqdEWDKaKFVaoEiyuvPG4RrT2_nVxDF9h7FVufI9jLz6IZ8leA08HpTqa2VUhdm7S1dz2NcC0NieXLfDl1ve-w1pkvaNS2oc-y4I64KfGA-M2Gw5E05-VGZpqJBJDgyLxfiz7X2UzAy6_wiqseSFeQVi7Z1lFuQjAmDwlx3TKJbSfBv5UbwB7BO6K0EnG9TaKA66ZHD9To9kR6lX3YIb6yCne6LVTmZH6rfbimJ9yRWd_QlOEzwJhsphCyVsxfx3BfekuSjmmmyYBK9yO9J_OGyTM1KVf2F8k68avtUzPhpZaAWAbqq8jGDMzVuyYNOxUaX4uTiiibvOqoZfY20IY-V9iTU5NmPfBY9WzeK9mG8gi5xOrTbCQYPQyBO--0pqIctU9VRUOQFkeMI2TyPJaA3Ys7rieGkFtxR_T6l1qf8532w_hiafaqhSSD4pdEi2WA2acaE1YQ6r7iLCjLD9F55vz5F2W4SWPAWFl3TWoRvMDersOa0I_ITdyKP0RYg1s_m0QZgOapJX1Ul3wde1AYepZOGcuR7NaqX6Gi8glr70LYsvezS51MPujPC9OnyoUYgqWWcr4ulNx-pG90jebig_J1TgWBJg4BUdhnZpcQEiSmFb8bq5wVOIfL3BDmTO7BzXG0x4J2MRT9GhYZPfaKYz8LDhloKZE9GAlR8zmiWh57h5QJjGjhTBj3NM3ZK5-RReejkpl1BcyehvESjArVKEt8-zC3Pqqj5ielq9diYmqvcCSPB13PdpXN7QuiLxhZXQ-EyWShAg9RXp8mrczy-otmE5KSao6rRefbmixtDO5haRzTLPzW_dDHW6VhNIdol6o3aZWLOQJg_MtBQGqKemyDcNaFONDRETuzehD94hjDFY1g7_Lia0T8Ro2YAHgEVYVxLdffT4hQeuRwy1n512ECIt12Xegu7nfErzlkfBEHjtDKn4UMQHaVsjeNrjHg4_9OFL82n5tHLY757Mb-TCbMmeSBW4dG__cxFFuR_fVVSVIDfsZbPpEfRZ5wpdkNEyaSVAVCYPt6466A1c2hsviRn8swI1ZCY3b77H_XMeGaJ_gS-mXxOYwipEHRA0g2DdTMKaRx4cjqgDsy_pv-u9f8l14CxLNK2cpzarSM_WnjyIJ31IuIktwV9FN3koM-kpGNqBk_oV2s--tn9XhgubazvVpil1b2kZFYDiXtMV1muPdbeZTV18znqRojZ1i2O58oT8FUusK_byFIHHEIbG2L_g1eYs65rJicA2IxcKiQguzk4j7SdZSg_wQH1mTEDLhR-dKeW_Mwi9ay8ZiCRHDj7Zo_4iVePVUkkjEwj5IjXrdfSI0476md34EyJd7k0K7ZFtR_zINZxVa2ypzTBgpAk-dQ" alt="" /></p>
  </li>
</ul>

<h2 id="albert-a-lite-bert-for-self-supervised-learning-of-language-representations">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</h2>

<ul>
  <li>많은 pre-trained모델은 많은 파라미터와 많은 레이어를 가지고 있는데 이로 인해 많은 메모리와 학습 시간이 필요로 했다.</li>
  <li>ALBERT는 기존 BERT모델보다 가볍게 하면서 성능 하락이 없는 모델이다.
    <ul>
      <li>Factorized Embedding Parameterization</li>
      <li>Cross-layer Parameter Sharing</li>
      <li>(For Performance) Sentence Order Prediction ← 새롭게 제안</li>
    </ul>
  </li>
  <li>Factorized Embedding Parameterization
    <ul>
      <li>Transformer에는 입력의 dimension이 첫 self-attention 입력에도 유지가 되고 다음 레이어의 입력에도 유지가 된다.</li>
      <li>ALBERT는 Embedding Layer의 dimension을 줄이는 기법을 제시했다.
        <ul>
          <li>기존 BERT는 word embedding의 dimension이 입력의 dimension과 같아야 했다.</li>
          <li>ALBERT는 word embedding의 dimension을 입력의 dimension보다 작게 하고 다시 입력의 dimension과 같게 할 수 있는 layer를 추가한다.</li>
          <li>만약, 기존 BERT의 embedding vector의 크기가 (500 $\times$ 100)라고 하고 ALBERT의 embedding vector의 크기는 (500 $\times$ 15), weight matrix의 크기 (15 $\times$ 100)를 가지게 된다고 가정하자.</li>
          <li>이때, BERT는 50000의 크기를 가지는데 반해 ALBERT는 7500 + 1500 = 9000의 크기만을 가지게 되어 메모리를 절약할 수 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Cross-layer Parameter Sharing
    <ul>
      <li>선형변환 matrix를 공유하는 아이디어이다.</li>
      <li>Shared-FFN: feed-forward network parameter를 공유</li>
      <li>Shared-attention: attention parameter를 공유</li>
      <li>All-shared: Shared-FFN + Shared-attention</li>
    </ul>

    <p><img src="https://lh3.googleusercontent.com/fife/ALs6j_EfySECCT18XMoFc36lzBq_F3Z8E8xl4dHCl9a3g-BovKcugM_iBDKoCRMUxC8-747hwO1F1uGVlU5zR48gswhkjhP7cYdDo_NXOPJMCYR9wZUYwVqJQzysPg71fpx8Rd3sY8VJYGoPSh7W9ZF_XtWyLng_OLb_HMwm2iwBeEeDjFDN4k0opE_UOn1WzcEdJQqpZwxH6UecBy5Ep7XPfyTw8s2fq9SYtjDsWqUd-ubhtqFSeloDmlJn_O5EYeLr3gUU9Npn2E7gfZRcy0hk1JeL-LpuELJQX_hjOqzyDINHECHrDBXvy4RWPiFDiFpk_VIHF-Wu4nlsLlBWDnvN3rerZIaN8brXbuGnpHYef01ukeMc84Kn_eRV-Y1yFburKzemsZTiql4R8UfSBLq-UQ3FpNXIq371TElxG9YXXzrmSTIfcz-rvCxCaAdC0NhnweYSSDSDYvfWzxLoLYkd_2AVdfjK0SUbxlVrqtl72ujtTAqFo-_t1yYT3UQOarrKrahhvreENX70K5xwA9MncsZOygOZqEekQLE6U5znhLFAw2n-Atj4uNkgtmkNuv4v89e2Gajy1tAUccDWFy9ISn9GakMTNxusRmug_MFWJ4FBgOUS9IsCU4VtTTBGXNXKgmNZxJ9qN5UmePts3FT5QBFBBrA4cwqdJcnJgtJiz6b2AFqdTLhkPgKhzc_IROSVxH3G1xybpDsZ10V3D5CDm7DaqIgdUXTPTQ1h_yqaarj9tXgLpMYda9eOyac6tXmi58wVhxllJJct_35Ge8jR2WtB4xqTTuf1kL1CBU8xd79nm6KXP-97GB5ISI_zjBt-MQ8l1FmDq1-7gAFTcGUgTJ3GI0Gbc1enesEHcODhqKd80BDSyGUk15YnlALmJNHSnv3jT79B7XxIpPScfbIzz_DPy50dDLyn0uC5xAz7ijm30s71O0O7wdkwqwdUEW8o8PJL-_ePuYDWkfYs3OnCn8nCpGDioQZQdMZlchKPUKyAYf60vilmT4jRfHYbv-zJU-ibp5ccL3DV-5lnD3ZUZi60fsEuTm6YWQXjvQawbpuky7EFLvV8XT5lLXTBlCI0SuScydDx_ONppTYjqNYyrpNlFte4IR5ETvRC5AWyxvoXXOeSe_9lhcT8j6ol3eUwj2pJANfH1Nke6q7PlfnAZ8pRZIPHHa0jHeYykoAaoGS1I5Tz-DqxDP_ZURvVKiaml_j7tVwdUijvgn-rWie7nGAged0CkveMO-Vgsu8TuXoZWy-Qr0GyKK2fvYY5tAoiom482A3-TtR0xRYz1peQBSiFeurMjbXM-Dn5T983VeaDVy1s5BumvrPiSPmNVXN6ZDhmUuQ-xfexmtUF52IRgkIZZsfljk4NIFc-BD2xOGqqbprLMwqI02jdYNulbG5JQWBmyopP64-AP5fuDE2ZFpk7yzuKFueBqjM440R3GixYXDnHSPgcBcYVl35oyjlK2G7kqNJhGcmqUN68I8eqTTQxTpx8EoHFcl_y7nG4ONOXZlACpvzcFG1z6cPy05XVWsG_gltALDgwmjoPtsU4hRJGAqH62siiHHOfRmOCDxZ8VD0Ud95QEzRxxKRXAcfFx_z_eBoBBoN1_8jKsupaXuIfxAWjTZVFguN6WXtHv8Y5mqQg7Q" alt="" /></p>

    <ul>
      <li>모두 공유했을 때 가장 적은 파라미터 수를 가지고 성능에 있어서는 하락폭이 크지 않다.</li>
    </ul>
  </li>
  <li>Sentence Order Prediction
    <ul>
      <li>기존 BERT모델에서는 Next Sentence Prediction task는 실효성이 없었다. → NSP를 좀 더 유의미한 pre-training을하고 싶었다.</li>
      <li>연속적인 두 문장을 가져와서 순서대로 SEP을 추가한 데이터와 순서를 반대로 한 데이터를 만들어  binary classification task를 수행하도록 했다.
        <ul>
          <li>Negative sample 또한 두 문서(docs)에서 랜덤하게 추출된 두 문장을 연결해서 next sentence가 아니도록 학습을 진행했다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="electra-efficiently-learning-an-encoder-that-classifies-token-replacements-accurately">ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately</h2>

<p><img src="https://lh3.googleusercontent.com/fife/ALs6j_HpWmLPTdU2gtZpmCOGzJ-N2cXhwmyeP_FRZ5XssmqELA-I2mcpMmW90w4ZpfBy1w0qlCnqnce0xXLFMqNPlZAZsjtFPkj8JpWYeNZpZzNrnI-73NwMmc2CsdP3O1ymivniMMDQ6D7OUxw9F-tfiqp3cOcytRsnuptukHAMaL16pUE9Pzi42cOaPFT_cSYFCTmVSi1rgWNj5KPHm5zMIwV9__ubp-mOEswr0HhSrw-U3_4v17KDOJqO_wQnmoUN8DNPlq5uLQVz5OzWfZDwnSRBzn78uYNWiGgmQ6IuJiArWgx_Q0PuEG9rS0sFZPQ1t_U7INTVLSzwYa4yAqQyYIsEyZO5uW1aOQrm8wJ9KKqDN8xHju3V74yk3u8O1CXjewiuFe4mqKuY3POhZ6BGrp8yus7aMFh_PiL1CDBiAogavDr7pSITp_m931hSVtGpjzVCJTHBPATeB_PLMuJ2bQMxxlB7O-cETrl2i0iF91HmxEyewIfd3LYfdqtR47CN2vcgdjHL5qtw9rwOtYsdtOs1PrD5ReHlLmQm5-wJg8pVHham_YEn4NoFafAJExx7kE2JjoYG5bG6IM-j6HhKqdTNXyRBJqFuuVELMVyg8aj3oYweX7-1Gb1vgkOq5w0ztD67wIjFq-dEaXK7pi4Hka3CQW1zI6LB8uc0WQDvc8EqTrw3Kl0L-pgceA_FwAWkE0xN-7Lb7MM7YNq1yHvdG-XUcVNhdyaRTaYTYB6oO1I28KME2vh54i-W54UMGEDrdzEjFjTJDTCAIhwM1mx9LM62hFPbwOeue_FqGONi3xKlWvSX14OIwbZWvV3DFtIw-T1WEUBcEcmdvDjcj0Rj1ZZoGflIgDZEm40gPPqbG1CTKf9gidKdrl4Z6C7yRk0MCcT8k5wKb7QM_4xn8aQH1sE1xycnU3PcNYvZN7NZ7wtJUOm0R6klMVXsUTFh-_4j-HE5AT0VXuwLE-SQD_Z3HMm1J9M8ThwFPoh6D-wRUB7pAw6wxgkctVLFMFiCvRCrJcWdHUAbtECcZWiU9FhPXP3V6pNJIeSlYOvHJz4Cy9AjvUAtX19G0GZvmP_Eg9TRKJ4YIUDa5BHPJOObNpBLhhMNaD5AUNhAqSWzPlaZnBKAXiO-xHcby3GeCuJs_NePxypLWUIF-JxGPxLe263BO8jIcAtvzqiOykNuYmYLj0Caefo4rwAoBlBRG19oFu4JRAdDMp9mX2HjkTb8vQhXcKWB5cXARX7h9xZOsKIEMgR6mHF8Hcy_6MCdiT0CNyWAsLVtKNw0_bMJTIKYXYXFcyHTjTcI1YF1CtoyZd7G4gJD_cS1TpeIaToinIziYlxnK6IMl-buhCSQUaXkqJumGgqOfUiOO3u6v8YDHYJxqpg9MF0HVWkzB4W-g4B2OBQMCy0QwIV5X3RSmA-6kIy2obHirTyMHDY9ftdLL0na97T1-EMnHbQWPvD_a55e-oWnlqCGzLe-EInZeV8tI1z3UfjN3uU1scN4lE708PwMzJnL7jRjcEAK_8Jo81fhVeqC_jjMLhDjVG6AeoNT_lr3bZpmjrwZ8xfuSndC07xcApq0VwoCsHx4ZFwUSBri9wRcjvvNeJTieNmOEPUFyh3DKox_FujhQyc-34uo04o2QJk-GSkh6Q" alt="" /></p>

<ul>
  <li>Generator는 MLM을 통해 [MASK]단어를 복원한다.</li>
  <li>Discriminator는 self-attention을 쌓은 모델인데 각 단어별로 original단어인지 어색하여 replaced된 모델인지 판단하는 모델이다.</li>
  <li>두 가지 모델이 Adversarial(적대적)관계로 학습이 진행된다. (Generative adversarial network의 아이디어와 같다.)</li>
  <li>이 모델의 경우 Discriminator를 pre-trained 모델로 사용하게 된다.</li>
</ul>

<h2 id="light-weight-models">Light-weight Models</h2>

<h3 id="distillbert">DistillBERT</h3>

<ul>
  <li>Teacher모델과 Student모델이 있는데 Teacher모델은 Student모델을 가르치는 역할을 한다. Student모델은 Layer나 파라미터 수가 Teacher모델보다 작지만 Teacher모델의 output을 잘 모사할 수 있도록 학습된다.</li>
  <li>Student모델의 ground truth probability는 Teacher 모델의 probability가 된다.</li>
</ul>

<h3 id="tinybert">TinyBERT</h3>

<ul>
  <li>knowledge distillation을 이용하여 DistilBERT와 비슷하게 Teacher모델과 Student모델이 존재한다.</li>
  <li>Student 모델이 Teacher모델의 probability를 따라할 뿐만 아니라 각 self-attention block이 가지는 $W_Q$, $W_K$, $W_V$와 같은 Attention matrix와 그 결과로 나오는 히든 스테이트 벡터들 까지도 유사해지도록 학습을 진행한다.</li>
  <li>Mean Squared Error(MSE)를 통해 가까워지도록 한다. 그러나 일반적으로 Student 모델의 차원은 Teacher 모델의 차원보다 작을 수 있기 때문에 일반적인 loss를 적용할 수는 없다.</li>
  <li>그래서 Teacher 모델의 학습가능한 Fully connected layer를 하나 두어서 차원이 다른 벡터의 mismatch를 줄이는 방식으로 학습을 진행한다.</li>
</ul>

<h2 id="fusing-knowledge-graph-into-language-model">Fusing Knowledge Graph into Language Model</h2>

<ul>
  <li>BERT 모델들은 주어진 문장이 있을 때 문맥을 파악하고 단어들 간의 유사도나 관계를 파악하는 것은 잘 하지만 주어져 있는 문장에 포함되어 있지 않은 정보가 필요한 경우에는 잘 활용하지 못한다.
    <ul>
      <li>예를들어, “꽃을 심기 위해 땅을 팠다”와 “건물을 짓기 위해 땅을 팠다”를 학습했을 때 “땅을 판 도구는 무엇인가?”라는 질문에 대해 제대로 답을 하지 못한다.</li>
      <li>인간의 경우 꽃을 심기 위해 부삽을 사용하고 건물을 짓기 위해 중장비를 이용한다는 외부지식(상식)을 가지고 있어 위의 답을 할 수 있다. 따라서, 모델들도 이런 상식이 답을 하기 위한 외부지식이 필요하다고 판단할 수 있다.</li>
      <li>Knowledge Graph는 이런 외부지식들을 담고 있는 그래프이다. “(땅을)파다”라는 객체와 “땅”이라는 객체가 연결되어 있고 다시 땅을 파기 위한 어떤 도구들이 연결되어 있다.</li>
    </ul>
  </li>
</ul>

<h3 id="ernie-enhanced-language-representation-with-information-entities">ERNIE: Enhanced Language Representation with Information Entities</h3>

<ul>
  <li>Information fusion layer는 token embedding과 entity embedding을 concatenation해준다.</li>
</ul>

<h3 id="kagnet-knowledge-aware-graph-networks-for-commonsense-reasoning">KagNET: Knowledge-Aware Graph Networks for Commonsense Reasoning</h3>

<ul>
  <li>A knowledge-aware reasoning framework for learning to answer commonsense questions</li>
  <li>질문과 답 pair에 대해 외부 knowledge graph에서 sub-graph검색하여 관련 지식을 capture한다.</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#nlp" class="page__taxonomy-item p-category" rel="tag">nlp</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#boostcamp" class="page__taxonomy-item p-category" rel="tag">BoostCamp</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-09-17T00:00:00+09:00">September 17, 2021</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/boostcamp/transformer/" class="pagination--pager" title="Transformer 정리
">Previous</a>
    
    
      <a href="/contest/relation-extraction/" class="pagination--pager" title="P2 KLUE Relation Extraction(RE) 대회 회고
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agent/gpt-oss-structed-output/" rel="permalink">Langchain GPT-OSS 구조화된 출력 트러블슈팅
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> November 22 2025</p>
    
    <p class="archive__item-excerpt" itemprop="description">Databricks-gpt-oss
사용한 LLM은 gpt-oss-20b 모델이고 databricks workspace에 통합된 모델을 사용했습니다. huggingface에 업로드된 gpt-oss 모델을 사용해본 경험은 없지만 같은 에러가 발생할 것 같습니다. 만약 똑같은 문제가 일...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agent/vibe-coding-vs-augmented-coding/" rel="permalink">바이브 코딩과 증강 코딩(Vibe Coding &amp; Augmented Coding)
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> August 19 2025</p>
    
    <p class="archive__item-excerpt" itemprop="description">Vibe Coding
바이브 코딩은 자연어 프롬프트를 이용해 AI가 코드를 작성하고 사용자는 작동 결과에 관심을 기울이는 형태의 프로그래밍(?) 방식입니다. 만약, 작동 결과가 사용자의 의도와 맞지 않다면, 피드백을 통해 AI가 코드를 수정하는 방식이며 사용자는 코드에 관여하지 않...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/data-engineer/spark-on-kubernetes/" rel="permalink">Spark on Kubernetes
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> July 27 2025</p>
    
    <p class="archive__item-excerpt" itemprop="description">Apache Spark
Spark는 대규모 데이터 처리를 위한 분석 엔진입니다. Java, Scala, Python 등의 언어를 지원하고 정형 데이터를 처리할 수 있는 SparkSQL, 머신러닝을 위한 MLlib, 스트리밍 처리를 위한 Spark Streaming 등을 포함한 많은...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/unet-and-ddpm-implementation/" rel="permalink">Python으로 Diffusion 바닥부터 구현하기[2] (im2col, UNet, DDPM)
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> May 24 2025</p>
    
    <p class="archive__item-excerpt" itemprop="description">Objective
이전 글에서 UNet의 구성요소인 ResidualBlock, AttentionBlock, UpsampleBlock을 구현했습니다.

  Python으로 Diffusion 바닥부터 구현하기[1] (ResidualBlock, AttentionBlock, Upsampl...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 emeraldgoose. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'emeraldgoose/emeraldgoose.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [['$','$']],
  displayMath: [['$$','$$']],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

  </body>
</html>
