<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Self-supervised Pre-training Models 정리 - gooooooooooose</title>
<meta name="description" content="Recent Trends">


  <meta name="author" content="goooose">
  
  <meta property="article:author" content="goooose">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="gooooooooooose">
<meta property="og:title" content="Self-supervised Pre-training Models 정리">
<meta property="og:url" content="http://localhost:4000/boostcamp/self-supervised-pretraining-model/">


  <meta property="og:description" content="Recent Trends">







  <meta property="article:published_time" content="2021-09-17T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/boostcamp/self-supervised-pretraining-model/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "emeraldgoose",
      "url": "http://localhost:4000/"
    
  }
</script>


  <meta name="google-site-verification" content="googleb34ce5276ba6573e" />





  <meta name="naver-site-verification" content="naver93cdc79a5629ab3736d7cf8ff7b51d80">


<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="gooooooooooose Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          gooooooooooose
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo.png" alt="goooose" itemprop="image" class="u-photo" width="110px" height="110px">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">goooose</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Dev blog.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:smk6221@naver.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/emeraldgoose" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Self-supervised Pre-training Models 정리">
    <meta itemprop="description" content="Recent Trends">
    <meta itemprop="datePublished" content="2021-09-17T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Self-supervised Pre-training Models 정리
</h1>
          
<!--
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  0 minute read
</p>
            devinlife comments :
                싱글 페이지(포스트)에 제목 밑에 Updated 시간 표기
                기존에는 read_time이 표기. read_time -> date 변경
-->
            <p class="page__date"><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2021-09-17T00:00:00+09:00">September 17, 2021</time></p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              
                <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
                <ul class="toc__menu"><li><a href="#recent-trends">Recent Trends</a></li><li><a href="#gpt-1">GPT-1</a><ul><li><a href="#improving-language-understanding-by-generative-pre-training">Improving Language Understanding by Generative Pre-training</a></li></ul></li><li><a href="#bert">BERT</a><ul><li><a href="#bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li><li><a href="#masked-language-model">Masked Language Model</a></li><li><a href="#pre-training-tasks-in-bert">Pre-training Tasks in BERT</a></li><li><a href="#pre-training-tasks-in-bert-next-sentence-prediction">Pre-training Tasks in BERT: Next Sentence Prediction</a></li><li><a href="#bert-summary">BERT Summary</a></li><li><a href="#bert-vs-gpt">BERT vs. GPT</a></li><li><a href="#machine-reading-comprehension-mrc-question-answering">Machine Reading Comprehension (MRC), Question Answering</a></li><li><a href="#bert-ablation-study">BERT: Ablation Study</a></li></ul></li><li><a href="#gpt-2">GPT-2</a><ul><li><a href="#gpt-2-language-models-are-unsupervised-multi-task-learners">GPT-2: Language Models are Unsupervised Multi-task Learners</a></li><li><a href="#gpt-2-motivation-decanlp">GPT-2: Motivation (decaNLP)</a></li><li><a href="#gpt-2-datasets">GPT-2: Datasets</a></li><li><a href="#gpt-2-model">GPT-2: Model</a></li><li><a href="#gpt-2-question-answering">GPT-2: Question Answering</a></li><li><a href="#gpt-2-summarization">GPT-2: Summarization</a></li><li><a href="#gpt-2-translation">GPT-2: Translation</a></li></ul></li><li><a href="#gpt-3">GPT-3</a><ul><li><a href="#gpt-3-language-models-are-few-shot-learners">GPT-3: Language Models are Few-Shot Learners</a></li></ul></li><li><a href="#albert-a-lite-bert-for-self-supervised-learning-of-language-representations">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></li><li><a href="#electra-efficiently-learning-an-encoder-that-classifies-token-replacements-accurately">ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately</a></li><li><a href="#light-weight-models">Light-weight Models</a><ul><li><a href="#distillbert">DistillBERT</a></li><li><a href="#tinybert">TinyBERT</a></li></ul></li><li><a href="#fusing-knowledge-graph-into-language-model">Fusing Knowledge Graph into Language Model</a><ul><li><a href="#ernie-enhanced-language-representation-with-information-entities">ERNIE: Enhanced Language Representation with Information Entities</a></li><li><a href="#kagnet-knowledge-aware-graph-networks-for-commonsense-reasoning">KagNET: Knowledge-Aware Graph Networks for Commonsense Reasoning</a></li></ul></li></ul>

              
            </nav>
            <!-- devinlife comment : right-sidebar ads -->
            <nav class="toc-custom">
              
            </nav>
          </aside>
        
        <h2 id="recent-trends">Recent Trends</h2>

<ul>
  <li>Transformer 모델과 self-attention block의 범용적인 sequence encoder와 decoder가 다양한 자연어 처리 분야에 좋은 성능을 내고 있다.</li>
  <li>Transformer의 구조적인 변경없이 인코더와 디코더의 스택을 12개 혹은 24개로 사용하고 이 상태에서 Fine-Tuning을 하는 방향으로 사용된다.</li>
  <li>추천 시스템, 신약 개발, 영상처리 등으로 확장된다.</li>
  <li>NLG에서 self-attention 모델이 아직도 greedy decoding에서 벗어나지 못하는 단점도 존재한다.</li>
</ul>

<h2 id="gpt-1">GPT-1</h2>

<h3 id="improving-language-understanding-by-generative-pre-training">Improving Language Understanding by Generative Pre-training</h3>

<ul>
  <li>
    <p>GPT-1</p>

    <p><img src="https://drive.google.com/uc?export=view&amp;id=1Dt6MzObB1bLToXuUl4s113c_2y2kvuiH" alt="" /></p>

    <ul>
      <li>
        <p>출처 : <a href="https://openai.com/blog/language-unsupervised">openai.com/blog/language-unsupervised</a></p>
      </li>
      <li>다양한 special token들을 제안해서 간단한 task 뿐만 아니라 다양한 자연어 처리 task에서 모두 사용할 수 있는 통합된 모델을 제안했다.</li>
      <li>만약, Classification을 위해 학습된 모델이 있고 다른 Task를 수행하고 싶을 때, 마지막 layer를 떼어내고 random initialized된 layer를 붙여 해당 Task를 위해 다시 학습을 한다.</li>
      <li>학습을 할 때는 마지막 layer는 학습을 해야 하지만 이미 학습된 나머지는 상대적으로 learning_rate을 작게 줌으로써 큰 변화가 일어나지 않도록 한다.</li>
      <li>이전 학습 내용이 잘 담겨있도록 하여 그 정보를 원하는 Task에 활용할 수 있는 형태로 Pre-training 및 메인 Task를 위한 Fine-Tuning과정이 일어나게 된다.</li>
      <li>Self-supervised learning이라는 것은 대규모 데이터로부터 얻은 지식을 소량의 데이터를 학습하는데 지식을 전이하여 성능을 향상하는 방법이다.</li>
    </ul>
  </li>
</ul>

<h2 id="bert">BERT</h2>

<h3 id="bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h3>

<ul>
  <li>BERT도 GPT와 마찬가지로 Language Modeling이란 Task로써 문장에 있는 일부 단어를 맞추도록 Pre-trained된 모델이다.</li>
  <li>Transformer 이전에 LSTM을 이용하여 접근한 ELMO 방법도 존재한다. ELMO는 인코더와 디코더를 모두 LSTM으로 대체한 모델이다.</li>
</ul>

<h3 id="masked-language-model">Masked Language Model</h3>

<ul>
  <li>Motivation
    <ul>
      <li>기존 한계점 : 이전 예측된 단어를 보고 다음 단어를 예측해야 하는 단점. 즉, 한쪽 방향에서의 정보만으로 예측한다.</li>
      <li>앞뒤의 문맥을 보고 예측하는 것이 좀 더 좋은 성능을 낼 것이다.</li>
    </ul>
  </li>
</ul>

<h3 id="pre-training-tasks-in-bert">Pre-training Tasks in BERT</h3>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1DCM7-K5IQcaIR1x1GUK-zj1iig_S11Mm" alt="" /></p>

<ul>
  <li>
    <p>출처 : <a href="nlp.stanford.edu/seminar/details/jdevlin.pdf">nlp.stanford.edu/seminar/details/jdevlin.pdf</a></p>
  </li>
  <li>중간에 단어를 [MASK]로 치환해서 단어가 무엇인지 맞추는 형태로 학습을 진행한다.</li>
  <li>예측할 단어의 비율을 하이퍼 파라미터로 둔다.
    <ul>
      <li>너무 적게 masking 한 경우 : 학습시 cost가 비싸다.(ex. 100단어를 읽어서 1단어를 예측함)</li>
      <li>너무 많이 masking 한 경우 : context를 잡아내는 것이 충분하지 않다.</li>
    </ul>
  </li>
  <li>여기서는 k=15%로 두고 학습을 진행했지만 [MASK]를 100% 모두 예측할 수는 없다.
    <ul>
      <li>[MASK]라는 것에 익숙해진 모델이 나올 수 있다. 그래서 [MASK]를 치환할 때도 아래와 같이 다르게 치환한다.</li>
      <li>[MASK]로 치환할 단어들 중 80%는 실제로 [MASK]로 치환한다.</li>
      <li>남은 20% 중 10%는 random word로 치환한다.</li>
      <li>남은 10%는 그대로 둔다. 다른 단어로 바뀌어야 한다고 할 때 원래 있는 단어와 동일해야 한다고 알려주기 위함이다.</li>
    </ul>
  </li>
</ul>

<h3 id="pre-training-tasks-in-bert-next-sentence-prediction">Pre-training Tasks in BERT: Next Sentence Prediction</h3>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1wj90_COt5PwbCmbAeUstBvYW29Cj4Jj9" alt="" /></p>

<ul>
  <li>문장 레벨에서 Pre-training하기 위한 기법이다.</li>
  <li>[MASK]위치의 단어를 예측하고 [CLS]토큰을 위해 Binary Classification layer를 하나 추가하여 입려된 두 문장이 연결되었는지 혹은 연결되지 않았는지 판단하도록 한다.</li>
</ul>

<h3 id="bert-summary">BERT Summary</h3>

<ul>
  <li>모델의 구조는 Trasformer 구조를 그대로 사용한다.</li>
  <li>Input Representation
    <ul>
      <li>WordPiece embeddings : 각각의 sequence의 단위를 subword라고 부르는 단위로 임베딩하여 입력벡터로 넣어준다.</li>
      <li>Learned positional embeddings : positional embedding 벡터 또한 학습시킨다.</li>
      <li>[CLS] - Classification embeddings</li>
      <li>[SEP] - Packed sentence embedding</li>
      <li>Segment Embedding : 또다른 Positional Embedding 벡터인데 문장레벨에서의 position을 반영한 벡터이다.</li>
    </ul>
  </li>
</ul>

<h3 id="bert-vs-gpt">BERT vs. GPT</h3>

<ul>
  <li>GPT는 특정 타임스텝 이후의 단어의 접근을 허용하지 않는다. BERT는 특정 타임스텝에서도 모든 단어의 접근을 허용한다.([MASK]와 같은 토큰도 포함)</li>
  <li>Traning-data size
    <ul>
      <li>GPT는 BookCorpus(800M)인 반면, BERT는 BookCorpus(2500M)을 학습했다.</li>
    </ul>
  </li>
  <li>Training special toekns during training
    <ul>
      <li>BERT는 [SEP], [CLS], sentence A/B를 구분할 수 있는 Segment embedding을 넣어 학습한다.</li>
    </ul>
  </li>
  <li>Batch size
    <ul>
      <li>BERT - 128,000 words; GPT - 32,000 words</li>
      <li>BERT는 큰 batch size를 사용했을 때 안정된 결과를 얻을 수 있는데 이는 gradient descent 알고리즘을 수행할 때 다수의 데이터로 도출된 gradient로 학습하기 때문이다. 하지만 batch size를 키우기 위해 한번에 load해야 하는 메모리 + backpropagation을 위한 메모리가 너무 크다.</li>
    </ul>
  </li>
  <li>Task-specific fine-tuning
    <ul>
      <li>GPT는 모든 fine-tuning에서 동일한 learning rate을 사용했지만 BERT는 task마다 learning rate를 fine-tuning했다.</li>
      <li>BERT도 GPT와 동일하게 마지막 layer를 제거하고 새로운 task를 수행하기 위한 layer를 붙이고 남은 구조의 learning rate을 작게 해서 학습을 진행했다.</li>
    </ul>
  </li>
</ul>

<h3 id="machine-reading-comprehension-mrc-question-answering">Machine Reading Comprehension (MRC), Question Answering</h3>

<ul>
  <li>기계독해 기반</li>
  <li>문서와 질문을 이해하고 질문에 답을 찾는 Task</li>
  <li>SQuAD Dataset
    <ul>
      <li>v1.1 : 질문과 문서에 답이 존재</li>
      <li>v2.0 : 질문이 있지만 문서에 답이 없는 경우도 존재</li>
    </ul>
  </li>
  <li>SWAG Dataset</li>
</ul>

<h3 id="bert-ablation-study">BERT: Ablation Study</h3>

<ul>
  <li>layer와 paramter를 점진적으로 늘리면서 학습하면 성능이 지속적으로 증가한다.</li>
</ul>

<h2 id="gpt-2">GPT-2</h2>

<h3 id="gpt-2-language-models-are-unsupervised-multi-task-learners">GPT-2: Language Models are Unsupervised Multi-task Learners</h3>

<ul>
  <li>transformer의 레벨을 더 쌓아서 크기를 키운 모델이다. 또한, pre-trained 모델은 Language Model Task를 수행한다.</li>
  <li>40GB의 text로 학습을 진행했고 데이터의 퀄리티를 증가시켰다.</li>
  <li>여러 downstream task가 zero-shot setting으로써 다루어질 수 있다.
    <ul>
      <li>zero-shot setting : 어떠한 파라미터나 아키텍쳐의 변경이 없는 세팅</li>
    </ul>
  </li>
</ul>

<h3 id="gpt-2-motivation-decanlp">GPT-2: Motivation (decaNLP)</h3>

<ul>
  <li>“The Natual Language Decathlon: Multitask Learning as Question Answering” 논문에 영향을 받았다.</li>
  <li>모든 종류의 자연어 처리 task가 모두 Quesition Answering task로 바꿀 수 있다는 논문으로 자연어 처리 task들의 통합에 대해 다루고 있다.</li>
</ul>

<h3 id="gpt-2-datasets">GPT-2: Datasets</h3>

<ul>
  <li>Reddit의 글에 있는 문서와 외부링크가 첨부되어 있는 경우 외부링크를 타서 내부 문서까지 모두 크롤링한다.</li>
  <li>Byte pair encoding (BPE)를 사용한다.</li>
</ul>

<h3 id="gpt-2-model">GPT-2: Model</h3>

<ul>
  <li>Layer normalization이 각 sub-block의 입력으로 이동했다.</li>
  <li>각 레이어를 random initialization할 때, 레이어의 인덱스에 비례해서 더 작은 값으로 초기화한다.
    <ul>
      <li>레이어가 위로 올라갈 수록 거기에 쓰이는 선형변환의 해당하는 값들이 점점 더 0에 가까워지도록 하여 위쪽 레이어가 하는 역할이 점점 더 줄어들게 의도했다.</li>
    </ul>
  </li>
</ul>

<h3 id="gpt-2-question-answering">GPT-2: Question Answering</h3>

<ul>
  <li>대화형의 질의응답 데이터(Conversion question answering dataset(CoQA))가 있을 때, 학습데이터를 쓰지않고 질문을 읽고 답을 하게 했을 경우 55의 F1 score를 얻었다고 한다.
    <ul>
      <li>Fine-Tuned BERT는 89의 F1 score을 얻었다.</li>
    </ul>
  </li>
</ul>

<h3 id="gpt-2-summarization">GPT-2: Summarization</h3>

<ul>
  <li>CNN and Daily Mail Dataset</li>
  <li>Article 뒤에 TL;DR(Too long, didn’t read)이라는 것을 삽입하여 TL;DR이 나올경우 앞쪽 모든 문단에 대해 요약하는 Task를 수행한다.</li>
</ul>

<h3 id="gpt-2-translation">GPT-2: Translation</h3>

<ul>
  <li>WMT14 en-fr dataset</li>
</ul>

<h2 id="gpt-3">GPT-3</h2>

<h3 id="gpt-3-language-models-are-few-shot-learners">GPT-3: Language Models are Few-Shot Learners</h3>

<ul>
  <li>
    <p>GPT-3는 GPT-2를 개선한 모델인데 개선방향은 GPT-2의 파라미터보다 훨씬 많은 파라미터와 훨씬 더 많은 레이어를 쌓고 더 많은 데이터, 더 큰 배치 사이즈로 학습한 모델이다.</p>

    <p><img src="https://drive.google.com/uc?export=view&amp;id=1IfMt9cN6S_Shf4wzA4bvgp56jYMtEGjq" alt="" /></p>
  </li>
  <li>
    <p>Few-shot Learners</p>

    <p><img src="https://drive.google.com/uc?export=view&amp;id=12dXEgelAp4Gdc2ha7WPS1i43Ogq4r6iB" alt="" /></p>

    <ul>
      <li>Zero-shot: task-description을 가지고 바로 answer를 답하도록 하는 task</li>
      <li>One-shot: 한 가지 예를 주고 answer를 답하도록 하는 task</li>
      <li>Few-shot: 여러가지 예를 주고 answer를 답하도록 하는 task</li>
      <li>이전에는 downstream을 위해 output layer를 추가하여 학습시키는 과정을 거쳤지만 지금은 변경하지 않고 inference과정 중에 예시를 input text에 example를 포함시키는 방법을 사용한다. → zero-shot보다 성능이 좋아졌음</li>
    </ul>
  </li>
  <li>
    <p>모델 사이즈를 키울수록 zero-shot, few-shot 성능이 빠르게 올라간다.</p>

    <p><img src="https://drive.google.com/uc?export=view&amp;id=1JHHgOz0D46RmVx5SHegfJsfgIHcMjrBZ" alt="" /></p>
  </li>
</ul>

<h2 id="albert-a-lite-bert-for-self-supervised-learning-of-language-representations">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</h2>

<ul>
  <li>많은 pre-trained모델은 많은 파라미터와 많은 레이어를 가지고 있는데 이로 인해 많은 메모리와 학습 시간이 필요로 했다.</li>
  <li>ALBERT는 기존 BERT모델보다 가볍게 하면서 성능 하락이 없는 모델이다.
    <ul>
      <li>Factorized Embedding Parameterization</li>
      <li>Cross-layer Parameter Sharing</li>
      <li>(For Performance) Sentence Order Prediction ← 새롭게 제안</li>
    </ul>
  </li>
  <li>Factorized Embedding Parameterization
    <ul>
      <li>Transformer에는 입력의 dimension이 첫 self-attention 입력에도 유지가 되고 다음 레이어의 입력에도 유지가 된다.</li>
      <li>ALBERT는 Embedding Layer의 dimension을 줄이는 기법을 제시했다.
        <ul>
          <li>기존 BERT는 word embedding의 dimension이 입력의 dimension과 같아야 했다.</li>
          <li>ALBERT는 word embedding의 dimension을 입력의 dimension보다 작게 하고 다시 입력의 dimension과 같게 할 수 있는 layer를 추가한다.</li>
          <li>만약, 기존 BERT의 embedding vector의 크기가 (500 $\times$ 100)라고 하고 ALBERT의 embedding vector의 크기는 (500 $\times$ 15), weight matrix의 크기 (15 $\times$ 100)를 가지게 된다고 가정하자.</li>
          <li>이때, BERT는 50000의 크기를 가지는데 반해 ALBERT는 7500 + 1500 = 9000의 크기만을 가지게 되어 메모리를 절약할 수 있다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Cross-layer Parameter Sharing
    <ul>
      <li>선형변환 matrix를 공유하는 아이디어이다.</li>
      <li>Shared-FFN: feed-forward network parameter를 공유</li>
      <li>Shared-attention: attention parameter를 공유</li>
      <li>All-shared: Shared-FFN + Shared-attention</li>
    </ul>

    <p><img src="https://drive.google.com/uc?export=view&amp;id=1D-sF2dFk_xHfZ76naH542k0eAIkWOYNA" alt="" /></p>

    <ul>
      <li>모두 공유했을 때 가장 적은 파라미터 수를 가지고 성능에 있어서는 하락폭이 크지 않다.</li>
    </ul>
  </li>
  <li>Sentence Order Prediction
    <ul>
      <li>기존 BERT모델에서는 Next Sentence Prediction task는 실효성이 없었다. → NSP를 좀 더 유의미한 pre-training을하고 싶었다.</li>
      <li>연속적인 두 문장을 가져와서 순서대로 SEP을 추가한 데이터와 순서를 반대로 한 데이터를 만들어  binary classification task를 수행하도록 했다.
        <ul>
          <li>Negative sample 또한 두 문서(docs)에서 랜덤하게 추출된 두 문장을 연결해서 next sentence가 아니도록 학습을 진행했다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="electra-efficiently-learning-an-encoder-that-classifies-token-replacements-accurately">ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately</h2>

<p><img src="https://drive.google.com/uc?export=view&amp;id=17k8Gjv6xybgtgjvJW3aYXN-Ymi65hSjJ" alt="" /></p>

<ul>
  <li>Generator는 MLM을 통해 [MASK]단어를 복원한다.</li>
  <li>Discriminator는 self-attention을 쌓은 모델인데 각 단어별로 original단어인지 어색하여 replaced된 모델인지 판단하는 모델이다.</li>
  <li>두 가지 모델이 Adversarial(적대적)관계로 학습이 진행된다. (Generative adversarial network의 아이디어와 같다.)</li>
  <li>이 모델의 경우 Discriminator를 pre-trained 모델로 사용하게 된다.</li>
</ul>

<h2 id="light-weight-models">Light-weight Models</h2>

<h3 id="distillbert">DistillBERT</h3>

<ul>
  <li>Teacher모델과 Student모델이 있는데 Teacher모델은 Student모델을 가르치는 역할을 한다. Student모델은 Layer나 파라미터 수가 Teacher모델보다 작지만 Teacher모델의 output을 잘 모사할 수 있도록 학습된다.</li>
  <li>Student모델의 ground truth probability는 Teacher 모델의 probability가 된다.</li>
</ul>

<h3 id="tinybert">TinyBERT</h3>

<ul>
  <li>knowledge distillation을 이용하여 DistilBERT와 비슷하게 Teacher모델과 Student모델이 존재한다.</li>
  <li>Student 모델이 Teacher모델의 probability를 따라할 뿐만 아니라 각 self-attention block이 가지는 $W_Q$, $W_K$, $W_V$와 같은 Attention matrix와 그 결과로 나오는 히든 스테이트 벡터들 까지도 유사해지도록 학습을 진행한다.</li>
  <li>Mean Squared Error(MSE)를 통해 가까워지도록 한다. 그러나 일반적으로 Student 모델의 차원은 Teacher 모델의 차원보다 작을 수 있기 때문에 일반적인 loss를 적용할 수는 없다.</li>
  <li>그래서 Teacher 모델의 학습가능한 Fully connected layer를 하나 두어서 차원이 다른 벡터의 mismatch를 줄이는 방식으로 학습을 진행한다.</li>
</ul>

<h2 id="fusing-knowledge-graph-into-language-model">Fusing Knowledge Graph into Language Model</h2>

<ul>
  <li>BERT 모델들은 주어진 문장이 있을 때 문맥을 파악하고 단어들 간의 유사도나 관계를 파악하는 것은 잘 하지만 주어져 있는 문장에 포함되어 있지 않은 정보가 필요한 경우에는 잘 활용하지 못한다.
    <ul>
      <li>예를들어, “꽃을 심기 위해 땅을 팠다”와 “건물을 짓기 위해 땅을 팠다”를 학습했을 때 “땅을 판 도구는 무엇인가?”라는 질문에 대해 제대로 답을 하지 못한다.</li>
      <li>인간의 경우 꽃을 심기 위해 부삽을 사용하고 건물을 짓기 위해 중장비를 이용한다는 외부지식(상식)을 가지고 있어 위의 답을 할 수 있다. 따라서, 모델들도 이런 상식이 답을 하기 위한 외부지식이 필요하다고 판단할 수 있다.</li>
      <li>Knowledge Graph는 이런 외부지식들을 담고 있는 그래프이다. “(땅을)파다”라는 객체와 “땅”이라는 객체가 연결되어 있고 다시 땅을 파기 위한 어떤 도구들이 연결되어 있다.</li>
    </ul>
  </li>
</ul>

<h3 id="ernie-enhanced-language-representation-with-information-entities">ERNIE: Enhanced Language Representation with Information Entities</h3>

<ul>
  <li>Information fusion layer는 token embedding과 entity embedding을 concatenation해준다.</li>
</ul>

<h3 id="kagnet-knowledge-aware-graph-networks-for-commonsense-reasoning">KagNET: Knowledge-Aware Graph Networks for Commonsense Reasoning</h3>

<ul>
  <li>A knowledge-aware reasoning framework for learning to answer commonsense questions</li>
  <li>질문과 답 pair에 대해 외부 knowledge graph에서 sub-graph검색하여 관련 지식을 capture한다.</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#nlp" class="page__taxonomy-item p-category" rel="tag">nlp</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#boostcamp" class="page__taxonomy-item p-category" rel="tag">BoostCamp</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-09-17T00:00:00+09:00">September 17, 2021</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/boostcamp/transformer/" class="pagination--pager" title="Transformer 정리
">Previous</a>
    
    
      <a href="/contest/relation-extraction/" class="pagination--pager" title="P2 KLUE Relation Extraction(RE) 대회 회고
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/atcoder/abc250/" rel="permalink">AtCoder Beginner Contest 250
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> May 10 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">A. Adjacent Squares
(H, W) 크기의 행렬이 주어졌을 때 위치 (R, C)에서 인접한 원소의 수를 출력하는 문제이다.
H와 W가 1인 경우를 예외처리해주면 쉽게 풀 수 있다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/airflow/airflow-final/" rel="permalink">airflow 체험기_최종
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> January 11 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">
  이전 글에서 SequentialExecutor에서 CeleryExecutor로 변경하기 위해 삽질한 경험글입니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/airflow/airflow/" rel="permalink">airflow 체험기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> January 09 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">
  부캠 때 에러로 사용못한 airflow를 이제서야 체험해본 것을 정리한 글입니다. (무지성 주의)

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/atcoder/abc233/" rel="permalink">AtCoder Beginner Contest 233
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> January 05 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">A. 10yen Stamp
10엔 스탬프를 모아서 현재 $X$에서 목표치 $Y$를 달성할 때까지 몇 번을 봉투에 넣어야 하는지 계산하는 문제이다.
$X$가 $Y$보다 큰 경우 0을 출력해주는 예외처리만 해주면 된다.
```cpp
#include 
#include 
#include ...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 emeraldgoose. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'emeraldgoose/emeraldgoose.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [['$','$']],
  displayMath: [['$$','$$']],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

  </body>
</html>
