<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Encoder-Decoder &amp; Attention mechanism 정리 - gooooooooooose</title>
<meta name="description" content="Seq2Seq Model">


  <meta name="author" content="goooose">
  
  <meta property="article:author" content="goooose">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="gooooooooooose">
<meta property="og:title" content="Encoder-Decoder &amp; Attention mechanism 정리">
<meta property="og:url" content="http://localhost:4000/boostcamp/attention-mechanism/">


  <meta property="og:description" content="Seq2Seq Model">







  <meta property="article:published_time" content="2021-09-10T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/boostcamp/attention-mechanism/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "emeraldgoose",
      "url": "http://localhost:4000/"
    
  }
</script>


  <meta name="google-site-verification" content="googleb34ce5276ba6573e" />





  <meta name="naver-site-verification" content="naver93cdc79a5629ab3736d7cf8ff7b51d80">


<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="gooooooooooose Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          gooooooooooose
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo.png" alt="goooose" itemprop="image" class="u-photo" width="110px" height="110px">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">goooose</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Dev blog.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:smk6221@naver.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/emeraldgoose" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Encoder-Decoder &amp; Attention mechanism 정리">
    <meta itemprop="description" content="Seq2Seq Model">
    <meta itemprop="datePublished" content="2021-09-10T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Encoder-Decoder &amp; Attention mechanism 정리
</h1>
          
<!--
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  0 minute read
</p>
            devinlife comments :
                싱글 페이지(포스트)에 제목 밑에 Updated 시간 표기
                기존에는 read_time이 표기. read_time -> date 변경
-->
            <p class="page__date"><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2021-09-10T00:00:00+09:00">September 10, 2021</time></p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="seq2seq-model">Seq2Seq Model</h2>

<ul>
  <li>
    <p>seq2seq는 RNN의 구조 중 Many-to-many 형태에 해당한다.</p>

    <p><img src="https://drive.google.com/uc?export=view&amp;id=1HVLlyrlYemel2aojBhxw6Oe8ypoCTdq2" alt="" /></p>

    <ul>
      <li>인코더와 디코더는 서로 공유하지 않는 모듈을 사용한다.</li>
      <li>인코더는 마지막까지 읽고 마지막 타임스텝의 hidden state vector는 디코더의 $h_0$ 즉, 이전 타임스텝의 hidden state vector로써 사용된다.</li>
      <li>디코더에 첫 번째 단어로 넣어주는 어떤 하나의 특수문자로서 <start>토큰 혹은 <SoS>(Start of Sentence)토큰이라 부른다.
</SoS></start>        <ul>
          <li>이런 특수한 토큰은 Vocabulary 상에 정의해두고 이를 가장 처음 디코더 타임스텝에 넣어서 실질적으로 생성되는 첫 번째 단어부터 예측을 수행하게 된다.</li>
        </ul>
      </li>
      <li>또한, 문장이 끝나는 시점에도 특수문자인 <EoS>(End of Sentence)토큰이 나오게 되는데 디코더는 EoS 토큰이 나올때까지 구동된다. EoS토큰이 생성되면 다음부터 단어가 생성되지 않는다.</EoS></li>
    </ul>
  </li>
  <li>
    <p>Seq2Seq Model with Attention</p>
    <ul>
      <li>기존 lstm의 경우 문장의 길이와 상관없이 모든 단어의 정보를 맨 마지막 vector에 욱여넣어야 한다. 이때, 멀리 있는 단어의 경우 정보가 소실되거나 줄어버리는 문제(bottleneck problem)가 있기 때문에 Attention을 사용하려고 한다.</li>
      <li>
        <p>단어를 넣을 때마다 생성된 모든 hidden state vector를 디코더에 제공하고 각 타임스텝에서 단어를 생성할 때 그때그때 필요한 인코더 hidden state vector를 선별적으로 가져가서 예측에 도움을 주는 형태로 제공된다.</p>

        <p><img src="https://drive.google.com/uc?export=view&amp;id=1Uxv60oMwaj6_L3wHl9FDlpwez_rj1pZf" alt="" /></p>

        <ul>
          <li>각 단어가 입력되고 계산된 hidden state vector가 있고 마지막 타임스텝에서 생성되어 $h_0$가 디코더로 들어가게 된다.</li>
          <li>디코더는 start 토큰이 word embedding으로 주어지고 $h_0$를 디코더의 hidden state vector $h_1^{(d)}$를 계산하게 된다.</li>
          <li>$h_1^{(d)}$는 다음 단어의 예측에 사용될 뿐만 아니라 이 벡터를 통해 인코더에서 주어진 4개의 hidden state vector 중 어떤것을 필요로 하는지 고를 수 있다.
            <ul>
              <li>고르는 과정은 각각의 $h_i^{(e)}$와 $h_1^{(d)}$를 내적을 통해 그 내적값은 인코더 state vector와 디코더 state vector의 유사도라 생각할 수 있다.</li>
              <li>각각 내적된 값들을 각각의 인코더 hidden state vector에 대응하는 확률값을 계산해 줄 수 있는 입력벡터 혹은 logit vector로 생각할 수 있다.</li>
              <li>softmax에 통과시킨 값은 인코더 hidden state vector의 가중치로 사용될 수 있고 각 hidden state vector에 적용하여 가중 평균으로서 나오는 하나의 인코딩 벡터를 구할 수 있다. 여기서 Attention output이 이 인코딩 벡터이고 Attention output을 Context vector로 부르기도 한다.</li>
              <li>Attention distribution은 softmax layer를 통과한 output인데 합이 1인 형태의 가중치를 Attention vector라고 부른다.</li>
            </ul>
          </li>
          <li>
            <p>이제 Attention output(혹은 context vector)과 디코더의 hidden state vector가 concat이 되어서 output layer의 입력으로 들어가게 되고 다음에 나올 단어를 예측하게 된다.</p>

            <p><img src="https://drive.google.com/uc?export=view&amp;id=1t6m4xai2cnUGW7TjCVXqmH1e6X8UaCnz" alt="" /></p>
          </li>
          <li>
            <p>두 번째도 동일하게 $h_2^{(d)}$와 각 encoder hidden state vector의 내적을 통해 Attention output(context vector)를 구하게 된다.</p>

            <p><img src="https://drive.google.com/uc?export=view&amp;id=1EKG7hU16-ELwPhBgTZ7ZIjgjI5RzqJqw" alt="" /></p>
          </li>
          <li>
            <p>EoS토큰이 나오기 전까지 반복한다.</p>

            <p><img src="https://drive.google.com/uc?export=view&amp;id=1KW1z7ICYoq1ABBqr1mvk354s2lE4RR17" alt="" /></p>
          </li>
          <li>Backpropagation
            <ul>
              <li>디코더의 hidden state vector는 output layer의 입력으로 들어감과 동시에 인코더의 각 word별로 어떤 hidden state vector를 가져와야 할지 결정하는 attention 가중치를 결정해주는 역할을 수행하게 된다.</li>
              <li>Backpropagation을 수행하게 되면 디코더와 attention 가중치까지 업데이트 하게 된다.</li>
            </ul>
          </li>
          <li>한 가지 중요한 점은 Train 단계에서의 입력과 Inference 단계에서의 입력의 방법이 다르다는 점이다.
            <ul>
              <li>Train 단계에서는 Ground Truth 단어들을 입력으로 넣어주는데 그 이유는 어떤 단계에서 잘못된 단어가 출력이 되었고 그 출력을 다시 입력으로 넣었을 때 다시 잘못된 방향으로 학습할 수 있기 때문이다. 이러한 입력 방식을 “Teacher forcing”이라한다.</li>
              <li>반면에 Inference 단계에서는 출력된 단어를 다시 입력으로 넣어 다음 단어를 예측하도록 한다. 이러한 입력 방식은 실제 모델을 사용했을 때와 가깝다는 특징이 있다. 이러한 방법은 자기회기(autoregressive)하도록 추론을 적용하는 방식이다.</li>
              <li>Teacher forcing방법만으로 학습하게 되면 test 시 괴리가 있을 수 있는데 초반부터 teacher  forcing 방법으로 학습하다가 후반부에 출력을 입력으로 넣는 방법을 사용하여 괴리를 줄이는 방법이 존재한다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="difference-attention-mechanisms">Difference Attention Mechanisms</h2>

<ul>
  <li>앞에서 내적을 통해 hidden state vector간의 유사도를 계산했는데 내적이 아닌 다른 방법으로 유사도를 구하는 attention이 있다.</li>
  <li>$socre(h_t,\bar{h_s})=\begin{cases} h_t^{\top} \bar{h_s}, &amp; dot \cr h_t^{\top} W_a\bar{h_s}, &amp; general \cr v_a^{\top} tanh(W_a[h_t;\bar{h_s}]), &amp;concat \end{cases}$
    <ul>
      <li>인코더 hidden state vector $\bar{h_s}$, 디코더 hidden state vector $h_t$</li>
    </ul>
  </li>
  <li>general dot product
    <ul>
      <li>$W_a$는 두 hidden state vector 사이에서 행렬곱을 하게 하여 모든 서로 다른 dimension끼리의 곱해진 값들에 각각 부여되는 가중치이면서 학습가능한 파라미터 형태인 행렬이다.</li>
    </ul>
  </li>
  <li>concat
    <ul>
      <li>디코더 hidden state vector와 유사도를 구해야 하는 인코더 hidden state vector가 입력으로 주어졌을 때 유사도(scalar)를 output으로 하는 Multi layer perceptron 혹은 neural net을 구성할 수 있다.
        <ul>
          <li>Layer를 더 쌓아서 구성할 수도 있다.</li>
        </ul>
      </li>
      <li>위 수식의 경우 $W_a$를 첫 번째 layer의 가중치로 두고 tanh를 non-linear unit으로 적용한다. 마지막에 선형변환에 해당하는 $v_a$를 적용하여 최종 scalar값을 출력하도록 한다. 중간 layer의 결과가 vector이므로 scalar로 계산해야 하므로 $v_a$또한 벡터로 주어져야 한다.</li>
    </ul>
  </li>
  <li>Attention is Great!
    <ul>
      <li>NMT(Neural Machine Translation) performance를 상당히 올려주었다.</li>
      <li>bottleneck problem을 해결하고 vanishing gradient problem에 도움이 된다.
        <ul>
          <li>이 모델에서의 bottleneck problem은 맨 마지막 hidden state vector에 모든 단어의 정보가 담겨있지 못하는 문제를 말한다. Attention을 통해 디코더에서 필요한 인코더의 hidden state vector를 사용할 수 있게 해주면서 마지막 인코더 hidden state vector의 부담이 줄어든다.</li>
        </ul>
      </li>
      <li>Attention은 interpretablilty(해석 가능성)을 제공해준다. 또한, 언제 어떤 단어를 배워야 하는지 스스로 alignment를 학습한다.</li>
    </ul>
  </li>
</ul>

<h2 id="beam-search">Beam search</h2>

<ul>
  <li>자연어 생성 모델에서 Test에서 보다 더 좋은 품질의 생성결과를 얻을 수 있도록 하는 기법이다.</li>
  <li>Greedy decoding
    <ul>
      <li>어떤 sequence로서의 전체적인 문장의 어떤 확률값을 보는게 아니라 근시안적으로 현재 타임스텝에서 가장 좋아보이는 단어를 그때그때 선택(aproach)하는 형태를 말한다. (Greedy Aproach)</li>
      <li>만약 잘못 생성된 단어가 있을 때는 뒤로 돌아가지 못한다.</li>
    </ul>
  </li>
  <li>Exhaustive search
    <ul>
      <li>$P(y|x)=P(y_1|x)P(y_2|y_1,x)P(y_3|y_2,y_1,x)…P(y_T|y_1,…,y_{T-1},x)=\Pi_1^TP(y_t|y_1,…,y_{t-1},x)$
        <ul>
          <li>$P(y_1|x)$ : $x$를 입력했을 때 출력문장 $y$에서의 첫번째 단어 $y_1$의 확률</li>
          <li>$P(y_2|y_1,x)$ : $x$와 $y_1$이 주어졌을 때 $y_2$의 확률</li>
          <li>입력문장 $x$에 대해 출력문장 $y$가 나올 확률을 최대한 높여야 한다. 첫 번째 $P(y_1|x)$를 조금 낮춰서라도 뒤의 확률을 높일 수 있다면 전체 확률 또한 증가할 것이다.</li>
        </ul>
      </li>
      <li>타임스텝 $t$까지의 가능한 모든 경우를 다 따진다면 그 경우는 매 타임스텝마다 고를 수 있는 단어의 수가 Vocabulary 사이즈가 되고 그것을 $V$라 하자. 그렇다면 $V^t$인 가능한 경우의 수를 구할 수 있다. 그러나 너무 크기 때문에 시간이 너무 오래 걸린다.</li>
    </ul>
  </li>
  <li>Beam search
    <ul>
      <li>Greedy aproach와 exhaustive aproach의 중간에 해당하는 기법이다.</li>
      <li>디코더의 매 타임스텝마다 우리가 정해놓은 $k$개의 가능한 경우를 고려하고 마지막 까지 진행한 $k$개의 candidate 중에서 가장 확률이 높은 것을 선택하는 방식이다.</li>
      <li>$k$개의 경우의 수에 해당하는 디코딩의 output을 하나하나의 가설(hypothesis)라 부른다.</li>
      <li>$k$는 beam size라 부르고 일반적으로 5 ~ 10사이에서 설정한다.</li>
      <li>joint probability에 log를 취해 덧셈으로 만들어버린다.
        <ul>
          <li>$score(y_1,…,y_t) = logP_{LM}(y_1,…,y_t|x) = \sum_{i=1}^t logP_{LM}(y_i|y_1,…,y_{i-1},x)$</li>
          <li>물론 log는 단조증가하는 함수이므로 최댓값은 유지된다.</li>
        </ul>
      </li>
      <li>Beam search는 모든 경우의 수를 보는 것은 아니지만 완전탐색하는 것보다 효율적으로 계산할 수 있게 한다.
        <ul>
          <li>
            <p>아래 그림은 $k=2$인 beam search의 예제이다.</p>

            <p><img src="https://drive.google.com/uc?export=view&amp;id=1ii3SARsgu5Hp83eTqC5UzlJtBDEa1kf3" alt="" /></p>

            <ul>
              <li>
                <p>그림 출처 : <a href="https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf">https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf</a></p>
              </li>
              <li>
                <p>각 단계에서 생성되어 확률을 확인하고 그 중 $k$개까지만 다음 단어를 생성할 수 있도록 한다.</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>greedy decoding은 모델이 END 토큰을 생성할 때 끝내도록 되어있다.</li>
      <li>beam searching decoding은 서로 다른 시점에서 END 토큰을 생성할 수 있다.
        <ul>
          <li>어떤 hypothesis가 END토큰을 만든다면 현재 시점에서 그 hypothesis는 완료한다. 완성된 문장은 임시 공간에 넣어둔다.</li>
          <li>남은 hypothesis 또한 END토큰을 만들때까지 수행한 후 임시 공간에 넣어둔다.</li>
        </ul>
      </li>
      <li>미리 정한 타임스텝 $T$까지 디코딩하여 빔 서치 과정을 중단하거나 $T$시점에 END토큰을 발생시켜 중단하도록 한다. 또는 미리 정한 $n$개 만큼의 hypothesis를 저장하게 되면 빔 서치를 종료하게 된다.</li>
      <li>빔 서치 종료 후 완성된 hypotheses의 리스트를 얻게 되고 이 중 가장 높은 score를 가진 하나를 뽑아야 한다.
        <ul>
          <li>그러나 긴 길이의 hypotheses들은 낮은 score를 가진다는 예상을 할 수 있다. 왜냐하면 log때문에 새로운 단어가 생성될 때마다 음수값을 더해줘야 하기 때문이다.</li>
          <li>그래서 좀 더 공평하게 비교하기 위해 각 hypotheses별로 Normalize를 수행한다.
            <ul>
              <li>$score = \frac{1}{t}\sum_{i=1}^t logP_{LM}(y_i|y_1,…,y_{i-1},x)$</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="blue-score">BLUE score</h2>

<ul>
  <li>기존 평가방법의 한계
    <ul>
      <li>정답 I love you와 예측 oh I love you라는 문장이 주어진 경우 각각의 위치에는 모두 다른 단어이기 때문에 예측률 0%로 잡히게 된다.</li>
      <li>Precision and Recall
        <ul>
          <li>Precision = # of correct words / length of prediction(예측)</li>
          <li>Recall = # of correct words / length of reference(정답)</li>
          <li>F-measure = (precision $\times$ recall) / (precision + recall) / 2 (조화평균)
            <ul>
              <li>조화평균은 Precision과 Recall 중 작은 수에 치중하는 특징을 가지고 있다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>하지만 F-measure가 높다고 해서 그 문장의 문법까지 맞지는 않는 단점이 있다.</li>
    </ul>
  </li>
  <li>BLEU(BiLingual Evaluation Understudy) score
    <ul>
      <li>N-gram이라는 연속된 N개의 연속된 단어가 ground truth와 얼마나 겹치는 가를 계산하여 평가에 반영하는 방법이다.</li>
      <li>Precision 만을 반영하고 Recall은 무시한다. 정답에서 몇 개의 단어가 떨어져도 비슷한 해석이 가능하기 때문이다.</li>
      <li>BLUE = min(1, length_of_precision / length_of_reference)$(\Pi_{i=1}^4 precision_i)^{\frac{1}{4}}$ (기하평균)
        <ul>
          <li>min(1, length precision/length reference)는 brevity panelty라 부르는데  길이만을 고려했을 때 ground truth 문장과 짧을 경우 짧은 비율만큼 뒤에서 계산된 precision을 낮추는 역할을 한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#attention" class="page__taxonomy-item p-category" rel="tag">attention</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#boostcamp" class="page__taxonomy-item p-category" rel="tag">BoostCamp</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-09-10T00:00:00+09:00">September 10, 2021</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/boostcamp/recurrent-neural-network/" class="pagination--pager" title="Basics of Recurrent Nerual Networks (RNN)
">Previous</a>
    
    
      <a href="/boostcamp/transformer/" class="pagination--pager" title="Transformer 정리
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/rnn-impl/" rel="permalink">python으로 RNN 구현하기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> December 28 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">Related

  이전 포스트에서 CNN을 구현했고 이번에는 RNN을 구현하는 과정을 정리하려고 합니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/data-engineer/data-pipeline/" rel="permalink">데이터 파이프라인 구축해보기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> December 02 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">Motivation

  빅데이터를 지탱하는 기술을 읽다가 데이터 엔지니어링에 사용되는 플랫폼들을 전체 파이프라인으로 구축해보고 싶어서
이 사이드 프로젝트를 진행하게 되었습니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/cnn-implementation/" rel="permalink">python으로 CNN 구현하기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> September 21 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">Related

  이전 포스트에서 MLP를 구현했고 이번에는 CNN을 구현하는 삽질을 진행했습니다.
여기서는 Conv2d의 구현에 대해서만 정리하려고 합니다. 밑바닥부터 구현하실때 도움이 되었으면 좋겠습니다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/dl-implement/" rel="permalink">Python으로 딥러닝 구현하기
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> July 04 2022</p>
    
    <p class="archive__item-excerpt" itemprop="description">
  모기업 코딩테스트에 파이썬 기본 라이브러리로만 MLP를 구현하는 문제가 나왔던 적이 있습니다. 당시에 학습이 되지 않아 코딩테스트에서 떨어졌었고 구현하지 못했던 것이 계속 생각나서 구현해봤습니다.

</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 emeraldgoose. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'emeraldgoose/emeraldgoose.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [['$','$']],
  displayMath: [['$$','$$']],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

  </body>
</html>
