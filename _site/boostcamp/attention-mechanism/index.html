<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Encoder-Decoder &amp; Attention mechanism 정리 - gooooooooooose</title>
<meta name="title" content="Encoder-Decoder &amp; Attention mechanism 정리">
<meta name="description" content="Seq2Seq Model">


  <meta name="author" content="goooose">
  
  <meta property="article:author" content="goooose">
  


<!-- open-graph tags -->
<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="gooooooooooose">
<meta property="og:title" content="Encoder-Decoder &amp; Attention mechanism 정리">
<meta property="og:url" content="http://localhost:4000/boostcamp/attention-mechanism/">


  <meta property="og:description" content="Seq2Seq Model">







  <meta property="article:published_time" content="2021-09-10T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/boostcamp/attention-mechanism/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "emeraldgoose",
      "url": "http://localhost:4000/"
    
  }
</script>


  <meta name="google-site-verification" content="googleb34ce5276ba6573e" />





  <meta name="naver-site-verification" content="naver93cdc79a5629ab3736d7cf8ff7b51d80">


<!-- end _includes/seo.html -->




<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/" itemprop="item"><span itemprop="name">Home</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#boostcamp" itemprop="item"><span itemprop="name">Boostcamp</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Encoder-Decoder & Attention mechanism 정리</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo.png" alt="goooose" itemprop="image" class="u-photo" width="110px" height="110px">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">goooose</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in ML/DL, Data Engineering.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:smk6221@naver.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://github.com/emeraldgoose" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
          
            <li><a href="https://www.linkedin.com/in/minseong-kim-84428b231/" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://gooooooooooose.tistory.com/" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Problem Solving OJ (KOR)</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Encoder-Decoder &amp; Attention mechanism 정리">
    <meta itemprop="description" content="Seq2Seq Model">
    <meta itemprop="datePublished" content="2021-09-10T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Encoder-Decoder &amp; Attention mechanism 정리
</h1>
          
<!--
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  0 minute read
</p>
            devinlife comments :
                싱글 페이지(포스트)에 제목 밑에 Updated 시간 표기
                기존에는 read_time이 표기. read_time -> date 변경
-->
            <p class="page__date"><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2021-09-10T00:00:00+09:00">September 10, 2021</time></p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="seq2seq-model">Seq2Seq Model</h2>

<ul>
  <li>
    <p>seq2seq는 RNN의 구조 중 Many-to-many 형태에 해당한다.</p>

    <p><img src="https://lh3.googleusercontent.com/fife/ALs6j_FjoMbCbBNLFL9qYDHbR6PBItpGgxqKC_MzDRjlMYZucpQIQxGq2_8nd91fYtK6ik5uuKLUlawUuPqEWQgKdFxobV9f2vumFrt0m1NPBILWyLPsrGgjht-zASPm67dcoGKB09oavexSvDBS7hqLlYNmZxXj17R4Wr1Vn5CLz2X2DzazMleB9UJC-F59huD0M4zP9JjrrmmBEWycYu0-MhfRbcO8VKP43zsFgLgxTPZWtjqD2Hs4vmKc6G7_SdftL7-_lX8gpOMrS3hJ3R-WoY2p9qWx45m6E3Ws-0xiPDUBbA5lqe0yRf1LmBkS4rhNDkeDDpsjzKIGwsMPBwTJZCY0UpdbwFxweAkyZ_Bhr83fWu0lZaZhJdlL08TRDlKaEsjMTEyEWLgwjVRm0fYUWO35EsIhqzfloLQsuigrI_ACgSov2Kdu61OizsA7pECgAf8MYgzzKQ9IA5vHO6YSxGrbvqPeTwGZSJi2swaRe_JydyL7Ls51WANh8JbHtFjuh0-hRGPFrtXsL_ZXmm7gnb-4ki6xo55QIDyMWLYaI_Pr_o8euOgwoEOqP9w1SGmm5-PMPh0tbhIGoq6S2Ds0UdkRCAOuMDPArhiwyFEikk2j9l1JequOMWY_lM-kRthbXnBejGidCsLYQubJm7fRcG1LkkRcYrFXevfs-m8IgK1GmC0tHTU6sAJQHZY5g1n_GlhD_j-cptdQVYY46Irgl0vcEOz_qqfXzXfQCVYpQ2kU2Va9d2BRU6qlcWyUl65gmOvD1kOex4aWttLXEWSUbBzBbby-BiezBhsYepwspW_1LQK_artMR0_emr8oTYDy8wFpE2d1jbI-gdIsdxUoX-W0ZhBX6q2kHPamC4-THg3hzhWTuyhyE4oH6TeGgPLcj7YnM09fyP0wsHr8ObUTFP3w77XX16fb-xRljvO4IGgrUuKH2-8CEaMylpaIGGmmKVfHpCndBvApoY4mMzmS2LfF8T56c8cPYgXfwKhaapYhuCFLWBLdjr6QpP6MLoQJ4Chcbj_eg44JoRxSy1QlbJqavmHE7EpAfwSmkvbw_SbwTW4I_u1VZDKj7iSNZ2T02BXkUSobM4wVuTPeHgvYdtzaIm-9TC1aawvS5NPl7MSVihN0vRcc7wPSTv6phM5JqOzEuBf4kfFpp1ThLri2IeEsZ4T2UkyK8YKfBbtQEaZhwzBunXWduBoyxdjz1mueE69_-_UzKlpgreR4xNe0Vx1dXk7LuTk1Nw_alVzWNUzJP9j_N29GFmmLEGn1tiLWRoBKsxLJ8w176lR_Xy2Gfn2mIgXdTaKWe_z8TwtH2yby0-uJPK2eBf_SrjEi6-8H5eOUs7Tz6N1R8nPRXQXDotoLYf5IezU3klLcydQg6F6m4Z04UgK9kDG3fWKGp883m3LL-LZ50TCD5uFWWsw2tmgxUqC_EdaijTXEn8ZhPE5BuUUvmqJlUfxLmR8NDrdTF6FNNbNfHTF_0wOdc15ThK3mNN7wWjs8x9n3PGrZm2apS2GRkQZkqUJXRmhtt4HEL1SI_OdJOGUTceVUTExcgrLSl7tjvzKEC4LKJRadyYIP4SUVQNAOkYfBlG8uywV_lj7LGwJUwHsle2FxTr_x8evsBrsDpitx0ArmwzyPU2MxDa8n" alt="" /></p>

    <ul>
      <li>인코더와 디코더는 서로 공유하지 않는 모듈을 사용한다.</li>
      <li>인코더는 마지막까지 읽고 마지막 타임스텝의 hidden state vector는 디코더의 $h_0$ 즉, 이전 타임스텝의 hidden state vector로써 사용된다.</li>
      <li>디코더에 첫 번째 단어로 넣어주는 어떤 하나의 특수문자로서 <start>토큰 혹은 <SoS>(Start of Sentence)토큰이라 부른다.
</SoS></start>        <ul>
          <li>이런 특수한 토큰은 Vocabulary 상에 정의해두고 이를 가장 처음 디코더 타임스텝에 넣어서 실질적으로 생성되는 첫 번째 단어부터 예측을 수행하게 된다.</li>
        </ul>
      </li>
      <li>또한, 문장이 끝나는 시점에도 특수문자인 <EoS>(End of Sentence)토큰이 나오게 되는데 디코더는 EoS 토큰이 나올때까지 구동된다. EoS토큰이 생성되면 다음부터 단어가 생성되지 않는다.</EoS></li>
    </ul>
  </li>
  <li>
    <p>Seq2Seq Model with Attention</p>
    <ul>
      <li>기존 lstm의 경우 문장의 길이와 상관없이 모든 단어의 정보를 맨 마지막 vector에 욱여넣어야 한다. 이때, 멀리 있는 단어의 경우 정보가 소실되거나 줄어버리는 문제(bottleneck problem)가 있기 때문에 Attention을 사용하려고 한다.</li>
      <li>
        <p>단어를 넣을 때마다 생성된 모든 hidden state vector를 디코더에 제공하고 각 타임스텝에서 단어를 생성할 때 그때그때 필요한 인코더 hidden state vector를 선별적으로 가져가서 예측에 도움을 주는 형태로 제공된다.</p>

        <p><img src="https://lh3.googleusercontent.com/fife/ALs6j_HVNYGFWHslEwRdPetqXanZqJv6Qg1SMkewfImtTZsDiJJer7B0HXL3GI1u1h04dOveexTucPJS4jNzBSAaBkeEjIvwDERA5cV_hLwSArIssdqLXTdOo3u4uQwhYgpcJrHpRS99TFI8yUyqO6x0RXcu8r3ptSj1nZtgYCAG6VUyRE3z6xSantSHaQP4e9pjsV3gHT2x049ZdHYzz6h5RasnslHc3UL6iGH1JvlHZzB4dVY3iGB1LEYptAVnDXuSGwtJJRfub2bdrxlc9uRtP3QZkY3SBlyWEMjzXUsd-2pVNTYkFsU0tRzfqHGX1uslDsZkFxk-08ogaLPK1V7svGI46cxSxdwR_jtjnhfgU0J9UDmwiQyciw9GRF3e7k8AJwcMM9TloYMIaqqpo2Gze4DDVf8GGBbX6tQphsWHYfnIG_QW0k73PA3HXl32tuYIJG8lm2X6U3U0siEo5BJgg8OhHsgm1dQAGy5NULF9oGm7cR3KTbZDuvP-JtsTRDKwBT5Jzl65_GIFvPPHfMHOxzgRcbtFTj-YZhdXwkPm4VDFyNkT0A112QeV7me-geplnN2eu0VbdFzYKAbY6Siu0EDeh30XZkSx0wRQWOIuypbDJgQjVivVyawUMIU99rvK4T4tbv_Xx4ztN6dlpy-_c3RXHwFcrOoZBaGOnxYCoITjq8ndLc_HNKLJuAbtcc9jVs5doMJwOsgNG6Xp55mPDpWa52Wjr5GPcrq3uMEogO3pZ-xeK4CNSErfATm-Z0rY8ILvAnu8cfYrG6nILihqtAT5WXgkgYHgFqnih8SGi6a2uiR95x9nE6u4fu_Ye2uRvsdBLZ6ad_EzSno0nJ3-Xo12Z84tDF4xy3ApOKiFAyCwkp0Ddr_aed73Mn9xnzyXsWfRk3auYIlUnLi-UmHg2u-tAz_trxMRBWqJyYHWYOuRP0a3H_CJyCJ0NlTH9wn6ABafcHiu2tMVmZo6AYA_imeSumIybaigSLJQMW4Jl9gABWe4lyOvEWH0r7SFUDo_62eRvWCtZiouRMB6k9DbX6Q6PCsRwQirPsDTVYv3fN952OJKCDKj6Lh2iOabUHyXG7lUwai2pD--9ZLJHbN3FC6S0VoRyEzPTLtqLcVGdsyonvF2xxLZX64ugCo92ZX-t0TncwUjgppejizuqDuqbm9r25Unt_Vw0ULhwz27qDSUfMDlSWN2eCpqCrquwcmxq7nJHfIvyx5Vu7qLkMIlbD4okzQWtHPx8Dl1HlHBeNA5TSBkWGNfJvzBfs3dTn0Am8S8SEcz5NY_AXUtmbp6kaZ8GOYYX7d-EbTc9CdMCaDKBNEc5iALhNTe2PjNHK_GqcTYsiQPNRi3RzfEPeF4IJxbfbGCTma_QLp1GbMn5xldK3MW9_lTz0Mm0ZexK4kX2scBj_yAfd4wXCpdKb9yT7RwHPqeQJ7dl4FwVO6srqB3ayA-HSQyGJxVIUGZaJaMr0vUFSQVcCoIpHhCb4SlMVn_89tDefwR7-KLX-IeP8FR8RKjvJDSj4T2OvZJcrWfLkZaJCi3ijgUp5g0SKhb_ZMTFQn281S8hjnzVEg6T3ckryusd7OfMl5UU7P1qiUCHLS6JaHAr9DJnsC_IX3QqlBz-Bnc2iZVt0hBDoal_TURc0z1AA" alt="" /></p>

        <ul>
          <li>각 단어가 입력되고 계산된 hidden state vector가 있고 마지막 타임스텝에서 생성되어 $h_0$가 디코더로 들어가게 된다.</li>
          <li>디코더는 start 토큰이 word embedding으로 주어지고 $h_0$를 디코더의 hidden state vector $h_1^{(d)}$를 계산하게 된다.</li>
          <li>$h_1^{(d)}$는 다음 단어의 예측에 사용될 뿐만 아니라 이 벡터를 통해 인코더에서 주어진 4개의 hidden state vector 중 어떤것을 필요로 하는지 고를 수 있다.
            <ul>
              <li>고르는 과정은 각각의 $h_i^{(e)}$와 $h_1^{(d)}$를 내적을 통해 그 내적값은 인코더 state vector와 디코더 state vector의 유사도라 생각할 수 있다.</li>
              <li>각각 내적된 값들을 각각의 인코더 hidden state vector에 대응하는 확률값을 계산해 줄 수 있는 입력벡터 혹은 logit vector로 생각할 수 있다.</li>
              <li>softmax에 통과시킨 값은 인코더 hidden state vector의 가중치로 사용될 수 있고 각 hidden state vector에 적용하여 가중 평균으로서 나오는 하나의 인코딩 벡터를 구할 수 있다. 여기서 Attention output이 이 인코딩 벡터이고 Attention output을 Context vector로 부르기도 한다.</li>
              <li>Attention distribution은 softmax layer를 통과한 output인데 합이 1인 형태의 가중치를 Attention vector라고 부른다.</li>
            </ul>
          </li>
          <li>
            <p>이제 Attention output(혹은 context vector)과 디코더의 hidden state vector가 concat이 되어서 output layer의 입력으로 들어가게 되고 다음에 나올 단어를 예측하게 된다.</p>

            <p><img src="https://lh3.googleusercontent.com/fife/ALs6j_G2xL6cfqUQVQpzpcJRKYtKwAsUYZlHRTAIrDmccq7XbI5MXghDUkmhaRVv5_96vzy86MsiFsTNA4C9WRTpVOZZr8z0FegbtZRPzwukaZAIc0Vk3sZBycv6zz3Q_RV9XKPg741feN4Fg0HqTaLX3RkXk1qNgEMBydTkHjULSSApZDvRNLFEoH_dv-f3ZGKFhX1hmVfiMAj_de_brz-wyLxuV-0B9bLaORzohCosZkCYHQreCeAVkn--LB8CJMs5YrTEUBhZ3viI0yr4QC3PwclK6WN4TBUSukifqKqBASSGg4oOqrsy5ZiZHmVCKe4iwE5WipaQH4dWmiwt3mNypTevnpNCAYMA6g8ljQZyoY06BIJI04t8UCpyHWbgBx45dDQ4O1WPFAxPO27ZhMJRSucA9lIQjSsuzowHXdD2SCyfWDHm4cUh9nw6jTaK1nR7rEISCLoFXbxphT7YPjDC10nXnn-uBLNzwiVGiFlli4bdctPm_6Q7nQR9TZbz0itFHUNwMcAKaMgN_OwtNzq5ZIwHDPo7pEs5bS_F2MLlWq8jwX_m-lcKI2W1CXthv0RUW_anx2HA1z6Xw4_fgTVEvdhbhfQRV5_uYDh70p6MztOS16_CqigKKwJurNswBpDPpdiJAslTfxMS2FsfRnHxpsjXndGfxZ29edGZCwOukGbCFoRjCr4U8CDFViubIjal2VLEwa0-DwZpwAazBmLTjLsouQlDxSQoj-gYzmbhN-YA17a4lWqodURlz1WvfdVULoU1ZhekeId9_O5YUGtzlfQs5Y1EvmAJsw6OnXqYMOh1WHXOXn7LjDsgfm_1yu7uMmYBD7ll8As3rbTteDt8frqHrhEq4XkUoF4Jo463a_cRcae_V4ps8Y3HRHbrNoqJF3WwlsI326ucid1-u3pqiUFG5fYywJirLIiJDWdtJTHPfelRLIJCxmSI79EH32Qytn00j_9LzUgYA4ORcmAt4WnoqkYJ17Nl_1Y_yZlh9EX0goOBH4g7iTZqX5OTo3kluv60Qyk1ZX920LAkDYdiLtpdxLGdNFoOFlEiX6xvlt_29hEUuKxtP4I68-xCCyX4hXyjL6SHQ_UP8cr3Bu__HjuLjj3Nbee71gYEUzsLy5oSa0mjkZuVB8eAn1QPR-aN6DtXm2PVUE8wPoK4DDzl-DQS5jQ2dhzqeiaz9t4-9LAAMXu2-JJ2yziCHskerDQ2alVYHEheKmrPizBaFgTsjTuHs4eqUpPt530GjTmLCyDqSTznmbWmrmQ4jWcp838G3TBY3MX29MLyiWSBQSBdUz79mVBYNMPw0ciXlx0obVHnKCMZH5JrfYeZWkUDddZTmcevfYJO_mW0DQ9yyrt89GVoqA-5ojEnDharHazgbcMwRiVYatNsJDb9FAO_HfJMI0ni2d6D9FW1gn7UrfLzlFzgdirbhw6rGwPSuIutrVCfcgboDXMVQKAK9scj8-G6Y48hvaSTj92MhXKqg9QrAhpC7Kn-O8stUvFP-hNr0ElwKPh5iUBZjglTFXIAS2eTu4kvM62KBW3165S89voqxZp0_lbF-2JjdfEoQ21gihp_i0LTlwaHuS6H9mGAfuAtesybNzzmW9ECt0onxgwLw-zUXLf8RjcEPmaX-nKTGDUaAedjaw" alt="" /></p>
          </li>
          <li>
            <p>두 번째도 동일하게 $h_2^{(d)}$와 각 encoder hidden state vector의 내적을 통해 Attention output(context vector)를 구하게 된다.</p>

            <p><img src="https://lh3.googleusercontent.com/fife/ALs6j_ExbRyimMVZIaoe_KqAS7bEL97m-N-fYHMc6kPzklpiX5aIazZrKxB_RBv-uvu0zWymA-vriS2tv6KBTFL-E0fYPdjo-iqo3UoBp5z6zs23_ye3k6BZFuXi3PYiBs70ZKmrOBJ6lvBZ3TCboFfkTAucfZ5Qe19dwWvmJAB52E8XOiTv2QHLKyJ9q9O3Vycmb6th6EnSflQE53gMFn_5hKWt8bzRjId0MbWD1wy0d7TtVnf4uTXk7wUW46Cwl17qeku9A1KCXwrk05LDkp8eyddK3ACZsMwDRsgIhbsHplq8MPw641LF5ic_vXNs7BA8HS97jZ_2Ia8KCfUlmLA-fWa8mhwGt-fJhXM6yw4zqtOKJ3UiXTDfhKhl87e5Kw_eSaH1WA3BfpJ1za6AQpYANALR2D0kE2wUJwc_Rj7tEXivm4rdeYgyvNMRb4SMMDsUQL3RVfzV6ccfyH_KNPFScOde_2nxFKmmKHUWAKs-p3e3m1r6fmd7pdn5cRepLHHgg6D36WWCXnWi2vplBK3fzBi8vBDbpq4EG25EOz46OINaRujyfhcPma-EKCKn7-bN_LrHkRECJtJaCB-EEYSlU3BMjmVipzrFA7UVdjGl_3gQAS1AhjEO0hSkUBKbxw0TRkKe_DrXKy6PSKzvCx3N042NxFjWpgUNSCeun6AmrBwn4Xt75_LVhuVwNZHfB_9iOEB_RIjmxb0YI1Kkngss1Uo_1TYSjESy1-qPBL08tfzyhCjU37Bj_xtlS47BX4qqRBu9n38Q4zR8aeT9LutMCByJBCMWykkGUKQvqFJOilhOuR4JGt0h0FCK7n2vcUhepyPwxO93ttLV4X6PUBfU5bxOlToJxVC4YHiaT6HFd6WubHNr3fSox8n7_bpQv913pxw0cDSvhoo1_W4dTAV_hf1PcuveMttb5jk88puFfnQo8HzlmDji53W39k14ZxbShFKLevB-Oh3V81xsmnZ6a1MWvA7zkIxuChlEWR-OKUsNbe2jEd14E0NuNUzd-NNYFa_awRXSRn7MvDT_3lNwy7ICoh4VCuP22nH8AQHDXZrI28rNoBFAKf8cAX2tqLsMN0H34Th68Rtz3_JtYCvQvmdtYCChTUQO1KgC5KPJp3wdIjt5-cacSiaJtut2XVAL2ws9TJdW4oLo2XQvJvmrwKQUPJYHVXUWs0pEr1mNk-1RT8NXumsmmdP-hsOOxZtIQt_cGanQpJfjHXCqNjBxqeZhVFQiDG9mxAsbHze3kV5WC86AdjMWyHoILA9YWbyXEJk299vZqvZEQI35uVcLGln3Bkph1N6wuBasB_cdegpf_xOyzdzNQKk9cuaYVKQAM6eJ7yi0oVO9EfJtx_GhLoFdANFVjH0EhCZJdJOPigjVCDFieUWGi7tTwfPXEgyYf5i0tkhfb-Ngt2Ar6-SHB1TV-UQZNFl0fPEIU2nxqzkbDyPk5qvxTb7LpYFDAFvMQfou2BZPsZKOUYCKuBwKh0NkNPtTU4tFJR14bWNbpiMI7Wjb3oOgPNIOb3h4tvbhttdUl0UsrQ8rOE2vz7WmUuQqoi8BF2HnlRQ_klOiYWvDz_2AElHVKgBJlKG6W71kSFrqUF1XIkxgNHZzGYJg39BxD5SRB-Ga0g3YeGMPVp6gam61vw" alt="" /></p>
          </li>
          <li>
            <p>EoS토큰이 나오기 전까지 반복한다.</p>

            <p><img src="https://lh3.googleusercontent.com/fife/ALs6j_HKu2Nsc2jQznFoc3iGI5lkRo4x9xFT6iqAmtgPCwcdd4IOy4dsf5GGrjHeRPryu5dg_20VM5sjTPN7YtdsTv5Xoxo4f3Abvnck0NV5yfjnfHAjGXqo6dmb8f1MK1KriUd304QcOHqyQahgY5nMCeyph2x9wThVuRX3D-5OMD9HnUU1s-HqixCmg0M7G95xDVlpADjIfSOMuT4W1EN2xUujXKyvJIpB2-hQBXwt4aiwAzAJ0bHm9xS1SR3f2kf_7_VAdJGyZ8Hsl9ZQk63EWWiGyZa_jGPI_Mw90V80SA9mvAfxnsDWtPCUVxxYQW2KLRNl8jZCWHqZx-g4ebfGNCAO9H4c7EMfLQt4cXGshMJJ3ZPrtxdKlqipf5_-M_sBql4puQtwxoMOPW6R3H1lMhTcMMcJs6r_jYWXFnElT5L98kR7hpFKb8_FtJjwk8wXkRbeh3-dQltp7VGrHbecA8zHlHLXFuntWUVHWG1AwwFPIGz1Q_czH7YmquBajUtRpOiX_KHAwHb_Rd_u9kbNL4hasZVqOqNoB25AYCpJEW-AJCrvozYs4VckxjEVKIVQAaUPxc_rlyNmC1fNhTvNBz-_Wdp8D8ZSP-VrWCXViSznkGJ87wrjVDJEEBc9y9-ln4cXiYV37YV-5ObmSQoyOUVZOVkuXqxMX737hh89zRjdE1f8nlgoqW7hMNC42bu1Emr-3iDNFabSv3HVUjkG_tfGGx2z7IHGVPWEK55b09Op6ZIaeJBc9moro2tPZaHXhapnwZzaJMujREpolsrqIHBaH6kwLvMdWMlxhrIcEApcj-Jk4CuNltkTKc88cANrp51hz_HdAXWMBqbDK7ll4hd2P5RIx-FPMHa2sSo5YQHSI-gN6GhNy37gP4a_0Bi0UPrWbRw7hp4LIMZon2F6C-6vCeZQYJNBcbxVolxQwBw_-eUbCov37tW5vAfOoGS2GMCbpX9duUktCzXTzLbVcJye3yL3f9EQ45LM5jEhmRERt_NpDyKOmwFALuhi3hQRKXdJ2ymmAwftJRclt0Y4hyQbdqQaC32gXRWUN_0hePXI3D_6HDe4gkWObssJyIyh8ArmhLT4kaBtxMDqNuEhViIdrotkZ21s0a7Zjh4c9w1pZb_eOYwcTw-ZuWQNL9KY-l_LoqiJhluE8ewaNbj2-GMl2lLIPPwQo7KfjwkEDnLPq0VHp9nPRkdYuqLxpwQqcNu3AAB3XNN3jg4Xr_jz6VAFDEc_mkwBSmPOwU6FZ-6rMIcOfmoRDkPysZEuquV-79_qXaLd7o2AFz2xx-AYaRq1zjDD8BFG0cIQDG4qxcIve_oKjSCAi6Wq64TEX5917sfPXFfF9GgNLSyKLiJ7eLjp7WWC7oqnGllcrVEct3JUe-ezxgVjdFCMOFZuSm_ynKh0p6ht-SrHid3mNrAGijLY89SU4O9wrx0CEuflBRKL6zzij6i_NGGnCNDUBsxeuFuNCFD7D98Ie5RkcYZvBZRYkGoznIMBLn7xfbEfq5sabZchcQVO-aka4eoS7KX4J9bp109hpX-xhx0Iwga9JDjVPt7pnp6a7-VrWWHt-PCuytWRXX3VuVcA85W4oSC_Q6ZWS92Y3Gvo_hyUUtM1O-LgwVDOLLtnBaS2u2EZ8IurfS6X8w" alt="" /></p>
          </li>
          <li>Backpropagation
            <ul>
              <li>디코더의 hidden state vector는 output layer의 입력으로 들어감과 동시에 인코더의 각 word별로 어떤 hidden state vector를 가져와야 할지 결정하는 attention 가중치를 결정해주는 역할을 수행하게 된다.</li>
              <li>Backpropagation을 수행하게 되면 디코더와 attention 가중치까지 업데이트 하게 된다.</li>
            </ul>
          </li>
          <li>한 가지 중요한 점은 Train 단계에서의 입력과 Inference 단계에서의 입력의 방법이 다르다는 점이다.
            <ul>
              <li>Train 단계에서는 Ground Truth 단어들을 입력으로 넣어주는데 그 이유는 어떤 단계에서 잘못된 단어가 출력이 되었고 그 출력을 다시 입력으로 넣었을 때 다시 잘못된 방향으로 학습할 수 있기 때문이다. 이러한 입력 방식을 “Teacher forcing”이라한다.</li>
              <li>반면에 Inference 단계에서는 출력된 단어를 다시 입력으로 넣어 다음 단어를 예측하도록 한다. 이러한 입력 방식은 실제 모델을 사용했을 때와 가깝다는 특징이 있다. 이러한 방법은 자기회기(autoregressive)하도록 추론을 적용하는 방식이다.</li>
              <li>Teacher forcing방법만으로 학습하게 되면 test 시 괴리가 있을 수 있는데 초반부터 teacher  forcing 방법으로 학습하다가 후반부에 출력을 입력으로 넣는 방법을 사용하여 괴리를 줄이는 방법이 존재한다.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="difference-attention-mechanisms">Difference Attention Mechanisms</h2>

<ul>
  <li>앞에서 내적을 통해 hidden state vector간의 유사도를 계산했는데 내적이 아닌 다른 방법으로 유사도를 구하는 attention이 있다.</li>
  <li>$socre(h_t,\bar{h_s})=\begin{cases} h_t^{\top} \bar{h_s}, &amp; dot \cr h_t^{\top} W_a\bar{h_s}, &amp; general \cr v_a^{\top} tanh(W_a[h_t;\bar{h_s}]), &amp;concat \end{cases}$
    <ul>
      <li>인코더 hidden state vector $\bar{h_s}$, 디코더 hidden state vector $h_t$</li>
    </ul>
  </li>
  <li>general dot product
    <ul>
      <li>$W_a$는 두 hidden state vector 사이에서 행렬곱을 하게 하여 모든 서로 다른 dimension끼리의 곱해진 값들에 각각 부여되는 가중치이면서 학습가능한 파라미터 형태인 행렬이다.</li>
    </ul>
  </li>
  <li>concat
    <ul>
      <li>디코더 hidden state vector와 유사도를 구해야 하는 인코더 hidden state vector가 입력으로 주어졌을 때 유사도(scalar)를 output으로 하는 Multi layer perceptron 혹은 neural net을 구성할 수 있다.
        <ul>
          <li>Layer를 더 쌓아서 구성할 수도 있다.</li>
        </ul>
      </li>
      <li>위 수식의 경우 $W_a$를 첫 번째 layer의 가중치로 두고 tanh를 non-linear unit으로 적용한다. 마지막에 선형변환에 해당하는 $v_a$를 적용하여 최종 scalar값을 출력하도록 한다. 중간 layer의 결과가 vector이므로 scalar로 계산해야 하므로 $v_a$또한 벡터로 주어져야 한다.</li>
    </ul>
  </li>
  <li>Attention is Great!
    <ul>
      <li>NMT(Neural Machine Translation) performance를 상당히 올려주었다.</li>
      <li>bottleneck problem을 해결하고 vanishing gradient problem에 도움이 된다.
        <ul>
          <li>이 모델에서의 bottleneck problem은 맨 마지막 hidden state vector에 모든 단어의 정보가 담겨있지 못하는 문제를 말한다. Attention을 통해 디코더에서 필요한 인코더의 hidden state vector를 사용할 수 있게 해주면서 마지막 인코더 hidden state vector의 부담이 줄어든다.</li>
        </ul>
      </li>
      <li>Attention은 interpretablilty(해석 가능성)을 제공해준다. 또한, 언제 어떤 단어를 배워야 하는지 스스로 alignment를 학습한다.</li>
    </ul>
  </li>
</ul>

<h2 id="beam-search">Beam search</h2>

<ul>
  <li>자연어 생성 모델에서 Test에서 보다 더 좋은 품질의 생성결과를 얻을 수 있도록 하는 기법이다.</li>
  <li>Greedy decoding
    <ul>
      <li>어떤 sequence로서의 전체적인 문장의 어떤 확률값을 보는게 아니라 근시안적으로 현재 타임스텝에서 가장 좋아보이는 단어를 그때그때 선택(aproach)하는 형태를 말한다. (Greedy Aproach)</li>
      <li>만약 잘못 생성된 단어가 있을 때는 뒤로 돌아가지 못한다.</li>
    </ul>
  </li>
  <li>Exhaustive search
    <ul>
      <li>$P(y|x)=P(y_1|x)P(y_2|y_1,x)P(y_3|y_2,y_1,x)…P(y_T|y_1,…,y_{T-1},x)=\Pi_1^TP(y_t|y_1,…,y_{t-1},x)$
        <ul>
          <li>$P(y_1|x)$ : $x$를 입력했을 때 출력문장 $y$에서의 첫번째 단어 $y_1$의 확률</li>
          <li>$P(y_2|y_1,x)$ : $x$와 $y_1$이 주어졌을 때 $y_2$의 확률</li>
          <li>입력문장 $x$에 대해 출력문장 $y$가 나올 확률을 최대한 높여야 한다. 첫 번째 $P(y_1|x)$를 조금 낮춰서라도 뒤의 확률을 높일 수 있다면 전체 확률 또한 증가할 것이다.</li>
        </ul>
      </li>
      <li>타임스텝 $t$까지의 가능한 모든 경우를 다 따진다면 그 경우는 매 타임스텝마다 고를 수 있는 단어의 수가 Vocabulary 사이즈가 되고 그것을 $V$라 하자. 그렇다면 $V^t$인 가능한 경우의 수를 구할 수 있다. 그러나 너무 크기 때문에 시간이 너무 오래 걸린다.</li>
    </ul>
  </li>
  <li>Beam search
    <ul>
      <li>Greedy aproach와 exhaustive aproach의 중간에 해당하는 기법이다.</li>
      <li>디코더의 매 타임스텝마다 우리가 정해놓은 $k$개의 가능한 경우를 고려하고 마지막 까지 진행한 $k$개의 candidate 중에서 가장 확률이 높은 것을 선택하는 방식이다.</li>
      <li>$k$개의 경우의 수에 해당하는 디코딩의 output을 하나하나의 가설(hypothesis)라 부른다.</li>
      <li>$k$는 beam size라 부르고 일반적으로 5 ~ 10사이에서 설정한다.</li>
      <li>joint probability에 log를 취해 덧셈으로 만들어버린다.
        <ul>
          <li>$score(y_1,…,y_t) = logP_{LM}(y_1,…,y_t|x) = \sum_{i=1}^t logP_{LM}(y_i|y_1,…,y_{i-1},x)$</li>
          <li>물론 log는 단조증가하는 함수이므로 최댓값은 유지된다.</li>
        </ul>
      </li>
      <li>Beam search는 모든 경우의 수를 보는 것은 아니지만 완전탐색하는 것보다 효율적으로 계산할 수 있게 한다.
        <ul>
          <li>
            <p>아래 그림은 $k=2$인 beam search의 예제이다.</p>

            <p><img src="https://lh3.googleusercontent.com/fife/ALs6j_FQHl6hJSUztJ_8AdRWPK3K7EP8sRaKTgiS3G2_YcEVM2PzIt60IIgfbUHIFbvXq-8yz57cEWUb8fn1I5PCC0XQKsNACgqXZPlkAlJC0Di9O-QfjlbHM-b4vFqWJ8-4TYOCFAHzvUSctQjJS06DOgYmmbPXf-PZhCz4sw4LsMl3C1qsYfh003_3bHRTSLGpLxHzDsCEP-CTOWcvnjCNuZCN1qnDSoskAz6zesSMpUQeiiSqmLjc_clFS-kif47zvaLXFXisH_WOb2PmutbSlpoIoPOCZIo-kUoDsSQMOwwpayotg7OtqQwT7Jdv4BMeLHHFwTyclEvjFXl-hh0cx5TvztEqTdS8qECSuwAeoqTRszCEvu7Lj3yL9QRArryCA0rvB_0tVOcFfAv7RGVULN0lCIXDtL_vq0WKuxUHa1GtExCk2gNhzwCKzwMdYVnBAC6C-ue19B29ZJVWMCz1Um3tcdLk4qIQBap0sCs7BwM4KHmwsDCqlnghBl23ZeFgrkEoaDNj2OfyFAjsYmPxw5UcDzei9H3XaS1J2FRLVUbiVn8yCk3aMaOR7F3TKKtYoCuueCcSPBHKgIhxvzXjDxpPj01qPN1ddmCf07g7e52oJp5Qojg3QPB-xJivmyJ580UAf1ZQ-2W6S2xwtW95_77iw6kV9qT7OkYUZ8ZwiOje3dZi2cIUx8RY4JjHdc3EgMR_QjsdsFt3_O34sNy94dvcDnqxDz_jM-Lqr5DTDNWHggbRIS1cTVE3AOxpcJ1CDYlCgbV0G2l6KsVDegmo93ZjBrPyGMm9RYo4dXzLstvSp6KfAbytK-aRF4G8KDoPNvitR7khSlqwY7SXO3msQfrVOX0fAsGiJo3ty8WySTseC4dfC4ctWAOGNo5CA_-L4W67d9SWzNGoG-CyHWRLo_8XkONjBpqgarCMUsENyNLtb6tlXJEnRAznRhfb5xIulYkVJALd7WeyF7AnxpFRHk1PPoqXM97RHd3XGaTYG4dKtbgoU8aEmyR77U6aSza5CV9ehSlaaVqNWmP1HMSxkeXN-IiI8VyChlG7s1XG0452rEZorP668b2I7oft_lhpSYQgUFgaj9s_bP5lEGMbYhf9PpgRd5FPjl3t-oB6xaAoy8zPAbT9LZgYNyUZANOBDYFIBDFKnYxLk2wzrnhe-ycdBEYQ3RFJ8EQkUwzYDHxHvMdQ-gjiujZPDySnJVB1Pde39RFHpkpGJiMafL5vxvIQg_Jxa4_aikN2pNKIFu_L9sK_MDOhuOOrdMDaoE--aamXnrNAvEJEIH5DvZOnf-IjSgijTCJ3DK0AWVvk0f31VTwpfil0hNb7kexEJAQLti9cpC7G1pAI2SuYROIXNxwkjBv0wVnRT-7VXemcxu1DwFKkM9vexhShhPCahL3NA79sY2Vi3r8QBB4JuhMJYhSvA31L1U569IWk6jC3dsCWLkVa2H5uy2CflhVqSQMjydm3PgefW9xD8wcYWH5pk-U5C9PsNZ_n5GG4G4511cfN9KQpF9G3OkImFHGCxx6fI5YLdbXiZ4cuE9-p60ox3OlgXn_xbo35mTF5nMW5xaHvhEnWItRnGwWXsOEXVoUpWvHsByKWaa4c62DYbGqM6NJJYUjT0ZSPl-zO3BquQXJ-mCyrBQ" alt="" /></p>

            <ul>
              <li>
                <p>그림 출처 : <a href="https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf">https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf</a></p>
              </li>
              <li>
                <p>각 단계에서 생성되어 확률을 확인하고 그 중 $k$개까지만 다음 단어를 생성할 수 있도록 한다.</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>greedy decoding은 모델이 END 토큰을 생성할 때 끝내도록 되어있다.</li>
      <li>beam searching decoding은 서로 다른 시점에서 END 토큰을 생성할 수 있다.
        <ul>
          <li>어떤 hypothesis가 END토큰을 만든다면 현재 시점에서 그 hypothesis는 완료한다. 완성된 문장은 임시 공간에 넣어둔다.</li>
          <li>남은 hypothesis 또한 END토큰을 만들때까지 수행한 후 임시 공간에 넣어둔다.</li>
        </ul>
      </li>
      <li>미리 정한 타임스텝 $T$까지 디코딩하여 빔 서치 과정을 중단하거나 $T$시점에 END토큰을 발생시켜 중단하도록 한다. 또는 미리 정한 $n$개 만큼의 hypothesis를 저장하게 되면 빔 서치를 종료하게 된다.</li>
      <li>빔 서치 종료 후 완성된 hypotheses의 리스트를 얻게 되고 이 중 가장 높은 score를 가진 하나를 뽑아야 한다.
        <ul>
          <li>그러나 긴 길이의 hypotheses들은 낮은 score를 가진다는 예상을 할 수 있다. 왜냐하면 log때문에 새로운 단어가 생성될 때마다 음수값을 더해줘야 하기 때문이다.</li>
          <li>그래서 좀 더 공평하게 비교하기 위해 각 hypotheses별로 Normalize를 수행한다.
            <ul>
              <li>$score = \frac{1}{t}\sum_{i=1}^t logP_{LM}(y_i|y_1,…,y_{i-1},x)$</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="blue-score">BLUE score</h2>

<ul>
  <li>기존 평가방법의 한계
    <ul>
      <li>정답 I love you와 예측 oh I love you라는 문장이 주어진 경우 각각의 위치에는 모두 다른 단어이기 때문에 예측률 0%로 잡히게 된다.</li>
      <li>Precision and Recall
        <ul>
          <li>Precision = # of correct words / length of prediction(예측)</li>
          <li>Recall = # of correct words / length of reference(정답)</li>
          <li>F-measure = (precision $\times$ recall) / (precision + recall) / 2 (조화평균)
            <ul>
              <li>조화평균은 Precision과 Recall 중 작은 수에 치중하는 특징을 가지고 있다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>하지만 F-measure가 높다고 해서 그 문장의 문법까지 맞지는 않는 단점이 있다.</li>
    </ul>
  </li>
  <li>BLEU(BiLingual Evaluation Understudy) score
    <ul>
      <li>N-gram이라는 연속된 N개의 연속된 단어가 ground truth와 얼마나 겹치는 가를 계산하여 평가에 반영하는 방법이다.</li>
      <li>Precision 만을 반영하고 Recall은 무시한다. 정답에서 몇 개의 단어가 떨어져도 비슷한 해석이 가능하기 때문이다.</li>
      <li>BLUE = min(1, length_of_precision / length_of_reference)$(\Pi_{i=1}^4 precision_i)^{\frac{1}{4}}$ (기하평균)
        <ul>
          <li>min(1, length precision/length reference)는 brevity panelty라 부르는데  길이만을 고려했을 때 ground truth 문장과 짧을 경우 짧은 비율만큼 뒤에서 계산된 precision을 낮추는 역할을 한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#attention" class="page__taxonomy-item p-category" rel="tag">attention</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#boostcamp" class="page__taxonomy-item p-category" rel="tag">BoostCamp</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-09-10T00:00:00+09:00">September 10, 2021</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/boostcamp/recurrent-neural-network/" class="pagination--pager" title="Basics of Recurrent Nerual Networks (RNN)
">Previous</a>
    
    
      <a href="/boostcamp/transformer/" class="pagination--pager" title="Transformer 정리
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/data-engineer/kafka-pipeline/" rel="permalink">ksqlDB: 실시간 데이터 처리 후 시각화까지
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> September 03 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">ksqlDB
ksqlDB는 Kafka Streams에 기반하는 SQL 엔진입니다. ksqlDB는 Kafka topic에 이벤트 스트리밍 애플리케이션을 구축할 수 있는 쿼리 계층을 제공합니다. Kafka Streams와 달리 ksqlDB는 SQL로 새로운 스트림을 생성하거나 Mate...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/transformer-scratch-implementation-2/" rel="permalink">python으로 Transformer 바닥부터 구현하기[2] (Transformer)
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> August 01 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Objective
앞에서 구현한 LayerNorm, MultiHeadAttention, GELU를 사용하고 이전에 구현해둔 Linear, Dropout, Softmax 클래스를 사용하여 Transformer 클래스를 구현하여 테스트해봅니다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/transformer-scratch-implementation-1/" rel="permalink">python으로 Transformer 바닥부터 구현하기[1] (MultiHead-Attention, LayerNorm, GELU)
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> July 31 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Transformer
트랜스포머(Transformer)는 2017년에 등장한 모델입니다. Attention is all you need 논문은 트랜스포머 모델에 대해 설명하고 있으며 이후 BERT, GPT라는 새로운 모델을 탄생시키는 배경이 됩니다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/paper/1-bit-llm/" rel="permalink">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 03 2024</p>
    
    <p class="archive__item-excerpt" itemprop="description">Abstract
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 emeraldgoose. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'emeraldgoose/emeraldgoose.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [['$','$']],
  displayMath: [['$$','$$']],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

  </body>
</html>
