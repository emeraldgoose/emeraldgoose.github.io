<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Python으로 Transformer 바닥부터 구현하기[1] (MultiHead-Attention, LayerNorm, GELU) - gooooooooooose</title>
<meta name="title" content="Python으로 Transformer 바닥부터 구현하기[1] (MultiHead-Attention, LayerNorm, GELU)">
<meta name="description" content="Transformer 트랜스포머(Transformer)는 2017년에 등장한 모델입니다. Attention is all you need 논문은 트랜스포머 모델에 대해 설명하고 있으며 이후 BERT, GPT라는 새로운 모델을 탄생시키는 배경이 됩니다.">


  <meta name="author" content="goooose">
  
  <meta property="article:author" content="goooose">
  


<!-- open-graph tags -->
<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="gooooooooooose">
<meta property="og:title" content="Python으로 Transformer 바닥부터 구현하기[1] (MultiHead-Attention, LayerNorm, GELU)">
<meta property="og:url" content="http://localhost:4000/pytorch/transformer-scratch-implementation-1/">


  <meta property="og:description" content="Transformer 트랜스포머(Transformer)는 2017년에 등장한 모델입니다. Attention is all you need 논문은 트랜스포머 모델에 대해 설명하고 있으며 이후 BERT, GPT라는 새로운 모델을 탄생시키는 배경이 됩니다.">







  <meta property="article:published_time" content="2024-07-31T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/pytorch/transformer-scratch-implementation-1/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "emeraldgoose",
      "url": "http://localhost:4000/"
    
  }
</script>


  <meta name="google-site-verification" content="googleb34ce5276ba6573e" />





  <meta name="naver-site-verification" content="naver93cdc79a5629ab3736d7cf8ff7b51d80">


<!-- end _includes/seo.html -->




<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>

<!-- Lightbox2 -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/css/lightbox.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/js/lightbox-plus-jquery.js" integrity="sha512-oaWLach/xXzklmJDBjHkXngTCAkPch9YFqOSphnw590sy86CVEnAbcpw17QjkUGppGmVJojwqHmGO/7Xxx6HCw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/js/lightbox-plus-jquery.min.js" integrity="sha512-U9dKDqsXAE11UA9kZ0XKFyZ2gQCj+3AwZdBMni7yXSvWqLFEj8C1s7wRmWl9iyij8d5zb4wm56j4z/JVEwS77g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/" itemprop="item"><span itemprop="name">Home</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#pytorch" itemprop="item"><span itemprop="name">Pytorch</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Python으로 Transformer 바닥부터 구현하기[1] (MultiHead-Attention, LayerNorm, GELU)</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo.png" alt="goooose" itemprop="image" class="u-photo" width="110px" height="110px">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">goooose</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Interested in ML/DL, Data Engineering.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:smk6221@naver.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://github.com/emeraldgoose" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
          
            <li><a href="https://www.linkedin.com/in/minseong-kim-84428b231/" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://gooooooooooose.tistory.com/" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Problem Solving OJ (KOR)</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Python으로 Transformer 바닥부터 구현하기[1] (MultiHead-Attention, LayerNorm, GELU)">
    <meta itemprop="description" content="Transformer트랜스포머(Transformer)는 2017년에 등장한 모델입니다. Attention is all you need 논문은 트랜스포머 모델에 대해 설명하고 있으며 이후 BERT, GPT라는 새로운 모델을 탄생시키는 배경이 됩니다.">
    <meta itemprop="datePublished" content="2024-07-31T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Python으로 Transformer 바닥부터 구현하기[1] (MultiHead-Attention, LayerNorm, GELU)
</h1>
          
<!--
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  0 minute read
</p>
            devinlife comments :
                싱글 페이지(포스트)에 제목 밑에 Updated 시간 표기
                기존에는 read_time이 표기. read_time -> date 변경
-->
            <p class="page__date"><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2024-07-31T00:00:00+09:00">July 31, 2024</time></p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h1 id="transformer">Transformer</h1>
<p>트랜스포머(Transformer)는 2017년에 등장한 모델입니다. <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a> 논문은 트랜스포머 모델에 대해 설명하고 있으며 이후 BERT, GPT라는 새로운 모델을 탄생시키는 배경이 됩니다.</p>

<h1 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h1>
<p>트랜스포머 모델은 셀프 어텐션(Self-Attention) 매커니즘을 사용하고 있습니다. 셀프 어텐션 매커니즘은 자기 자신에 대해 어텐션 밸류를 구하는 방법으로 문장 내 단어간의 관계를 학습할 수 있고 병렬로 처리가능한 점이 장점입니다. 트랜스포머 등장 이전 모델의 경우 단어가 나타나는 시간 흐름에 따라 정보를 파악해야 하는 한계가 있었지만 어텐션을 통해 시간 흐름에 상관없이 관계를 학습할 수 있는 방법이 되었습니다.</p>

<p>Self Attention 매커니즘의 핵심은 Scaled Dot Product Attention입니다. Dot-product한 값을 Scale Down하여 어텐션을 구하는 매커니즘으로 하나의 단어마다 계산하는 것이 아닌 문장 전체 단어에 대해 어텐션을 계산할 수 있습니다.</p>

<h2 id="forward">Forward</h2>
<p>SPDA의 forward 수식을 살펴보겠습니다.</p>

<p>$Attention(Q,K,V) = \text{softmax}(\frac{QK^{\top}}{\sqrt{d_k}})V$</p>

<p>Query는 현재 단어의 정보를 말합니다. 문장 내 임의의 단어들의 정보가 Query로 사용되면 다른 단어의 정보를 담은 Key와의 유사도(Similarity) 연산을 Dot-product로 수행하여 Softmax 연산을 통해 Attention Score를 구하게 됩니다. 스코어와 Value를 곱하여 Attention Value를 계산하게되는데 이것은 스코어를 이용해 Value에서 얼마나 정보를 가져올 것인지 결정하는 것입니다.</p>

<h3 id="scaling">Scaling</h3>
<p>Query와 Key와의 유사도 연산 후에 Scale Down을 하는 이유는 Dot-product 후 값이 너무 커지는 것을 방지하기 위함입니다. $d_k$가 길어질 수록 $QK^{\top}$의 값도 같이 커져 기울기가 0이 되어 gradient vanishing 문제가 발생할 수 있기 때문에 스케일링이 필요합니다.</p>

<p>다음의 간단한 코드와 그래프로 스케일링 전후의 차이를 쉽게 이해할 수 있습니다.</p>

<script src="https://gist.github.com/emeraldgoose/0e868296f2d457d433b4b7cfbae6d45c.js"></script>

<p>간단하게 랜덤한 Query와 Key가 주어질 때 dot-product를 수행하고 softmax를 실행하면 다음의 왼쪽 그래프를 얻을 수 있습니다. 그러나 위 코드에서 <code class="language-plaintext highlighter-rouge">softmax(np.dot(Q, k.T) / np.sqrt(d_k))</code>로 수정하게 되면 다음의 오른쪽 그래프를 얻을 수 있습니다.</p>

<figure class="half">
  <a href="https://1drv.ms/i/s!AoC6BbMk0S9Qm0ld7x9iBmT1MwWP?embed=1&amp;width=826&amp;height=813" data-lightbox="gallery"><img src="https://1drv.ms/i/s!AoC6BbMk0S9Qm0ld7x9iBmT1MwWP?embed=1&amp;width=826&amp;height=813" alt="01" /></a>
  <a href="https://1drv.ms/i/s!AoC6BbMk0S9Qm0tbtiMJQyPGzMUW?embed=1&amp;width=826&amp;height=813" data-lightbox="gallery"><img src="https://1drv.ms/i/s!AoC6BbMk0S9Qm0tbtiMJQyPGzMUW?embed=1&amp;width=826&amp;height=813" alt="02" /></a>
</figure>

<p>왼쪽 그래프에서 9번 확률이 1.0이지만 오른쪽 그래프에서는 0.4x로 줄고 나머지 번호들의 정보들이 드러나는 것을 볼 수 있습니다. 즉, softmax로 정보가 사라지는 문제를 스케일링으로 해결할 수 있습니다.</p>

<p>또한, $d_k$를 사용하는 이유는 다음과 같습니다.</p>

<p>Query와 Key가 가우시안 분포를 따른다고 가정하면 평균은 0, 분산은 1을 가지게 됩니다. $QK^{\top} = \sum_{i}^{d_k}Q_iK_i$이고 Query와 Key는 independent하므로 평균은 0, 분산은 $d_k$를 가지게 됩니다. 따라서 $\sqrt{d_k}$로 스케일링하여 분산을 1로 줄일 수 있게 됩니다.</p>

<script src="https://gist.github.com/emeraldgoose/a23756410bc023d518cabaef8e5e1945.js"></script>

<p>attention padding mask 처럼 어텐션 스코어에 마스킹을 위한 masked_fill 함수는 <code class="language-plaintext highlighter-rouge">numpy.ma.array</code>를 이용하여 구현했습니다.</p>

<h2 id="backward">Backward</h2>
<p>Backward 함수를 구현하기 위해 Attention 계산에 참여하는 Query, Key, Value 세 가지 입력에 대한 기울기가 필요합니다.</p>

<p>먼저, Attention forward 수식을 나눠서 생각해보겠습니다.</p>

<p>$1. \ A = \frac{QK^{\top}}{\sqrt{d_k}}$<br />
$2. \ W = \text{softmax}(A)$<br />
$3. \ Z = WV$</p>

<p>1번 수식으로부터 $\frac{\partial A}{\partial Q}$와 $\frac{\partial A}{\partial K}$를 구할 수 있습니다.</p>

<p>$\frac{\partial A}{\partial Q} = \frac{K}{\sqrt{d_k}}$</p>

<p>$\frac{\partial A}{\partial K} = \frac{Q}{\sqrt{d_k}}$</p>

<p>2번 수식에서 얻을 수 있는 $\frac{\partial W}{\partial A}$는 softmax의 미분결과이므로 W(1 - W)로 가정해보겠습니다.(실제 계산은 W(1 - W)가 아닙니다. 저는 미리 구현해둔 softmax 클래스를 이용할 것입니다.)</p>

<p>또한, 3번 수식으로부터 $\frac{\partial Z}{\partial W} = V^{\top}$도 알 수 있습니다.</p>

<p>다음, Value에 대한 기울기부터 계산합니다. upstream gradient를 $dZ$라 가정하겠습니다.</p>

<p>$\frac{\partial L}{\partial V} = \frac{\partial L}{\partial Z} \frac{\partial Z}{\partial V} = W^{\top} dZ$</p>

<p>다음, Query와 Key에 대한 기울기를 계산합니다.</p>

<p>$\frac{\partial L}{\partial Q} = \frac{\partial L}{\partial Z} \frac{\partial Z}{\partial W} \frac{\partial W}{\partial A} \frac{\partial A}{\partial Q} = dZ \cdot W(1-W) \cdot \frac{K}{\sqrt{d_k}}$</p>

<p>$\frac{\partial L}{\partial K} = \frac{\partial L}{\partial Z} \frac{\partial Z}{\partial W} \frac{\partial W}{\partial A} \frac{\partial A}{\partial K} = dZ \cdot W(1-W) \cdot \frac{Q}{\sqrt{d_k}}$</p>

<script src="https://gist.github.com/emeraldgoose/4ff45c20339d57fef96c3206d66583b2.js"></script>

<p>dK를 계산할 때는 사이즈를 맞추기 위해 einsum을 사용했습니다.</p>

<h1 id="multihead-attention">MultiHead Attention</h1>
<p>MultiHead Attention은 Attention을 여러개의 head가 동시에 수행하는 모듈입니다. 하나의 문장에 대해 관계에 집중하는 어텐션 혹은 단어에 집중하는 있는 어텐션 등을 학습할 수 있고 이러한 어텐션을 병렬로 처리하기 위해 구현되었습니다.</p>

<p>pytorch의 경우 MultiHead Attention의 파라미터는 self-attention인 경우와 아닌 경우 다르게 파라미터를 선언하도록 구현되었습니다. self-attention인 경우 $W_Q$, $W_K$, $W_V$가 concat된 하나의 weight(<code class="language-plaintext highlighter-rouge">in_proj_weight</code>, <code class="language-plaintext highlighter-rouge">in_proj_bias</code>)로 선언하고 최종 계산된 값을 3개의 청크로 나눠 리턴하도록 합니다.</p>

<p>하지만, 트랜스포머를 구현하면서 모든 어텐션을 셀프 어텐션으로 사용하지 않으므로 구현을 간단히 하기 위해 q_proj_weight, k_proj_weight, v_proj_weight, out_proj_weight로 따로 선언하여 구현했습니다.</p>

<h2 id="forward-1">Forward</h2>
<p>MultiHead Attention의 계산 수식은 다음과 같습니다.</p>

<p>$\text{MultiHead}(Q, K, V) = \text{Concat}(head_1,…,head_h)W^O$, $head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$</p>

<p>MultiHead Attention 모듈의 <code class="language-plaintext highlighter-rouge">embed_dim</code>은 모델의 전체 차원을 말합니다. 이것을 head의 개수(<code class="language-plaintext highlighter-rouge">num_heads</code>)로 나눈 값을 각 head에서 사용될 차원 <code class="language-plaintext highlighter-rouge">head_dim</code>으로 사용하게 됩니다. 따라서, <code class="language-plaintext highlighter-rouge">embed_dim</code>은 <code class="language-plaintext highlighter-rouge">num_heads</code>의 배수로 설정해야 합니다.</p>

<script src="https://gist.github.com/emeraldgoose/b8d3081f5885ead2f962bdcd337fbd47.js"></script>

<p><code class="language-plaintext highlighter-rouge">attn_output_transpose</code>는 backward에서 shape가 필요하기 때문에 저장했고 <code class="language-plaintext highlighter-rouge">attn_output_reshaped</code>는 backward에서 <code class="language-plaintext highlighter-rouge">d_out_proj_weight</code>를 계산하기 위해 저장했습니다. 차원을 맞춰주는 것에 주의하면서 구현해야 했습니다.</p>

<h2 id="backward-1">Backward</h2>
<p>backward 구현은 forward의 역순으로 구성하면 됩니다. 역시 차원을 맞춰주는 것이 가장 중요하기 때문에 이 부분만 주의하면 됩니다.</p>

<p>MultiHeadAttention 모듈은 입력값으로 Query, Key, Value가 들어가므로 당연히 Query에 대한 기울기, Key에 대한 기울기, Value에 대한 기울기을 리턴해야 합니다. 만약 Self-attention이라면 세 리턴값을 더한 값이 입력에 대한 기울기가 됩니다.</p>

<script src="https://gist.github.com/emeraldgoose/ad345617843049ba9dee9fdfcb5536dc.js"></script>

<h1 id="layernorm">LayerNorm</h1>
<p>Normalization은 입력값에 대해 정규화하기 위한 과정입니다. 학습 데이터를 배치 단위로 학습할 때, 랜덤하게 샘플링된 배치들의 데이터 분포가 다를 수 있습니다. 이러한 배치를 그대로 학습하게 되면 그에 맞게 네트워크는 가중치를 조절하게 되는데 이로 인해 학습에 들이는 시간이 증가될 수 있습니다. 따라서, 정규화를 통해 데이터 분포를 조정하여 안정적이고 학습이 빠르게 이루어지도록 하는 기법이 Normalization입니다.</p>

<p>배치 단위로 정규화를 진행하는 Batch Normalization(BN), 데이터 feature 단위로 정규화를 진행하는 Layer Normalization(LN), BatchNorm과 LayerNorm의 특징을 섞은 Group Normalization(GN)이 있습니다. 여기서는 Layer Normalization을 구현합니다.</p>

<p>Layer Normalization은 각 샘플마다 모든 feature들에 대해 평균을 0, 분산을 1로 조정하여 배치 사이즈에 의존적이지 않게 됩니다.</p>

<p>또한, RNN 계열의 모델은 BN을 사용하기 어렵습니다. 왜냐하면, Sequential한 모델은 각 타임스텝에 따라 다른 데이터가 들어와야 하고 길이가 다른 경우 padding 처리가 필요했습니다. 그래서 배치 정규화를 적용하기 어려웠기 때문에 LN을 적용합니다.</p>

<h2 id="forward-2">Forward</h2>
<p>LN의 수식은 다음과 같습니다.</p>

<p>$y = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}} * \gamma + \beta$</p>

<p>$\gamma$와 $\beta$는 학습가능한 파라미터입니다. $\gamma$와 $\beta$를 사용하는 이유는 Normalization으로 인해 데이터 특성이 사라지는데 이것이 학습하는데 그리 좋지 못합니다. 그래서 파라미터를 추가하여 네트워크가 학습하기 좋도록 스케일링, 시프트 작업을 수행하기 위해 $\gamma$
와 $\beta$가 추가되었습니다.</p>

<script src="https://gist.github.com/emeraldgoose/819c446042184f37fd2a7086286a1061.js"></script>

<h2 id="backward-2">Backward</h2>
<p>먼저, 수식을 다음과 같이 분리하여 생각해보겠습니다.</p>

<ol>
  <li>
    <p>$\mu = \frac{1}{N}\sum x$</p>
  </li>
  <li>
    <p>$\sigma^2 = \frac{1}{N}\sum(x - \mu)^2$</p>
  </li>
  <li>
    <p>$\hat{x} = \frac{x-\mu}{\sqrt{\sigma^2 + \epsilon}}$</p>
  </li>
  <li>
    <p>$y = \gamma \hat{x} + \beta$</p>
  </li>
</ol>

<p>먼저, $\hat{x}$에 대한 기울기를 계산하겠습니다.</p>

<p>$\frac{\partial L}{\partial \hat{x}} = \gamma$</p>

<p>다음, 분산에 대한 기울기를 계산하겠습니다.</p>

<p>$\frac{\partial L}{\partial \sigma^2} = \frac{\partial L}{\partial \hat{x}} \cdot \frac{\partial \hat{x}}{\partial \sigma^2}$</p>

<p>$\frac{\partial \hat{x}}{\partial \sigma^2} = -\frac{1}{2}(x - \mu)(\sigma^2 + \epsilon)^{-\frac{3}{2}}$</p>

<p>$\frac{\partial L}{\partial \sigma^2} = \frac{\partial L}{\partial \hat{x}} \cdot -\frac{1}{2}(x - \mu)(\sigma^2 + \epsilon)^{-\frac{3}{2}}$</p>

<p>다음, 평균에 대한 기울기를 계산하겠습니다. 2번에서 분산을 계산할 때 평균이 참여하므로 분산을 평균으로 미분하여 얻은 기울기도 추가되어야 합니다.</p>

<p>$\frac{\partial L}{\partial \mu} = \frac{\partial L}{\partial \hat{x}} \cdot \frac{\partial \hat{x}}{\partial \mu} + \frac{\partial L}{\partial \sigma^2} \cdot \frac{\partial \sigma^2}{\partial \mu}$</p>

<p>$\frac{\partial \sigma^2}{\partial \mu} = -\frac{2}{N}(x - \mu)$</p>

<p>$\frac{\partial L}{\partial \mu} = \frac{\partial L}{\partial \hat{x}} \cdot -\frac{1}{\sqrt{\sigma^2 + \epsilon}} + \frac{\partial L}{\partial \sigma^2} \cdot -\frac{2}{N}(x-\mu)$</p>

<p>마지막으로, x에 대한 기울기를 계산하겠습니다.</p>

<p>$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial \hat{x}} \cdot \frac{\partial \hat{x}}{\partial x} + \frac{\partial L}{\partial \mu} \cdot \frac{\partial \mu}{\partial x} + \frac{\partial L}{\partial \sigma^2} \cdot \frac{\partial \sigma^2}{\partial x}$</p>

<p>$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial \hat{x}} \cdot \frac{1}{\sqrt{\sigma^2 + \epsilon}} + \frac{\partial L}{\partial \sigma^2} \cdot \frac{2}{N}(x - \mu) + \frac{\partial L}{\partial \mu} \cdot \frac{1}{N}$</p>

<script src="https://gist.github.com/emeraldgoose/7d25c8808ddbbe96eab4e348ed8ef020.js"></script>

<p>코드로 구현할 때는 epsilon 값을 0으로 가정하여 구현되었습니다.</p>

<h1 id="gelu">GELU</h1>
<p>GELU는 ReLU의 단점을 보완한 활성화 함수입니다.</p>

<p>ReLU는 다음의 그래프와 같이 입력값이 음수일 경우 0을 출력하여 뉴런이 죽는 현상인 Dying ReLU 문제가 발생했습니다. 입력된 값이 음수가 한번이라도 등장하게 되면 활성화 함수의 기울기가 0이므로 그 뉴런과 연결된 다른 뉴런들 또한 기울기가 0이 됩니다. 대안으로 음수 입력에도 기울기를 제공하는 leaky relu가 있지만 음수에 대한 boundary가 없기 때문에 어떠한 음수값에 대해서도 기울기를 제공해버립니다. 그래서 swish나 GELU와 같이 음수 boundary를 제공하는 활성화 함수가 제안되었습니다.</p>

<figure class="half">
  <a href="https://1drv.ms/i/s!AoC6BbMk0S9Qm0o_jVY2iJmV4LNq?embed=1&amp;width=640&amp;height=480" data-lightbox="gallery"><img src="https://1drv.ms/i/s!AoC6BbMk0S9Qm0o_jVY2iJmV4LNq?embed=1&amp;width=640&amp;height=480" alt="03" /></a>
  <a href="https://1drv.ms/i/s!AoC6BbMk0S9Qm0yGxsrvrjyLkgip?embed=1&amp;width=640&amp;height=480" data-lightbox="gallery"><img src="https://1drv.ms/i/s!AoC6BbMk0S9Qm0yGxsrvrjyLkgip?embed=1&amp;width=640&amp;height=480" alt="04" /></a>
  <figcaption>Refs: https://pytorch.org</figcaption>
</figure>

<p>하지만, NLP 태스크에서 꼭 GELU여야 하는 이유는 잘 모르겠습니다. 다른 블로그에서는 “ReLU나 ELU보다 GELU가 성능이 좋았다”정도로만 다들 기록하고 있습니다.<br />
가우시안 분포의 연속성이 임베딩와 같은 벡터 학습에 유리하기 때문일 수도 있지만 정확한 이유를 찾기 어려웠습니다.</p>

<h2 id="forward-3">Forward</h2>
<p>GELU는 가우시안 분포의 누적 분포 함수(CDF)와 입력값을 곱하는 활성화 함수입니다.</p>

<p>$\text{GELU}(x) = x \cdot \Phi(x)$</p>

<p>제가 구현할 때는 tanh로 근사한 수식을 사용했습니다.</p>

<p>$\text{GELU}(x) = 0.5 * x * (1 + \text{tanh}(\sqrt{2/\pi} * (x + 0.044715 * x^3)))$</p>

<script src="https://gist.github.com/emeraldgoose/400eedea60daf0c9ecfd7854b0c1e3db.js"></script>

<h2 id="backward-3">Backward</h2>
<p>미분을 하기 위해 수식을 두개로 나누어 생각해보겠습니다.</p>

<ol>
  <li>
    <p>$\text{GELU}(x) = 0.5x(1 + \text{tanh}(y))$</p>
  </li>
  <li>
    <p>$y = \sqrt{\frac{2}{\pi}}(x + 0.044715x^3)$</p>
  </li>
</ol>

<p>먼저, 1번 수식에서 $\hat{x}$에 대한 기울기를 계산합니다.</p>

<p>$\frac{\partial \text{GELU}}{\partial x} = 0.5(1 + \text{tanh}(y)) + 0.5x\frac{\partial \text{tanh}(y)}{\partial x}$</p>

<p>$\frac{\partial \text{tanh}(y)}{\partial x} = (1 - \text{tanh}^2(y))\frac{\partial y}{\partial x}$</p>

<p>다음, 2번 수식에서 x에 대한 기울기를 계산합니다.</p>

<p>$\frac{\partial y}{\partial x} = \sqrt{\frac{2}{\pi}}(1 + 0.044715 * 3 * x^2)$</p>

<p>마지막으로, 정리합니다.</p>

<p>$\frac{\partial \text{GELU}}{\partial x} = 0.5(1 + \text{tanh}(y)) + 0.5x(1 - \text{tanh}^2(y))\sqrt{\frac{2}{\pi}}(1 + 0.044715 * 3 * x^2)$</p>

<script src="https://gist.github.com/emeraldgoose/aaa360c241799e50ea55448c9026a9dc.js"></script>

<h1 id="reference">Reference</h1>
<ul>
  <li><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention">torch.nn.functional.scaled_dot_product_attention.html</a></li>
  <li><a href="https://ai.stackexchange.com/questions/21237/why-does-this-multiplication-of-q-and-k-have-a-variance-of-d-k-in-scaled">why-does-this-multiplication-of-q-and-k-have-a-variance-of-d-k-in-scaled</a></li>
  <li><a href="https://www.blossominkyung.com/deeplearning/transformer-mha">https://www.blossominkyung.com/deeplearning/transformer-mha</a></li>
  <li><a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html">https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html</a></li>
  <li><a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html</a></li>
  <li><a href="https://lifeignite.tistory.com/47">https://lifeignite.tistory.com/47</a></li>
  <li><a href="https://pytorch.org/docs/stable/generated/torch.nn.GELU.html">https://pytorch.org/docs/stable/generated/torch.nn.GELU.html</a></li>
  <li><a href="https://brunch.co.kr/@kdh7575070/27">https://brunch.co.kr/@kdh7575070/27</a></li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#torch" class="page__taxonomy-item p-category" rel="tag">torch</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#pytorch" class="page__taxonomy-item p-category" rel="tag">Pytorch</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2024-07-31T00:00:00+09:00">July 31, 2024</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/paper/1-bit-llm/" class="pagination--pager" title="The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits
">Previous</a>
    
    
      <a href="/pytorch/transformer-scratch-implementation-2/" class="pagination--pager" title="Python으로 Transformer 바닥부터 구현하기[2] (Transformer)
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/unet-and-ddpm-implementation/" rel="permalink">Python으로 Diffusion 바닥부터 구현하기[2] (im2col, UNet, DDPM)
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> May 24 2025</p>
    
    <p class="archive__item-excerpt" itemprop="description">Objective
이전 글에서 UNet의 구성요소인 ResidualBlock, AttentionBlock, UpsampleBlock을 구현했습니다.

  Python으로 Diffusion 바닥부터 구현하기[1] (ResidualBlock, AttentionBlock, Upsampl...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pytorch/text-to-image-implementation/" rel="permalink">Python으로 Diffusion 바닥부터 구현하기[1] (ResidualBlock, AttentionBlock, UpsampleBlock)
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> May 20 2025</p>
    
    <p class="archive__item-excerpt" itemprop="description">Text-to-Image
최근 Stable Diffusion이라는 모델이 발전하면서 많은 사람들이 이미지 생성 AI를 사용하고 있습니다. Diffusion 모델은 노이즈를 기존 이미지에 더한 뒤에 노이즈를 예측하고 지워가며 복원하는 학습과정을 거치게 됩니다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agent/MCP/" rel="permalink">Model Context Protocol(MCP)
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> May 18 2025</p>
    
    <p class="archive__item-excerpt" itemprop="description">Model Context Protocol
최근 MCP가 AI의 확장을 도움을 주는 사례가 많아지면서 크게 주목받고 있습니다. MCP는 Anthropic에서 처음 제안되어 LLM에 컨텍스트를 제공하는 방법을 표준화하는 개방형 프로토콜입니다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/data-engineer/apache-iceberg-architecture/" rel="permalink">Apache Iceberg: Architecture
</a>
      
    </h2>
    


    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> March 24 2025</p>
    
    <p class="archive__item-excerpt" itemprop="description">What is a Data Lakehouse?
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 emeraldgoose. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'emeraldgoose/emeraldgoose.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [['$','$']],
  displayMath: [['$$','$$']],
  processEscapes: true
  },
  "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

  </body>
</html>
